<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>multiple pose estimation on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/multiple-pose-estimation/</link>
    <description>Recent content in multiple pose estimation on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>alexblack2202@gmail.com (Phạm Duy Tùng)</managingEditor>
    <webMaster>alexblack2202@gmail.com (Phạm Duy Tùng)</webMaster>
    <copyright>&amp;copy; 2018 Phạm Duy Tùng. Website chia sẻ kiến thức của Phạm Duy Tùng và Đặng Thị Hằng. Vui lòng liên hệ email alexblack2202@gmail.com nếu bạn có thông tin cần trao đổi.</copyright>
    <lastBuildDate>Fri, 05 Oct 2018 00:19:00 +0300</lastBuildDate>
    <atom:link href="/tags/multiple-pose-estimation/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Deep Learning based Multiple Human Pose Estimation using OpenCV</title>
      <link>/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/</link>
      <pubDate>Fri, 05 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Lưu ý: Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv &amp;gt; 3.4.1.&lt;/p&gt;

&lt;p&gt;Ở bài viết trước, chúng ta đã tìm hiểu cách thức rút trích khung xương sử dụng DNN và đã áp dụng thành công trên ảnh có chứa 1 đối tượng người. Trong bài viết này, chúng ta sẽ thực hiện áp dụng mô hình cho bài toán có nhiều người trong cùng 1 bức ảnh.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-pretrain-model-trong-bài-toán-multiple-pose-estimation&#34;&gt;Sử dụng pretrain model trong bài toán multiple Pose Estimation&lt;/h2&gt;

&lt;p&gt;Trong bài viết này, chúng ta tiếp tục sử dụng mô hình MPI để dò tìm các điểm đặc trưng của con người và rút ra mô hình khung xương. Kết quả trả về của thuật toán gồm 15 đặc trưng như bên dưới.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
Left Ankle – 13, Chest – 14, Background – 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Áp dụng mô hình với ảnh của nhóm T-ARA.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2

nPoints = 15
POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]

protoFile = &amp;quot;pose/mpi/pose_deploy_linevec.prototxt&amp;quot;
weightsFile = &amp;quot;pose/mpi/pose_iter_160000.caffemodel&amp;quot;

net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

frame = cv2.imread(&amp;quot;tara1.jpg&amp;quot;)

inWidth = 368
inHeight = 368
 
# Prepare the frame to be fed to the network
inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
 
# Set the prepared object as the input blob of the network
net.setInput(inpBlob)

output = net.forward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử show lên vị trí vùng cổ trong hình.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
i = 0
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_head_heatmap.png&#34; alt=&#34;Hình với điểm đặc trưng vùng đầu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thử show lên hình điểm đặc trưng vùng cổ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 1
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_neck_heatmap.png&#34; alt=&#34;Hình với điểm đặc trưng vùng cổ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bằng một số phép biến đổi quen thuộc có sẵn trong opencv, chúng ta hoàn toàn có thể lấy được toạ độ của các điểm keypoint một cách dễ dàng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Find the Keypoints using Non Maximum Suppression on the Confidence Map
def getKeypoints(probMap, threshold=0.1):
    
    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)

    mapMask = np.uint8(mapSmooth&amp;gt;threshold)
    keypoints = []
    
    #find the blobs
    _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    #for each blob find the maxima
    for cnt in contours:
        blobMask = np.zeros(mapMask.shape)
        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)
        maskedProbMap = mapSmooth * blobMask
        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)
        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))

    return keypoints


detected_keypoints = []
keypoints_list = np.zeros((0,3))
keypoint_id = 0
threshold = 0.1
for i in range(nPoints):
    probMap = output[0, i, :, :]
    probMap = cv2.resize(probMap, (frameWidth, frameHeight))

    keypoints = getKeypoints(probMap, threshold)
    keypoints_with_id = []
    for j in range(len(keypoints)):
        keypoints_with_id.append(keypoints[j] + (keypoint_id,))
        keypoints_list = np.vstack([keypoints_list, keypoints[j]])
        keypoint_id += 1

    detected_keypoints.append(keypoints_with_id)



frameClone = cv2.cvtColor(frameCopy,cv2.COLOR_BGR2RGB)
for i in range(nPoints):
    for j in range(len(detected_keypoints[i])):
        cv2.circle(frameClone, detected_keypoints[i][j][0:2], 3, [0,0,255], -1, cv2.LINE_AA)

plt.imshow(frameClone) 
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_keypoint.png&#34; alt=&#34;Hình toàn bộ điểm đặc trưng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cuối cùng, chúng ta sẽ nối các điểm đặc trưng của các nhân vật thông qua thuật toán Part Affinity Heatmaps. Thuật toán này được đề xuất bởi nhóm tác giả Zhe Cao, Tomas Simon,Shih-En Wei, Yaser Sheikh thuộc phòng thí nghiệm The Robotics Institute trường đại học Carnegie Mellon. Các bạn có nhu cầu có thể tìm hiểu ở &lt;a href=&#34;https://arxiv.org/pdf/1611.08050.pdf&#34;&gt;https://arxiv.org/pdf/1611.08050.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
mapIdx = [[16,17], [18,19], [20,21], [22,23], [24,25], [26,27], [28,29], [30,31], [32,33], [34,35], [36,37], [38,39], [40,41], [42,43]]



colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],
         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],
         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]
# Find valid connections between the different joints of a all persons present
def getValidPairs(output):
    valid_pairs = []
    invalid_pairs = []
    n_interp_samples = 10
    paf_score_th = 0.1
    conf_th = 0.5
    # loop for every POSE_PAIR
    for k in range(len(mapIdx)):
        # A-&amp;gt;B constitute a limb
        pafA = output[0, mapIdx[k][0], :, :]
        pafB = output[0, mapIdx[k][1], :, :]
        pafA = cv2.resize(pafA, (frameWidth, frameHeight))
        pafB = cv2.resize(pafB, (frameWidth, frameHeight))


        # Find the keypoints for the first and second limb
        candA = detected_keypoints[POSE_PAIRS[k][0]]
        candB = detected_keypoints[POSE_PAIRS[k][1]]
        nA = len(candA)
        nB = len(candB)

        # fig=plt.figure(figsize=(8, 8))

        # interp_coord = list(zip(np.linspace(candA[0][0], candB[0][0], num=n_interp_samples),
        #                                     np.linspace(candA[0][1], candB[0][1], num=n_interp_samples)))

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 1)
        
        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)


        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafA, alpha=0.5)

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 2)
        

        

        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)
        
        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafB, alpha=0.5)
        # plt.show()



        

        # If keypoints for the joint-pair is detected
        # check every joint in candA with every joint in candB 
        # Calculate the distance vector between the two joints
        # Find the PAF values at a set of interpolated points between the joints
        # Use the above formula to compute a score to mark the connection valid
        
        if( nA != 0 and nB != 0):
            valid_pair = np.zeros((0,3))
            for i in range(nA):
                max_j=-1
                maxScore = -1
                found = 0
                for j in range(nB):
                    # Find d_ij
                    d_ij = np.subtract(candB[j][:2], candA[i][:2])
                    norm = np.linalg.norm(d_ij)
                    if norm:
                        d_ij = d_ij / norm
                    else:
                        continue
                    # Find p(u)
                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),
                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))
                    # Find L(p(u))
                    paf_interp = []
                    for k in range(len(interp_coord)):
                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],
                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) 
                    # Find E
                    paf_scores = np.dot(paf_interp, d_ij)
                    avg_paf_score = sum(paf_scores)/len(paf_scores)
                    
                    # Check if the connection is valid
                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -&amp;gt; Valid Pair  
                    if ( len(np.where(paf_scores &amp;gt; paf_score_th)[0]) / n_interp_samples ) &amp;gt; conf_th :
                        if avg_paf_score &amp;gt; maxScore:
                            max_j = j
                            maxScore = avg_paf_score
                            found = 1
                # Append the connection to the list
                if found:            
                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)

            # Append the detected connections to the global list
            valid_pairs.append(valid_pair)

            pprint(valid_pair)
        else: # If no keypoints are detected
            print(&amp;quot;No Connection : k = {}&amp;quot;.format(k))
            invalid_pairs.append(k)
            valid_pairs.append([])
    pprint(valid_pairs)
    return valid_pairs, invalid_pairs

# This function creates a list of keypoints belonging to each person
# For each detected valid pair, it assigns the joint(s) to a person
# It finds the person and index at which the joint should be added. This can be done since we have an id for each joint
def getPersonwiseKeypoints(valid_pairs, invalid_pairs):
    # the last number in each row is the overall score 
    personwiseKeypoints = -1 * np.ones((0, 19))

    for k in range(len(mapIdx)):
        if k not in invalid_pairs:
            partAs = valid_pairs[k][:,0]
            partBs = valid_pairs[k][:,1]
            indexA, indexB = np.array(POSE_PAIRS[k])

            for i in range(len(valid_pairs[k])): 
                found = 0
                person_idx = -1
                for j in range(len(personwiseKeypoints)):
                    if personwiseKeypoints[j][indexA] == partAs[i]:
                        person_idx = j
                        found = 1
                        break

                if found:
                    personwiseKeypoints[person_idx][indexB] = partBs[i]
                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]

                # if find no partA in the subset, create a new subset
                elif not found and k &amp;lt; 17:
                    row = -1 * np.ones(19)
                    row[indexA] = partAs[i]
                    row[indexB] = partBs[i]
                    # add the keypoint_scores for the two keypoints and the paf_score 
                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]
                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])
    return personwiseKeypoints

valid_pairs, invalid_pairs = getValidPairs(output)

personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)


for i in range(nPoints-1):
    for n in range(len(personwiseKeypoints)):
       
        index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]
        if -1 in index:
            continue
        B = np.int32(keypoints_list[index.astype(int), 0])
        A = np.int32(keypoints_list[index.astype(int), 1])
        cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)
       
        
        
plt.imshow(frameClone)
    # plt.imshow(mapMask, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_t-ara_finalresult.png&#34; alt=&#34;Hình kết quả cuối cùng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hẹn gặp lại các bạn ở những bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết này được viết dựa vào nguồn &lt;a href=&#34;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&#34;&gt;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&lt;/a&gt; của tác giả VIKAS GUPTA. Tôi sử dụng tập model và hình ảnh khác với bài viết nguyên gốc của tác giả.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>