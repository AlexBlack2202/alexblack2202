<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>normalization on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/normalization/</link>
    <description>Recent content in normalization on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2016-{year} Phạm Duy Tùng. All Rights Reserved.</copyright>
    <lastBuildDate>Fri, 25 Feb 2022 00:19:00 +0300</lastBuildDate><atom:link href="/tags/normalization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Các kỹ thuật chuẩn hóa trong Deep Learning</title>
      <link>/blog/2022-02-25-normalization/</link>
      <pubDate>Fri, 25 Feb 2022 00:19:00 +0300</pubDate>
      
      <guid>/blog/2022-02-25-normalization/</guid>
      <description>Tại sao chúng ta cần chuẩn hóa layer Mình nghĩ, câu trả lời thỏa đáng nhất là bởi vì nó làm tăng độ chính xác của mô hình. Trong quá trình thực nghiệm, các nhà nghiên cứu nhận thấy rằng việc thêm Layer Normalization cho kết quả test tốt hơn/ chạy nhanh hơn, hội tụ sớm hơn &amp;hellip; Và từ đó, các nhà nghiên cứu đổ hết tâm sức khai phá, đào bới nó ra thử sai , cải tiến, đề xuất các mô hình chuẩn hóa liên lục, tạo nên các mô hình mà mình sẽ liệt kê ở dưới.</description>
    </item>
    
  </channel>
</rss>
