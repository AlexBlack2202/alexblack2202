<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>thế giới di động on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/th%E1%BA%BF-gi%E1%BB%9Bi-di-%C4%91%E1%BB%99ng/</link>
    <description>Recent content in thế giới di động on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>alexblack2202@gmail.com (Phạm Duy Tùng)</managingEditor>
    <webMaster>alexblack2202@gmail.com (Phạm Duy Tùng)</webMaster>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Sun, 14 Apr 2019 00:13:00 +0300</lastBuildDate>
    <atom:link href="/tags/th%E1%BA%BF-gi%E1%BB%9Bi-di-%C4%91%E1%BB%99ng/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Thử làm ứng dụng tô màu ảnh xám thành ảnh màu sử dụng tensorflow</title>
      <link>/blog/2019-04-16-colorfull-grayscale-to-color/</link>
      <pubDate>Sun, 14 Apr 2019 00:13:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-16-colorfull-grayscale-to-color/</guid>
      <description>

&lt;h2 id=&#34;thực-hiện&#34;&gt;Thực hiện&lt;/h2&gt;

&lt;p&gt;Đây là một bài toán tiếp cận bằng Deep Learning, nên việc thu thập nhiều dữ liệu có ý nghĩa rất quang trọng trong việc đóng góp vào độ chính xác của mô hình. Ở đây, chúng ta sẽ download tập dữ liệu ảnh của &lt;a href=&#34;http://places2.csail.mit.edu/download.html&#34;&gt;http://places2.csail.mit.edu/download.html&lt;/a&gt; và sử dụng mạng UNet để huấn luyện mô hình.&lt;/p&gt;

&lt;h2 id=&#34;thu-thập-hình-ảnh-và-tiền-xử-lý&#34;&gt;Thu thập hình ảnh và tiền xử lý&lt;/h2&gt;

&lt;p&gt;Dữ liệu sẽ được download tại địa chỉ &lt;a href=&#34;http://data.csail.mit.edu/places/places365/train_256_places365challenge.tar&#34;&gt;http://data.csail.mit.edu/places/places365/train_256_places365challenge.tar&lt;/a&gt;. Tập trên có kích thước 108 GB. Đây là tập ảnh thuộc hệ màu RGB. Chúng ta sẽ chuyển tập ảnh trên về hệ màu grayscale làm ảnh gốc cho quá trình huấn luyện. Có một mẹo nhỏ cho chúng ta rút ngắn quá trình huấn luyện nhưng vẫn đảm bảo được độ chính xác của mô hình là ngoài kênh màu RGB mà chúng ta hay xài, trên thế giới còn có kênh màu HSV, trong đó nếu chúng ta chuyển một ảnh ở kênh màu RGB về hệ màu HSV, và bỏ đi các giá trị H, S, chỉ giữ lại giá trị V, thì chất lượng ảnh xám của nó gần như là tương đương với ảnh grayscale sử dụng công thức &amp;ldquo;thần thánh&amp;rdquo; mà chúng ta được học ở môn xử lý ảnh grayscale =0.30*R + 0.59*G + 0.11*B&lt;/p&gt;

&lt;p&gt;Vì vậy, thay vì việc input là giá trị xám của ảnh, output là giá trị của các kênh màu RGB, chúng ta sẽ chuyển đổi bài toán lại là input là giá trị xám, output là giá trị H và S.&lt;/p&gt;

&lt;p&gt;Mô hình mạng Unet&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/unet.PNG&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mạng UNet là một mạng neural network được dùng khá phổ biến trong các cuộc thi phân đoạn ảnh, độ chính xác của nó so với các thuật toán khác là vượt trội hoàn toàn. Ở đây, chúng ta có 2 hướng tiếp cận, một là build một mạng Unet và random init weight rồi huấn luyện nó, cách thứ hai là build mạng unet sử dụng pretrain model rồi huấn luyện. Bởi vì đặc trưng của các pretrain model hoạt động khá tốt và được huấn luyện trên tập dataset lớn, nên mình sẻ sử dụng nó ở bài viết này. Song song đó, mình sẽ cung cấp một giải pháp kèm theo sử để sử dụng mạng mà không dùng pretrain model.&lt;/p&gt;

&lt;p&gt;Ú tưởng chính của mạng UNet tựa tựa như auto-encoder, từ ảnh gốc ban đầu, chúng sẽ được nén thông tin lại qua các phép biến đổi Conv2D (như các chú thích màu sắc của mũi tên trong hình trên), sau đó sẽ được &amp;ldquo;giải nén&amp;rdquo; về lại ảnh gốc ban đầu. Việc huấn luyện coi như là hoàn tất 100% nếu ảnh gốc với ảnh giải nén là là giống nhau hoàn toàn.&lt;/p&gt;

&lt;p&gt;Bài viết sẽ được cập nhật&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dự đoán giá cổ phiếu bằng mô hình mạng Echo State Networks</title>
      <link>/blog/2019-04-04-predicting-stock-prices-with-echo-state-networks---copy/</link>
      <pubDate>Thu, 04 Apr 2019 00:13:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-04-predicting-stock-prices-with-echo-state-networks---copy/</guid>
      <description>

&lt;p&gt;Trong cuốn The West Wing Script Book của Aaron Sorkin, ông ấy đã có một câu như thế này &amp;ldquo;There (is) order and even great beauty in what looks like total chaos. If we look closely enough at the randomness around us, patterns will start to emerge.&amp;rdquo;. Mình xin phép không dịch câu nói trên ra, bởi vì mình dịch khá tệ, và câu nói này khá nổi tiếng (đã được trích dẫn khá nhiều trên các bài viết của các bloger khác). Nhưng câu nói đó khá phù hợp với môi trường chứng khoán, nơi mà mọi thứ đều không rõ ràng và khá &amp;ldquo;hỗn loạn&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;dự-đoán-chuỗi-thời-gian&#34;&gt;Dự đoán chuỗi thời gian&lt;/h2&gt;

&lt;p&gt;Giá cổ phiếu trên thị trường chứng khoán thường được quy vào bài toán là time series. Các công ty đầu tư hoặc các nhà nghiên cứu, các nhà đầu tư hiện nay thường sử dụng phương pháp stochastic hoặc các cải tiến của phương pháp stochastic (ví dụ mô hình ARIMA, RegARIMA,&amp;hellip;) để đưa ra các dự đoán hợp lý phù hợp với các giá trị quá khứ. Mục tiêu cuối cùng là tìm ra một mô hình khả dĩ nhất để phản ánh quy luật của thị trường và sử dụng nó để sinh ra lợi nhuận (trở nên giàu có hơn :)).&lt;/p&gt;

&lt;h2 id=&#34;các-thuộc-tính-của-time-series&#34;&gt;Các thuộc tính của time series&lt;/h2&gt;

&lt;p&gt;Một trong các thuộc tính của chuỗi thời gian là tính dừng (stationary). Một chuỗi time series được gọi là có tính dừng nếu các thuộc tính có ý nghĩa thống kê của nó (ví dụ như là trung bình, độ lệch chuẩn) không đổi theo thời gian. Ở đây, chúng ta luận bàn nho nhỏ một chút vì sao tính dừng rất quang trọng trong chuỗi thời gian.&lt;/p&gt;

&lt;p&gt;Trước hết, hầu hết các mô hình về time series hiện tại được xây dựng trên một giả định tính dừng của chuỗi thời gian. Có nghĩa là nếu chuỗi thời gian ở trong quá khứ có một hành vi nào đó, thì khả năng cao là nó sẽ lặp lại trong tương lai. Ngoài ra, các lý thuyết liên quan đến tính dừng của chuỗi time series đã được các nhà nghiên cứu khai thác một cách triệt để và dễ ràng implement hơn là các lý thuyết về non-stationary trong time series.&lt;/p&gt;

&lt;p&gt;Tính dừng được định nghĩa bằng các tiêu chí rõ ràng và nghiêm ngặt. Tuy nhiên, trong bài toán thực tế, chúng ta có thể giả định rằng một chuỗi time series được coi là có tính dừng nếu các thuộc tính thống kê không đổi theo thời gian, nghĩa là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Giá trị trung bình không thay đổi. Nếu giá trị trung bình thay đổi, chuỗi thời gian sẽ có khuynh hướng đi lên hoặc đi xuống. Hình ảnh bên dưới, mô tả trực quan một chuỗi thời gian có tính dừng (trung bình không thay đổi), và một chuỗi thời gian không có tính dừng (trung bình thay đổi).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/const_mean_stationary_series.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Giá trị phương sai không thay đổi. Thuộc tính này còn được gọi là đồng đẳng (homoscedasticity). Hình bên dưới mô tả một chuỗi có phương sai thay đổi (không có tính dừng) và một chuỗi có phương sai bất biến (có tính dừng).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/const_variance_stationary_series.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tính tự tương tự không phụ thuộc vào thời gian&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/autocorrelation_stationary_series.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;vì-sao-chúng-ta-lại-quan-tâm-đến-tính-dừng-của-dữ-liệu&#34;&gt;Vì sao chúng ta lại quan tâm đến tính dừng của dữ liệu&lt;/h2&gt;

&lt;p&gt;Chúng ta quan tâm đến tính dừng của dữ liệu, đơn giản là bởi vì nếu dữ liệu không có tính dừng, chúng ta không thể xây dựng mô hình chuỗi thời gian (như đã nói ở trên, các nghiên cứu hiện nay đều dựa trên một cơ sở là dữ liệu có tính dừng). Trong trường hợp bạn có trong tay dữ liệu thuộc dạng time series, và một tiêu chí nào đó trong 3 tiêu chí mình đã liệu kê ở trên bị vi phạm, suy ra là dữ liệu của bạn không có tính dừng. Bạn phải chuyển đổi dữ liệu bạn đang có để cho nó có tính dừng. May mắn rằng cũng có nhiều nghiên cứu thực hiện việc này, ví dụ như &amp;ldquo;khử xu hướng (detrending)&amp;rdquo;, khử sai biệt (differencing)&amp;hellip;&lt;/p&gt;

&lt;p&gt;Nếu bạn mới chỉ bắt đầu phân tích chuỗi thời gian, bạn sẽ thấy việc làm trên khá là stupid. Lý thuyết tốt nhất hiện nay cho chuỗi thời gian là chia nhỏ nó ra thành các thành phần như là xu hướng (linear trend), mùa vụ (seasonal), chu kỳ, và yếu tố ngẫu nhiên. Dự đoán cho từng phần một, sau đó lấy tổng chúng lại.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/arima.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Đối với những ai đã quen thuộc với biến đổi Fourier, thì sẽ dễ dàng &amp;ldquo;cảm&amp;rdquo; hơn cái mình vừa nói ở trên.&lt;/p&gt;

&lt;h2 id=&#34;cách-xác-định-tính-dừng-của-dữ-liệu&#34;&gt;Cách xác định tính dừng của dữ liệu&lt;/h2&gt;

&lt;p&gt;Khá khó để xác định một biểu đồ chuỗi time series có tính dừng hay không (quan sát biểu đồ bằng mắt). Cho nên chúng ta sẽ sử dụng kiểm định Dickey-Fuller. Đây là một kiểm định thống kê để kiểm tra xem chuỗi dữ liệu có tính dừng hay không. Với giả thuyết null là chuỗi time series là một chuỗi không có tính dừng. Nếu giá trị  nhỏ hơn một ngưỡng p-value nào đó (thường là 0.05), chúng ta có quyền bác bỏ giả định null, và nói rằng chuỗi thời gian đang có là có tính dừng. Ở bài viết này, mình không đề cập đến mô hình kiểm định - vốn được học trong môn xác xuất thống kê. Các bạn có nhu cầu tìm hiểu thì có thể search trên google hoặc là xem lại sách xác suất thống kê.&lt;/p&gt;

&lt;h2 id=&#34;phương-pháp-dự-đoán-chuỗi-thời-gian-cơ-bản&#34;&gt;Phương pháp dự đoán chuỗi thời gian cơ bản&lt;/h2&gt;

&lt;p&gt;Phương pháp cơ bản nhất, đơn giản nhất, và để áp dụng nhất dược sử dụng để dự đoán chuỗi thời gian là moving average. Mô hình này thực hiện tính trung bình của t giá trị cuối cùng làm giá trị dự đoán của điểm tiếp theo. Ví dụ như để dự đoán giá chứng khoán của ngày thứ 2 của tuần tiếp theo, chúng ta sẽ lấy trung bình giá đóng của của 5 ngày trước đó (giá từ thứ hai đến thứ sáu tuần này).&lt;/p&gt;

&lt;p&gt;Đến đây, các bạn đã có một số hiểu biết về time series. Một mô hình khá nổi tiếng là ARIMA đã được sử dụng nhiều để phân tích và dự báo. Cách thực hiện của mô hình trên được trình bày tóm gọn trong hình mô tả bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/arima1.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;phương-pháp-dự-đoán-dựa-vào-mạng-neural-network&#34;&gt;Phương pháp dự đoán dựa vào mạng neural network&lt;/h2&gt;

&lt;p&gt;Thực tế, có rất nhiều mạng neural network đã được áp dụng để dự đoán mô hình chứng khoán. Các bạn có thể tìm đọc lại các bài viết trước đây của mình về sử dụng LSTM trong dự báo chứng khoán. Mô hình chứng khoán bằng mạng neural network nói chung phải đối mặt với một vấn đề khá &amp;ldquo;xương xẩu&amp;rdquo; là xử lý nhiễu và vanishing gradients. Trong đó, việc xử lý vanishing gradients là quan trọng nhất. Bản chất của mạng neural network là tối ưu hoá hàm lan truyền ngược bằng cách sử dụng đạo hàm giữa các lớp layer để chúng &amp;lsquo;học&amp;rsquo;. Trải qua nhiều layer, giá trị của đạo hàm sẽ càng ngày nhỏ dần vào xấp xỉ bằng 0. Giả sử chúng ta có một mô hình có 100 lớp hidden layer, chúng ta nhân 100 lần số 0.1 với nhau và boom, giá trị cuối cùng chung ta nhận được là 0, nghĩa là chúng ta chẳng học được cái gì cả.&lt;/p&gt;

&lt;p&gt;May mắn thay, tới thời điểm hiện tại, chúng ta có 3 cách để xử lý vấn đề trên:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Clipping gradients&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LSTM (Long Short Term Memory) hoặc GRU (Gate Recurrent Units)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Echo states RNNs&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kỹ thuật clipping gradients sử dụng một mẹo là khi giá trị đạo hàm quá lớn hoặc quá nhỏ, chúng ta sẽ không lấy đạo hàm nữa. Kỹ thuật này thoạt nhìn có vẻ hay, nhưng nó không thể ngăn chúng ta mất mát thông tin và đây là một ý tưởng khá tệ.&lt;/p&gt;

&lt;p&gt;RNN (LSTM hoặc GRU) là một kỹ thuật khác là điều chỉnh các kết nối theo một vài quy luật nhất định, ví dụ output của layer tầng 1 có thể là input của layer tầng 10, chứ không nhất thiết là input của layer tầng 2 như cách thông thường. Kỹ thuật này khá tốt về mặt lý thuyết. Tuy nhiên, có một vấn đề khá lớn khi sử dụng là chúng ta phải tính toán kỹ các kết nối để đảm bảo hệ thống hoạt động ổn đinh. Mô hình được xây dựng trên kỹ thuật này khá bự, làm cho thuật toán chạy chậm. Ngoài ra, tính hội tụ của thuật toán không được đảm bảo. Mô hình LSTM đơn giản mình có để ở hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/lstm.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mạng echo states network, là một mô hình mới được nghiên cứu gần đây, bản chất nó là một mảng recurrent neural network với các hidden layer liên kết &amp;ldquo;lỏng lẻo&amp;rdquo; với nhau. Lớp này được gọi là &amp;lsquo;reservoir&amp;rsquo; (như hình mô tả bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/echo_state_network.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Trong mô hình mạng  echo state network, chúng ta chỉ cần huấn luyện lại trọng số của lớp output, việc này giúp chúng ta rút ngắn thời gian huấn luyện mô hình, và tăng tốc qusa trình training.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-mạng-echo-state-networks&#34;&gt;Sử dụng mạng Echo State Networks&lt;/h2&gt;

&lt;p&gt;Về nguyên lý hoạt động của mô hình này, mình sẽ không đề cập ở đây. Chủ đề về mạng Echo State Networks mình sẽ nghiên cứu kỹ lưỡng và đề cập ở trong bài viết sắp tới. Mục tiêu của bài viết này là sử dụng mô hình Echo State Networks trong bài toán time series.&lt;/p&gt;

&lt;h4 id=&#34;dự-doán-chuỗi-time-series&#34;&gt;Dự doán chuỗi time series&lt;/h4&gt;

&lt;p&gt;Trước tiên, chúng ta sẽ import một số thư viện cần thiết, thư viện ESN đã có sẵn tại đường dẫn pyESN, các bạn download về rồi dùng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings(&#39;ignore&#39;)

# This is the library for the Reservoir Computing got it by: https://github.com/cknd/pyESN
from pyESN import ESN 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo chúng ta sẽ đọc file&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
data = open(&amp;quot;amazon.txt&amp;quot;).read().split()
data = np.array(data).astype(&#39;float64&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta sẽ xây dựng một mô hình ESN đơn giản&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
n_reservoir= 500
sparsity=0.2
rand_seed=23
spectral_radius = 1.2
noise = .0005


esn = ESN(n_inputs = 1,
      n_outputs = 1, 
      n_reservoir = n_reservoir,
      sparsity=sparsity,
      random_state=rand_seed,
      spectral_radius = spectral_radius,
      noise=noise)
      
      ```
      
Để đơn giản, mình sẽ tạo mô hình với dữ liệu tào lao như sau:input là một vector toàn số 1, output là các điểm dữ liệu của mình. Cho mô hình ESN học với số lượng phần tử là 1500, sau đó sẽ dự đoán 10 điểm dữ liệu tiếp theo. Với bước nhảy là 10, lặp 10 lần. Sau quá trình lặp, mình thu được 100 điểm dự đoán


```python
trainlen = 1500
future = 10
futureTotal=100
pred_tot=np.zeros(futureTotal)

for i in range(0,futureTotal,future):
    pred_training = esn.fit(np.ones(trainlen),data[i:trainlen+i])  # dữ liệu từ ngày i đến ngày i + trainlen
    prediction = esn.predict(np.ones(future))
    pred_tot[i:i+future] = prediction[:,0] # dự đoán cho ngày i+ trainlen + 1 đến ngày i + trainlen + future
    
    
    ```
    
Vẽ mô hình cùi mía của mình mới làm lên để xem dữ liệu dự đoán và dữ liệu thực tế chênh lệch như thế nào

```python
plt.figure(figsize=(16,8))
plt.plot(range(1000,trainlen+futureTotal),data[1000:trainlen+futureTotal],&#39;b&#39;,label=&amp;quot;Data&amp;quot;, alpha=0.3)
#plt.plot(range(0,trainlen),pred_training,&#39;.g&#39;,  alpha=0.3)
plt.plot(range(trainlen,trainlen+futureTotal),pred_tot,&#39;k&#39;,  alpha=0.8, label=&#39;Free Running ESN&#39;)

lo,hi = plt.ylim()
plt.plot([trainlen,trainlen],[lo+np.spacing(1),hi-np.spacing(1)],&#39;k:&#39;, linewidth=4)

plt.title(r&#39;Ground Truth and Echo State Network Output&#39;, fontsize=25)
plt.xlabel(r&#39;Time (Days)&#39;, fontsize=20,labelpad=10)
plt.ylabel(r&#39;Price ($)&#39;, fontsize=20,labelpad=10)
plt.legend(fontsize=&#39;xx-large&#39;, loc=&#39;best&#39;)
sns.despine()
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/echo_state_network_p1.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Độ phức tạp của mô hình là khá nhỏ khi so với mô hình RNN. Lý do là về bản chất, chúng ta chỉ huấn luyện trên trọng số của output layer, nó là một hàm tuyến tính. Do vậy, độ phức tạp tính toán chỉ giống như là việc tính một hàm hồi quy tuyến tính. Trong thực tế, độ phức tạp tính toán sẽ là O(N) với N là ố lượng hidden unit trong reservoir.&lt;/p&gt;

&lt;h4 id=&#34;tối-ưu-hoá-các-tham-số-hyper-parameters&#34;&gt;Tối ưu hoá các tham số Hyper parameters&lt;/h4&gt;

&lt;p&gt;Ở phần trước, chúng ta set đại các tham số spectral_radius = 1.2 và noise = .0005. Trong thực tế, chúng ta phải tìm các siêu tham số này bằng cách tìm ra mô hình trả về MSE là nhỏ nhất.&lt;/p&gt;

&lt;p&gt;Sử dụng kỹ thuật Grid Search với ngưỡng spectrum_radius nằm trong đoạn [0.5, 1.5] và noise nằm trong đoạn  noise [0.0001, 0.01], chú ý là các bạn có thể search ở đoạn lớn hơn. Kết quả thu được:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def MSE(yhat, y):
    return np.sqrt(np.mean((yhat.flatten() - y)**2))
    
    n_reservoir= 500
sparsity   = 0.2
rand_seed  = 23
radius_set = [0.9,  1,  1.1]
noise_set = [ 0.001, 0.004, 0.006]

radius_set = [0.5, 0.7, 0.9,  1,  1.1,1.3,1.5]
noise_set = [ 0.0001, 0.0003,0.0007, 0.001, 0.003, 0.005, 0.007,0.01]



radius_set_size  = len(radius_set)
noise_set_size = len(noise_set)

trainlen = 1500
future = 2
futureTotal= 100

loss = np.zeros([radius_set_size, noise_set_size])

for l in range(radius_set_size):
    rho = radius_set[l]
    for j in range(noise_set_size):
        noise = noise_set[j]

        pred_tot=np.zeros(futureTotal)

        esn = ESN(n_inputs = 1,
          n_outputs = 1, 
          n_reservoir = n_reservoir,
          sparsity=sparsity,
          random_state=rand_seed,
          spectral_radius = rho,
          noise=noise)

        for i in range(0,futureTotal,future):
            pred_training = esn.fit(np.ones(trainlen),data[i:trainlen+i])
            prediction = esn.predict(np.ones(future))
            pred_tot[i:i+future] = prediction[:,0]
        
        loss[l, j] = MSE(pred_tot, data[trainlen:trainlen+futureTotal])        
        print(&#39;rho = &#39;, radius_set[l], &#39;, noise = &#39;, noise_set[j], &#39;, MSE = &#39;, loss[l][j] )
        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 20.367056799629353)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 22.44956008062169)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 24.574909979223666)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 25.862558649155638)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 29.882933676750657)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 32.63942614291128)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 36.441245548726)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 44.77637915282457)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 19.560517902720054)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 20.12742795009036)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 20.81801427735713)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 21.26142619965559)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 23.270880660885513)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 26.061347331527354)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 30.298361979419834)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 39.17074955771047)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 18.612970860501118)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 18.681815816990774)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.835785386862582)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.982346096338105)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 20.81632098844061)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 24.60968377490799)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 30.231007189936882)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 41.28587340583505)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 18.23852181110818)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 18.27010615150326)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.36078059388596)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.47920006882226)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 20.613227951906246)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 25.153712109142973)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 31.700838835741898)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 44.23736750779224)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 17.981571756431556)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 18.009398312163942)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.09054736889828)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.218795249276663)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 20.82610561349463)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 26.272452530336505)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 33.91532767431614)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 48.22002405965967)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 17.72839068197909)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 17.799908079894703)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 17.92917208443474)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.143905288756557)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 22.20343747458126)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 30.05977704513729)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 40.56654468067572)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 59.43231026660687)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 17.627409489404897)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 17.835052829116567)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.100099619981393)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.481406587483956)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 24.887601182697498)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 36.34166374510305)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 50.99612645577753)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 75.94229622771246)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả thu được là giá trị MSE tốt nhất là spectrum radius =  1.5 và nnoise  = 0.0001&lt;/p&gt;

&lt;p&gt;Thử dự đoán giá cổ phiếu của tập đoàn thế giới di động (Mã cổ phiếu MWG) xem sao&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/echo_state_network_mwg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ở hình trên, mình không tiến hành grid search mà lấy lại các hyper parameters cũ để huấn luyện mô hình. Kết quả như hình trên  mình thấy cũng khá tốt rồi, nên mình không tiến hành grid search lại để tìm kết quả tốt hơn.&lt;/p&gt;

&lt;p&gt;Dựa vào kết quả chúng ta thu được, có thể nói rằng mô hình ESN dự đoán khá tốt dữ liệu thuộc dạng time series với độ hỗn loạn cao. Đây là một kết luận nhỏ của mình dựa vào bằng chứng trên việc mình test trên tập dữ liệu ngẫu nhiên mà mình có.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>