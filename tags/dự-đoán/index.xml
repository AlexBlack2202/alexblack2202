<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dự đoán on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/d%E1%BB%B1-%C4%91o%C3%A1n/</link>
    <description>Recent content in dự đoán on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>alexblack2202@gmail.com (Phạm Duy Tùng)</managingEditor>
    <webMaster>alexblack2202@gmail.com (Phạm Duy Tùng)</webMaster>
    <copyright>&amp;copy; 2018 Phạm Duy Tùng. Website chia sẻ kiến thức của Phạm Duy Tùng và Đặng Thị Hằng. Vui lòng liên hệ email alexblack2202@gmail.com nếu bạn có thông tin cần trao đổi.</copyright>
    <lastBuildDate>Fri, 03 May 2019 00:12:00 +0300</lastBuildDate>
    <atom:link href="/tags/d%E1%BB%B1-%C4%91o%C3%A1n/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>PredictionIO Phần 1 - Hướng dẫn cài đặt</title>
      <link>/blog/2019-05-04-setup-predictio/</link>
      <pubDate>Fri, 03 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-04-setup-predictio/</guid>
      <description>

&lt;h1 id=&#34;1-giới-thiệu-về-predictionio&#34;&gt;1. Giới thiệu về PredictionIO&lt;/h1&gt;

&lt;p&gt;PredictionIO là một &amp;ldquo;open source Machine Learning Server built on top of a state-of-the-art open source stack&amp;rdquo; giúp cho các developers và các data scientists tạo ra các engine dự đoán trong học máy. PredictionIO giúp chúng ta&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Xây dựng và triển khai các ứng dụng, dịch vụ một cách nhanh chóng bằng cách tuỳ chỉnh lại các template đã sẵn có.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Trả lời các câu truy vấn động trong thời gian thực.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;huấn luyện và so sánh/đánh giá nhiều mô hình khác nhau dễ dàng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hợp nhất hoá dữ liệu từ nhiều nền tảng khác nhau hoặc trong thời gian thực để thực hiện phân tích dự đoán.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hỗ trợ các thư viện máy học và xử lý dữ liệu như Spark MLLib và OpenNLP&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tự xây dựng, triển khai, customize một mô hình machine learning&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2-cơ-chế-hoạt-động-của-predictionio&#34;&gt;2. Cơ chế hoạt động của PredictionIO&lt;/h1&gt;

&lt;p&gt;PredictionIO bao gồm các thành phần sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PredictionIO platform: là nền tảng open source được apache xây dựng sẵn giúp chúng ta triển khai, xây dựng, đánh giá các mô hình máy học.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Event Server: là nơi giúp chúng ta chuẩn hoá các sự kiện từ nhiều nguồn khác nhau&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Template Gallery: là nơi chúng ta download các engine template máy học về. PredictionIO hỗ trợ cho chúng ta rất nhiều template mẫu khác nhau. Chúng ta sẽ lần lượt tìm hiểu và implement ở các bài viết tiếp theo.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;event-server&#34;&gt;Event Server&lt;/h3&gt;

&lt;p&gt;PredictionIO Event Server chịu trách nhiệu thu thập dữ liệu từ các ứng dụng của bạn. Bạn có thể nhìn kỹ hơn ở hình bên dưới, các ứng dụng web, mobile app &amp;hellip; khi người dùng tương tác sẽ phát sinh các sự kiện (Event Data), ví dụ sự kiện người dùng thêm 1 đơn hàng vào giỏ hàng, người dùng xem sản phẩn A, người dùng xem sản phẩm C sau khi xem sản phẩm A&amp;hellip; Event Server sẽ ghi nhận lại đống dữ liệu này, chuẩn hoá lại. PredictionIO engine sau đó sẽ xây dựng mô hình dự đoán dựa trên các dữ liệu chúng ta thu thập được. Sau khi bạn có được mô hình tối ưu, chúng ta sẽ deploy các predict webservice, lắng nghe các truy vấn từ các ứng dụng và trả về kết quả trong thời gian thực.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/predictionio-event.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 1: Event server trong predictionio&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Event Server sẽ thu thập dữ liệu của bạn trong thời gian thực hoặc theo chu kỳ. Sau đó, nó sẽ chuẩn hoá dữ liệu hỗn độn của bạn từ nhiều nguồn khác nhau thành một dạng chuẩn chung. Event Server chủ yếu phục vụ hai mục đính chính:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cung cấp dữ liệu cho các engine để huấn luyện và đánh giá&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cung cấp dữ liệu dạng chuẩn để data analysis&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cũng giống như một database server, Event Server có thể được sử dụng để phục vụ cho nhiều ứng dụng khác nhau. Dữ liệu được phân tách cho các ứng dụng bằng &amp;ldquo;app_name&amp;rdquo; duy nhất. Cái này sẽ nói lại lúc xây dựng ứng dụng ở bên dưới.&lt;/p&gt;

&lt;p&gt;Khi một Event Server được triển khai, bạn có thể gửi dữ liệu cho một &amp;lsquo;app_name&amp;rsquo; cụ thể nào đó, app-name được định danh bằng access key. Dữ liệu được gửi đến Event Server sử dụng &lt;strong&gt;EventAPI&lt;/strong&gt; sử dụng giao thức http (tham khảo thêm ở &lt;a href=&#34;https://predictionio.apache.org/datacollection/eventapi/&#34;&gt;https://predictionio.apache.org/datacollection/eventapi/&lt;/a&gt;) hoặc sử dụng các PredictionIO SDK. Tham khảo thêm các SDK ở &lt;a href=&#34;https://predictionio.apache.org/sdk/&#34;&gt;https://predictionio.apache.org/sdk/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trong một số trường hợp, bạn muốn engine đọc dữ liệu từ một datastore nào đó thay vì Event Server. Bạn có thể thực hiện thông qua hướng dẫn ở &lt;a href=&#34;https://predictionio.apache.org/start/customize/&#34;&gt;https://predictionio.apache.org/start/customize/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;engine&#34;&gt;Engine&lt;/h3&gt;

&lt;p&gt;Engine là nơi chịu trách nhiệu đưa ra các quyết định. Nó gồm một hoặc nhiều thuật toán học máy học khác nhau. Các Engine sẽ huấn luyện dữ liệu và xây dựng các mô hình dự đoán. Sau đó sẽ phát triển thành các webservice. Các webservice sẽ nhận các truy vấn từ ứng dụng, dự đoán và trả về kết quả cho ứng dụng.&lt;/p&gt;

&lt;p&gt;PredictionIO&amp;rsquo;s  cung cấp cho chúng ta rất nhiều các template khác nhau đáp ứng gần như là đẩy đủ các mô hình máy học mà chúng ta cần. Bạn có thể dễ dàng tạo một mô hình máy học từ các template. Các thành phần của một template dược đặt tên là &lt;strong&gt;Data Source, Data Preparator, Algorithm(s), Serving&lt;/strong&gt;, các bạn có thể dễ dàng customize lại tuỳ thuộc nhu cầu của bạn.&lt;/p&gt;

&lt;h1 id=&#34;3-cài-đặt-predictionio-trên-môi-trường-ubuntu&#34;&gt;3. Cài đặt PredictionIO trên môi trường Ubuntu&lt;/h1&gt;

&lt;p&gt;Trong thời đại docker, các bạn có thể cài đặt PredictionIO dựa vào các docker được xây dựng sẵn đầy rẫy trên mạng, chúng giúp bạn đỡ tốn công sức hơn. Tuy nhiên, trong bài viết này, mình sẽ cài đặt từng thành phần PredictiIO trên ubuntu, không sử dụng docker.&lt;/p&gt;

&lt;h3 id=&#34;download-và-build-apache-prediction-io&#34;&gt;Download và build Apache Prediction IO&lt;/h3&gt;

&lt;p&gt;Chúng ta sẽ download Prediction IO từ trang github chính chủ. Phiên bản hiện tại là 0.14.0. Các bạn có thể lưu dữ liệu ở đâu tuỳ ý các bạn. Mình lưu ở thư mục &lt;strong&gt;/data/pio&lt;/strong&gt;. Và trong suốt bài viết này, mình sẽ lưu các thứ liên quan trong thư mục /data/pio. Các bạn có cài đặt theo hướng dẫn của mình thì nhớ sửa lại cho đúng đường dẫn của các bạn. Chúng ta sẽ clone nguồn từ trang github predictionio. và sẽ switch qua branch release. Đây là branch chính thành phẩm, các branch khác đang trong giai đoạn phát triển nên có thể build không được. Lúc các bạn làm có thể nó đã phát triển lên bản 15, 16 hoặc 1.0 gì đó rồi. Các bạn cứ tự tin sử dụng phiên bản mới nhất.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;git clone https://github.com/apache/predictionio.git
git checkout release/0.14.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;biên-dịch-prediction-io&#34;&gt;Biên dịch Prediction IO&lt;/h3&gt;

&lt;p&gt;Sau khi tải về bộ nguồn của Prediction IO, chúng ta sẽ tiền hành biên dịch. Quá trình biên dịch sẽ xảy ra khá lâu, các bạn kiên nhẫn chờ đợi&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cd predictionio
./make-distribution.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết thúc quá trình biên dịch, các bạn sẽ thấy dòng chữ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PredictionIO binary distribution created at PredictionIO-0.14.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vậy là chúng ta đã thành công. Việc tiếp theo là giải nén file PredictionIO-0.14.0.tar.gz để sử dụng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tar xvzf PredictionIO-0.14.0.tar.gz -C /data/pio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nhắc lại 1 lần nữa là do thời điểm hiện tại mình viết bài viết này, PredictionIO mới release bản 0.14.0 nên file tập tin sẽ là PredictionIO-0.14.0.tar.gz. Các bạn nhớ giải nén đúng với tên file ứng với phiên bản PredictionIO tương ứng nhé.&lt;/p&gt;

&lt;h3 id=&#34;download-và-giải-nén-các-dependencies&#34;&gt;Download và giải nén các Dependencies&lt;/h3&gt;

&lt;p&gt;Mình sẽ sử dụng Spark, ElasticSearch, Hbase và zookeeper, nên mình download hết về. Mình có thói quen sử dụng phiên bản mới nhất. Nên mình lên trang chủ và lấy link download mới nhất của chúng thôi. Tất cả các Dependencies mình dùng đều được bỏ vào trong thư mục vendors&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
cd PredictionIO-0.14.0
mkdir vendors
cd vendors
wget https://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz

wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.9.tar.gz

wget https://www.apache.org/dyn/closer.lua/hbase/2.1.4/hbase-2.1.4-bin.tar.gz

wget https://www-us.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz

tar xvzf spark-2.4.2-bin-hadoop2.7.tgz

tar xvzf elasticsearch-5.6.9.tar.gz

tar xvzf hbase-2.1.4-bin.tar.gz

tar xvzf zookeeper-3.4.14/zookeeper-3.4.14.tar.gz

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cấu-hình-chương-trình&#34;&gt;Cấu hình chương trình&lt;/h3&gt;

&lt;h5 id=&#34;cấu-hình-dependency&#34;&gt;Cấu hình dependency&lt;/h5&gt;

&lt;p&gt;Chúng ta sẽ cấu hình một chút để PredictionIO nhận ra các dependency của mình và cấu hình các dependency&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ chỉnh sửa file &lt;strong&gt;hbase-site.xml&lt;/strong&gt; của HBase&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nano /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/conf/hbase-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thay đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;configuration&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bằng đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:///data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.zookeeper.property.dataDir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/data/pio/PredictionIO-0.14.0/vendors/zookeeper-3.4.14&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ add đường dẫn java cho hbase&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nano /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/conf/hbase-env.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thêm đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;các bạn hãy thay đường dẫn java tương ứng với đường dẫn trong máy bạn. Nếu chưa có java thì các bạn hãy cài vào, nếu các bạn đã cài java mà không biết nó nằm ở đâu, các bạn có thể gọi lệnh bên dưới để xem đường dẫn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;update-alternatives --config java
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để chắc chắn rằng trong máy của bạn có cài java bạn hãy gọi lện &lt;strong&gt;java -version&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ví dụ trong máy mình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$java -version
openjdk version &amp;quot;1.8.0_191&amp;quot;
OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Các bạn cố gắng sử dụng phiên bản java mới nhất. Nó sẽ tương thích tốt hơn với phiên bản mới nhất của HBase, hoặc đọc phiên bản java đề nghị trong trang chủ HBase. Tránh trường hợp sử dụng phiên bản java quá cũ HBase không hỗ trợ.&lt;/p&gt;

&lt;h5 id=&#34;cấu-hình-prediction-io&#34;&gt;Cấu hình Prediction IO&lt;/h5&gt;

&lt;p&gt;Chỉnh sửa file &lt;strong&gt;pio-env.sh&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
nano /data/pio/PredictionIO-0.14.0/conf/pio-env.sh

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mặc định PredictionIO sử dụng PosgresSQl làm event server. Mình không dùng nó mà thay thế bằng HBASE và ELASTICSEARCH.&lt;/p&gt;

&lt;p&gt;Một số thay đổi mình sẽ liệt kê bên dưới&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;SPARK_HOME=$PIO_HOME/vendors/spark-2.3.2-bin-hadoop2.7

HBASE_CONF_DIR=$PIO_HOME/vendors/hbase-2.1.4/conf

PIO_STORAGE_REPOSITORIES_METADATA_NAME=pio_meta
PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH

PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=pio_event
PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE

PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_model
PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS

#Comment các dòng này lại, do không dùng postgres
# PIO_STORAGE_SOURCES_PGSQL_PASSWORD accordingly
# PIO_STORAGE_SOURCES_PGSQL_TYPE=jdbc
# PIO_STORAGE_SOURCES_PGSQL_URL=jdbc:postgresql://localhost/pio
# PIO_STORAGE_SOURCES_PGSQL_USERNAME=pio
# PIO_STORAGE_SOURCES_PGSQL_PASSWORD=pio

PIO_STORAGE_SOURCES_ELASTICSEARCH_HOME=$PIO_HOME/vendors/elasticsearch-5.6.9
PIO_STORAGE_SOURCES_HBASE_HOME=$PIO_HOME/vendors/hbase-2.1.4
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-khởi-chạy-hệ-thống&#34;&gt;4.Khởi chạy hệ thống&lt;/h1&gt;

&lt;p&gt;Chúng ta sẽ add path của PredictIO vào biến môi trường để sử dụng cho các lần sau&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
nano ~/.bashrc
erport PATH=/data/pio/PredictionIO-0.14.0/bin:$PATH

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hoặc có thể add path trong mỗi session&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PATH=$PATH:/data/pio/PredictionIO-0.14.0/bin; export PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ cấp quyền cho thư mục PredictionIO&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sudo chmod -R 775 /data/pio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nếu không cấp quyền write cho thư mục thì PredictionIO không thể write log file được.&lt;/p&gt;

&lt;p&gt;Chạy PredictionIO Server bằng cách gọi câu lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio-start-all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Stopping PredictionIO Event Server...
Stopping HBase...
stopping hbase.............
Stopping Elasticsearch...
tgdd@U1604:/data/pio/PredictionIO-0.14.0/bin$ pio-start-all
Starting Elasticsearch...
Starting HBase...
running master, logging to /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/bin/../logs/hbase-tgdd-master-U1604.out
Waiting 10 seconds for Storage Repositories to fully initialize...
Starting PredictionIO Event Server...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để kiểm tra hệ thống khi start có lỗi lầm gì không, chúng ta sử dụng lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[INFO] [Management$] Inspecting PredictionIO...
[INFO] [Management$] PredictionIO 0.14.0 is installed at /data/pio/PredictionIO-0.14.0
[INFO] [Management$] Inspecting Apache Spark...
[INFO] [Management$] Apache Spark is installed at /data/spark-2.3.2-bin-hadoop2.7
[INFO] [Management$] Apache Spark 2.3.2 detected (meets minimum requirement of 2.0.2)
[INFO] [Management$] Inspecting storage backend connections...
[INFO] [Storage$] Verifying Meta Data Backend (Source: ELASTICSEARCH)...
[INFO] [Storage$] Verifying Model Data Backend (Source: LOCALFS)...
[INFO] [Storage$] Verifying Event Data Backend (Source: HBASE)...
[INFO] [Storage$] Test writing to Event Store (App Id 0)...
[INFO] [HBLEvents] The table pio_event:events_0 doesn&#39;t exist yet. Creating now...
[INFO] [HBLEvents] Removing table pio_event:events_0...
[INFO] [Management$] Your system is all ready to go.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bạn thấy dòng chữ &lt;strong&gt;[INFO] [Management$] Your system is all ready to go.&lt;/strong&gt; thì yên tâm, hệ thống đã chạy thành công.&lt;/p&gt;

&lt;p&gt;Để stop hệ thống, các bạn gọi lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio-stop-all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả khi stop&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Stopping PredictionIO Event Server...
Stopping HBase...
stopping hbase.............
Stopping Elasticsearch...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vậy là chúng ta đã tiến hành cài đặt thành công PredictionIO Server rồi. Hẹn gặp bạn ở bài thứ hai, cài đặt các template cho PredictionIO và tiến hành dự đoán.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PredictionIO Phần 2 - Cài đặt chương trình demo</title>
      <link>/blog/2019-05-07-predictio-mini-demo1/</link>
      <pubDate>Fri, 03 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-07-predictio-mini-demo1/</guid>
      <description>

&lt;h1 id=&#34;1-tạo-chương-trình-đầu-tiên-bằng-predictionio&#34;&gt;1. Tạo chương trình đầu tiên bằng PredictionIO&lt;/h1&gt;

&lt;p&gt;Đầu tiên, các bạn hãy tạo thư mục template ở đâu đó. Mình sẽ tạo ở trong thư mục /data/pio. Đường dẫn của mình sẽ là /data/pio/template&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mdkir /data/pio/template
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ clone templte trên github về, các bạn thực hiện lệnh sau&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;git clone https://github.com/apache/predictionio-template-recommender.git
cd predictionio-template-recommender
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ tạo một app đầu tiên, mình đặt tên là ourrecommendation, các bạn thích đặt tên gì thì đặt nha.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio app new ourrecommendation

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để liệt kê danh sách app đang có trong hệ thống, các bạn dùng lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio app list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả trong máy mình tại thời điểm viết bài là&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[INFO] [Pio$]                 Name |   ID |                                                       Access Key | Allowed Event(s)
[INFO] [Pio$]    ourrecommendation |    1 | Z93rJZ7Xq2pXiQwVC6B5nRK6jRykcfyMI5huOijKbdDJeUeKEnVT-ph5nabptIX1 | (all)
[INFO] [Pio$] Finished listing 1 app(s).

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mình mới tạo app đầu tiên tên là ourrecommendation nên chỉ có 1 app trong hệ thống. Sau này sẽ có nhiều hơn. À, sau khi tạo app, thì hệ thống sẽ generate tự động cho app với một Access Key, ví dụ access key của app ourrecommendateion của mình là Z93rJZ7Xq2pXiQwVC6B5nRK6jRykcfyMI5huOijKbdDJeUeKEnVT-ph5nabptIX1. Các bạn sẽ có access key khác với access key của mình, nên đừng copy của mình về làm gì hết :).&lt;/p&gt;

&lt;p&gt;Sau khi khởi tạo app xong, chúng ta sẽ import data vào hệ thống. Ở đây, mình sẽ download dữ liệu mẫu từ nguồn &lt;a href=&#34;https://gist.githubusercontent.com/vaghawan/0a5fb8ddb85e03631dd500d7c8f0677d/raw/17487437dd8269588d9dd1ac859b129a43842ba5/data-sample.json&#34;&gt;https://gist.githubusercontent.com/vaghawan/0a5fb8ddb85e03631dd500d7c8f0677d/raw/17487437dd8269588d9dd1ac859b129a43842ba5/data-sample.json&lt;/a&gt;. Sau khi download về các bạn import dữ liệu vào hệ thống bằng lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio import — appid 1 — input data-sample.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với appod 1 là id của ourrecommendation chúng ta vừa mới tạo. Nếu quên appid, các bạn có thể xem lại bằng lệnh &lt;strong&gt;pio app list&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Sau khi import thành công, chúng ta sẽ thay đổi giá trị của trường appname trong file engine.json thành tên của app mình, là ourrecommendation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nano engine.json

{
  &amp;quot;id&amp;quot;: &amp;quot;default&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;Default settings&amp;quot;,
  &amp;quot;engineFactory&amp;quot;: &amp;quot;org.example.recommendation.RecommendationEngine&amp;quot;,
  &amp;quot;datasource&amp;quot;: {
    &amp;quot;params&amp;quot; : {
      &amp;quot;appName&amp;quot;: &amp;quot;ourrecommendation&amp;quot;
    }
  },
  &amp;quot;algorithms&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;als&amp;quot;,
      &amp;quot;params&amp;quot;: {
        &amp;quot;rank&amp;quot;: 10,
        &amp;quot;numIterations&amp;quot;: 20,
        &amp;quot;lambda&amp;quot;: 0.01,
        &amp;quot;seed&amp;quot;: 3
      }
    }
  ]
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một lưu ý quang trọng là giá trị &amp;ldquo;org.example.recommendation.RecommendationEngine&amp;rdquo; trong &amp;ldquo;engineFactory&amp;rdquo; là của hệ thống. Và bạn đừng sửa, thay đổi chúng. Nói chung là ngoài giá trị của &amp;ldquo;appName&amp;rdquo; ra, bạn không nên thay đổi bất kỳ thức gì khác trong file  engine.json.&lt;/p&gt;

&lt;p&gt;Sau khi import file thành công. Chúng ta sẽ build app. Lệnh build có tác dụng kiểm tra lại hệ thống đã được cấu hình đúng và đủ chưa.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio build

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nếu build thành công, chúng ta sẽ thấy dòng chữ này.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
[INFO] [Engine$] Build finished successfully.
[INFO] [Pio$] Your engine is ready for training.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi build thành công, chúng ta sẽ tiến hành huấn luyện mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio build

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và chờ đợi dòng này xuất hiện&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
[INFO] [CoreWorkflow$] Training completed successfully.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dự đoán doanh số bán của các cửa hàng walmart</title>
      <link>/blog/2019-04-17-walmart-store-sales-forecasting/</link>
      <pubDate>Mon, 15 Apr 2019 00:09:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-17-walmart-store-sales-forecasting/</guid>
      <description>

&lt;h1 id=&#34;nghiên-cứu-dữ-liệu&#34;&gt;Nghiên cứu dữ liệu&lt;/h1&gt;

&lt;p&gt;Trong thực tế, Walmart đã chạy các chương trình khuyến mãi trong các ngày lễ lớn trong năm. Có 4 ngày lễ lớn đó là Siêu cúp bóng bầu dục Mỹ (Super Bowl - tổ chức vào chủ nhật đầu tiên của tháng Hai. Đây là một sự kiện thể thao lớn và ngày tổ chức Super Bowl được người Mỹ coi là ngày lễ quốc gia của Hoa Kỳ (theo wiki &lt;a href=&#34;https://vi.wikipedia.org/wiki/Super_Bowl)&#34;&gt;https://vi.wikipedia.org/wiki/Super_Bowl)&lt;/a&gt;), ngày lễ lao động (Labor Day - ngày một tháng 5), lễ tạ ơn (Thanksgiving, ngày lễ tạ ơn ở Mỹ được tổ chức vào ngày thứ Năm lần thứ tư của tháng 11, còn ở Canada ngày lễ tạ ơn được tổ chức vào ngày thứ hai lần thứ hai của tháng 10, theo wiki &lt;a href=&#34;https://en.wikipedia.org/wiki/Thanksgiving&#34;&gt;https://en.wikipedia.org/wiki/Thanksgiving&lt;/a&gt;), lễ giáng sinh (Christmas ngày 24 và 25 tháng 12 theo wiki &lt;a href=&#34;https://en.wikipedia.org/wiki/Christmas&#34;&gt;https://en.wikipedia.org/wiki/Christmas&lt;/a&gt; ). Những tuần có chứa những ngày lễ lớn này được đánh trọng số gấp 5 lần những tuần khác. Chúng ta phải xây dựng mô hình để mô hình hoá các tác động của việc giảm giá trong các tuần lễ này khi không có dữ liệu lịch sử đầy đủ.&lt;/p&gt;

&lt;p&gt;Tập dữ liệu được cung cấp bao gồm:&lt;/p&gt;

&lt;p&gt;Tập train: chứa dữ liệu số bán từ 05-02-2010 đến 01-11-2012. Các trường dữ liệu là: store number - mã cửa hàng, Dept number - mã sản phẩm, Date - Tuần, Weekly_Sales - số bán, IsHoliday - Nếu tuần đó có chứa các holidate thì đánh 1 ngược lại đánh 0.&lt;/p&gt;

&lt;p&gt;Tập test: Chứa dữ liệu test, có các cột thuộc tính như tập train&lt;/p&gt;

&lt;p&gt;Tập features: Chứa thông tin thêm về của hàng, bao gồm store - mã cửa hàng, Date - ngày, Temperature - Nhiệt độ, Fuel_Price - giá dầu (ở mỹ, mỗi khu vực khác nhau sẽ có giá nhiên liệu khác nhau), MarkDown1, MarkDown2,&amp;hellip; , MarkDown5 - một chỉ số gì đó mà tác giả không cung cấp định nghĩa cho chúng ta, CPI - chỉ số giá tiêu dùng, Unemployment - tình trạng thất nghiệp, IsHoliday - Tuần có chứa ngày nghỉ.&lt;/p&gt;

&lt;h1 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h1&gt;

&lt;p&gt;Mình sẽ import một số thư viện cần thiết&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd 
import numpy as np

#Do some statistics
from scipy.misc import imread
from scipy import sparse
import scipy.stats as ss
import math

#Nice graphing tools
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đọc các file data lên, merge các file lại với nhau&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

train = pd.read_csv(&#39;data/train.csv&#39;)
test = pd.read_csv(&#39;data/test.csv&#39;)
feature = pd.read_csv(&#39;data/features.csv&#39;)

train = train.merge(feature, how=&#39;left&#39;, on=[&#39;Store&#39;,&#39;Date&#39;])
test = test.merge(feature, how=&#39;left&#39;, on=[&#39;Store&#39;,&#39;Date&#39;])


# Merge in store info
stores = pd.read_csv(&amp;quot;data/stores.csv&amp;quot;)
train = train.merge(stores, how=&#39;left&#39;, on=&#39;Store&#39;)
test = test.merge(stores, how=&#39;left&#39;, on=&#39;Store&#39;)
print(train.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   Store  Dept        Date  Weekly_Sales  IsHoliday_x  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday_y Type    Size  Split
0      1     1  2010-02-05      24924.50        False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106        False    A  151315  Train
1      1     1  2010-02-12      46039.49         True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106         True    A  151315  Train
2      1     1  2010-02-19      41595.55        False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106        False    A  151315  Train
3      1     1  2010-02-26      19403.54        False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106        False    A  151315  Train
4      1     1  2010-03-05      21827.90        False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106        False    A  151315  Train
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mới có 5 dòng đầu tiên mà thấy các chỉ số markdown Nan rồi.&lt;/p&gt;

&lt;p&gt;Chúng ta tiến hành một số phân tích dữ liệu. À, Mình sẽ merge dữ liệu train và test lại rồi phân tích thống kê&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.concat([train,test],axis=0) # Join train and test

print(df.describe())

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;                 CPI           Dept     Fuel_Price      MarkDown1      MarkDown2      MarkDown3      MarkDown4      MarkDown5          Size          Store    Temperature   Unemployment   Weekly_Sales
count  498472.000000  536634.000000  536634.000000  265596.000000  197685.000000  242326.000000  237143.000000  266496.000000  536634.00000  536634.000000  536634.000000  498472.000000  421570.000000
mean      172.090481      44.277301       3.408310    7438.004144    3509.274827    1857.913525    3371.556866    4324.021158  136678.55096      22.208621      58.771762       7.791888   15981.258123
std        39.542149      30.527358       0.430861    9411.341379    8992.047197   11616.143274    6872.281734   13549.262124   61007.71180      12.790580      18.678716       1.865076   22711.183519
min       126.064000       1.000000       2.472000   -2781.450000    -265.760000    -179.260000       0.220000    -185.170000   34875.00000       1.000000      -7.290000       3.684000   -4988.940000
25%       132.521867      18.000000       3.041000    2114.640000      72.500000       7.220000     336.240000    1570.112500   93638.00000      11.000000      45.250000       6.623000    2079.650000
50%       182.442420      37.000000       3.523000    5126.540000     385.310000      40.760000    1239.040000    2870.910000  140167.00000      22.000000      60.060000       7.795000    7612.030000
75%       213.748126      74.000000       3.744000    9303.850000    2392.390000     174.260000    3397.080000    5012.220000  202505.00000      33.000000      73.230000       8.549000   20205.852500
max       228.976456      99.000000       4.468000  103184.980000  104519.540000  149483.310000   67474.850000  771448.100000  219622.00000      45.000000     101.950000      14.313000  693099.360000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phân tích một chút:&lt;/p&gt;

&lt;p&gt;Bỏ qua cột Dept và Store vì nó là mã sản phẩm và mã của hàng, người ta thích đặt số bao nhiêu thì đặt.&lt;/p&gt;

&lt;p&gt;Các chỉ số MarkDown có độ lệch chuẩn khá cao.&lt;/p&gt;

&lt;p&gt;Nhiệt độ min là -7.29, max là 101.95, trung bình là 58, nên không thể là độ C được, có thể là độ F&lt;/p&gt;

&lt;p&gt;Xem thử hệ số tương quan giữa các column như thế nào&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set(style=&amp;quot;white&amp;quot;)

# Compute the correlation matrix
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={&amp;quot;shrink&amp;quot;: .5})

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/walmart-corr.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hệ số tương quan giữa các cột trong dữ liệu&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Phân tích một chút, chúng ta thấy rằng MarkDown5 hầu như không có liên quan gì đến các column còn lại. Hệ số trải từ -0.3 đến 0.3 chứng tỏ mổi quan hệ giữa các cột là khá lỏng lẻo. Chỉ số giá tiêu dùng tương quan tỷ lệ nghịch với tình trạng thất nghiệp (hợp lý không nhỉ). Kích thước cửa hàng càng bự thì bán càng nhiều (ok hiển nhiên), sản phẩm có mã càng lớn thì bán càng nhiều (? có lẽ là sản phẩm mới, người mỹ thích mua sản phẩm mới chăng). Và một vấn đề quan trọng là giá nhiên liệu, isHoliday, nhiệt độ không có mối tương quan với weekly sales. Chỉ số CPI và tình trạng thất nghiệp cũng ảnh hưởng không lớn với  weekly sales.&lt;/p&gt;

&lt;p&gt;Thử plot lên hình ảnh về số lượng bán và kích thước cửa hàng xem sao&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter( df[&#39;Size&#39;],df[&#39;Weekly_Sales&#39;])
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/walmart-size-sales-coeff.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Tương quan giữa số bán và kích thước cửa hàng&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nhìn vào hình trên, chúng ta thấy rằng cửa hàng có kích thước nhỏ số bán cũng không tăng đột biến khi gặp ngày lễ, cửa hàng kích thước siêu bự có tỷ lệ đột biến thấp, cửa hàng trung trung có đột biến, ở khúc size 125000 và số bán là 700000. Chúng ta hãy xem những ngày có số bán lớn rơi vào ngày nào.
Dựa vào bảng desription ở phía trên đã phân tích, trung bình của số bán là 15981 và lệch chuẩn là 22711, cộng lại là  15981 + 22711 = 38692, nhìn trên đô thị thì phần đột biến khá lớn. Max là 700000, min là 0 (cái này nhìn hình, không phải số thực tế ở bảng mô tả), mình sẽ lấy ra những ngày có số bán lớn hơn 350000 (vượt qua ngưỡng trung bình + độ lệch chuẩn rất nhều -&amp;gt; ngoại lệ là đây) xem những ngày đó là ngày gì&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
print(df.loc[df[&#39;Weekly_Sales&#39;] &amp;gt;350000].head(10))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In ra top 10 thằng đầu tiên&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
               CPI        Date  Dept  Fuel_Price  IsHoliday_x  IsHoliday_y  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5    Size  Split  Store  Temperature Type  Unemployment  Weekly_Sales
37201   126.669267  2010-11-26    72       2.752         True         True        NaN        NaN        NaN        NaN        NaN  205863  Train      4        48.08    A         7.127     381072.11
37253   129.836400  2011-11-25    72       3.225         True         True     561.45     137.88   83340.33      44.04    9239.23  205863  Train      4        47.96    A         5.143     385051.04
88428   126.983581  2010-12-24     7       3.236        False        False        NaN        NaN        NaN        NaN        NaN  126512  Train     10        57.06    B         9.003     406988.63
95373   126.669267  2010-11-26    72       3.162         True         True        NaN        NaN        NaN        NaN        NaN  126512  Train     10        55.33    B         9.003     693099.36
95377   126.983581  2010-12-24    72       3.236        False        False        NaN        NaN        NaN        NaN        NaN  126512  Train     10        57.06    B         9.003     404245.03
95425   129.836400  2011-11-25    72       3.760         True         True     174.72     329.00  141630.61      79.00    1009.98  126512  Train     10        60.68    B         7.874     630999.19
115222  126.669267  2010-11-26    72       3.162         True         True        NaN        NaN        NaN        NaN        NaN  112238  Train     12        47.66    B        14.313     359995.60
115274  129.836400  2011-11-25    72       3.622         True         True    5391.83       8.00   63143.29      49.27    2115.67  112238  Train     12        53.25    B        12.890     360140.66
128984  182.544590  2010-12-24     7       3.141        False        False        NaN        NaN        NaN        NaN        NaN  200898  Train     14        30.59    A         8.724     356867.25
135665  182.783277  2010-11-26    72       3.039         True         True        NaN        NaN        NaN        NaN        NaN  200898  Train     14        46.15    A         8.724     474330.10

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nhìn vào bảng trên, chúng ta thấy rằng 10 ngày đầu tiên tập trung chủ yếu ở tháng 11 và tháng 12, tháng 12 là 24-25 tháng 12 -&amp;gt; ngày noel, còn tháng 11 là 25-26 tháng 11 (ngày gì vậy ta, trong mô tả không thấy)
Tra lịch thì ngày 25 tháng 11 năm 2011 trúng thứ sáu, tra trên mạng một thông tin khá quan trong là &amp;ldquo;Black Friday sẽ rơi vào khoảng ngày 23-29 tháng 11&amp;rdquo; -&amp;gt; không nghi ngờ gì nữa có thể là ngày này đây.
Thử tra tiếp ngày 26 tháng 11 năm 2010, cũng là thứ sáu luôn -&amp;gt; ngày black friday và ngày noel có sức mua điên cuồng quá.&lt;/p&gt;

&lt;p&gt;Mình dùng một kỹ thuật nhỏ là giảm dần số bán, để ra số bán tối thiểu mà ngày black friday và ngày nodel vẫn còn giữ vị trí thống trị. Kỹ thuật khá đơn giản thôi, từ giá trị 350000, mỗi lần mình sẽ giảm đi 10000, và đếm số lần xuất hiện của các ngày, nếu có ngày nào đó nằm ngoài tuần chứa black friday và nodel thì mình dừng. Sau một hồi tìm kiếm và số bán đã xuất hiện, đó là 290000&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(df.loc[df[&#39;Weekly_Sales&#39;] &amp;gt;290000,&amp;quot;Date&amp;quot;].value_counts())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;2010-11-26    16
2011-11-25    14
2010-12-24     8
2011-12-23     3
2010-02-05     1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;làm-sạch-dữ-liệu&#34;&gt;Làm sạch dữ liệu&lt;/h1&gt;

&lt;h3 id=&#34;xử-lý-missing-values&#34;&gt;Xử lý missing values&lt;/h3&gt;

&lt;p&gt;Một vấn đề khá quan trọng là trong tập dữ liệu này missing value khá nhiều, thử đếm số lượng null trong data cho ta biết được rằng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;CPI              38162
Date                 0
Dept                 0
Fuel_Price           0
IsHoliday_x          0
IsHoliday_y          0
MarkDown1       271038
MarkDown2       338949
MarkDown3       294308
MarkDown4       299491
MarkDown5       270138
Size                 0
Split                0
Store                0
Temperature          0
Type                 0
Unemployment     38162
Weekly_Sales    115064
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Các giá trị MarkDown bị null khá nhiều, cách đơn giản nhất là set 0 cho tất cả các giá trị null ( Mình lưu log lại những index null của các markdown).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.assign(md1_present = df[&#39;MarkDown1&#39;]notnull())
df = df.assign(md2_present = df[&#39;MarkDown2&#39;]notnull())
df = df.assign(md3_present = df[&#39;MarkDown3&#39;]notnull())
df = df.assign(md4_present = df[&#39;MarkDown4&#39;]notnull())
df = df.assign(md5_present = df[&#39;MarkDown5&#39;].notnull())

df.fillna(0, inplace=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;tạo-đặc-trưng&#34;&gt;Tạo đặc trưng&lt;/h1&gt;

&lt;p&gt;Đặc trưng holiday&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;IsHoliday&#39;] = &#39;IsHoliday_&#39; + df[&#39;IsHoliday_x&#39;].map(str)
holiday_dummies = pd.get_dummies(df[&#39;IsHoliday&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trưng ngày tháng&lt;/p&gt;

&lt;p&gt;Rút trích tháng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;DateType&#39;] = [datetime.strptime(date, &#39;%Y-%m-%d&#39;).date() for date in df[&#39;Date&#39;].astype(str).values.tolist()]
df[&#39;Month&#39;] = [date.month for date in df[&#39;DateType&#39;]]
df[&#39;Month&#39;] = &#39;Month_&#39; + df[&#39;Month&#39;].map(str)
Month_dummies = pd.get_dummies(df[&#39;Month&#39;] )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rút trích ngày trước giáng sinh và black friday&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;Black_Friday&#39;] = np.where((df[&#39;DateType&#39;]==datetime(2010, 11, 26).date()) | (df[&#39;DateType&#39;]==datetime(2011, 11, 25).date()), &#39;yes&#39;, &#39;no&#39;)
df[&#39;Pre_christmas&#39;] = np.where((df[&#39;DateType&#39;]==datetime(2010, 12, 23).date()) | (df[&#39;DateType&#39;]==datetime(2010, 12, 24).date()) | (df[&#39;DateType&#39;]==datetime(2011, 12, 23).date()) | (df[&#39;DateType&#39;]==datetime(2011, 12, 24).date()), &#39;yes&#39;, &#39;no&#39;)
df[&#39;Black_Friday&#39;] = &#39;Black_Friday_&#39; + df[&#39;Black_Friday&#39;].map(str)
df[&#39;Pre_christmas&#39;] = &#39;Pre_christmas_&#39; + df[&#39;Pre_christmas&#39;].map(str)
Black_Friday_dummies = pd.get_dummies(df[&#39;Black_Friday&#39;] )
Pre_christmas_dummies = pd.get_dummies(df[&#39;Pre_christmas&#39;] )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thêm các đặc trưng vào trong dữ liệu&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
df = pd.concat([df,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies],axis=1)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thêm đặc trưng trung vị của từng loại cửa hàng vào từng tháng, do một số của hàng sẽ bị NA ở cột số bán ở một thời điểm nào đó, nên chúng ta replace số bán là 0 có vẻ không hợp lý lắm. Mình chọn cách là thay thế bằng trung bình của số bán trong tháng của cửa hàng cùng loại. Nhưng trước tiên thì tính trung bình số bán của từng loại cửa hàng cái đã.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
medians = pd.DataFrame({&#39;Median Sales&#39; :df.loc[df[&#39;Split&#39;]==&#39;Train&#39;].groupby(by=[&#39;Type&#39;,&#39;Dept&#39;,&#39;Store&#39;,&#39;Month&#39;,&#39;IsHoliday&#39;])[&#39;Weekly_Sales&#39;].median()}).reset_index()
print(medians.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;     Type    Dept    Store     Month        IsHoliday  Median Sales
0  Type_A  Dept_1  Store_1   Month_1  IsHoliday_False     17350.585
1  Type_A  Dept_1  Store_1  Month_10  IsHoliday_False     23388.030
2  Type_A  Dept_1  Store_1  Month_11  IsHoliday_False     19551.115
3  Type_A  Dept_1  Store_1  Month_11   IsHoliday_True     19865.770
4  Type_A  Dept_1  Store_1  Month_12  IsHoliday_False     39109.390
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;thêm dữ liệu vào trong data chính, loại bỏ NA và tạo key cho mỗi dòng để dễ dàng truy xuất&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.merge(medians, how = &#39;outer&#39;, on = [&#39;Type&#39;,&#39;Dept&#39;,&#39;Store&#39;,&#39;Month&#39;,&#39;IsHoliday&#39;])

# Fill NA
df[&#39;Median Sales&#39;].fillna(df[&#39;Median Sales&#39;].loc[df[&#39;Split&#39;]==&#39;Train&#39;].median(), inplace=True) 

# Create a key for easy access

df[&#39;Key&#39;] = df[&#39;Type&#39;].map(str)+df[&#39;Dept&#39;].map(str)+df[&#39;Store&#39;].map(str)+df[&#39;Date&#39;].map(str)+df[&#39;IsHoliday&#39;].map(str)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta sẽ dự đoán số bán của tuần kế tiếp dựa vào kết quả số bán của tuần hiện tại, nên trong dữ liệu sẽ lưu trên ngày của tuần trước đó để dễ truy xuất. Vì 1 tuần có 7 ngày, chúng ta sẽ lưu giá trị là ngày ở cột hiện tại - 7&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;DateLagged&#39;] = df[&#39;DateType&#39;]- timedelta(days=7)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và giờ đây, chúng ta sẽ lặp qua toàn bộ các dòng trên tập dữ liệu, kiểm tra xem có dòng nào số bán nan hông, nếu có thì sẽ thay bằng trung bình đã tính ở trên. Ở đây mình tạo một sorted dataset để truy xuất cho nhanh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#Make a sorted dataframe. This will allow us to find lagged variables much faster!
sorted_df = df.sort_values([&#39;Store&#39;, &#39;Dept&#39;,&#39;DateType&#39;], ascending=[1, 1,1])
sorted_df = sorted_df.reset_index(drop=True) # Reinitialize the row indices for the loop to work

sorted_df[&#39;LaggedSales&#39;] = np.nan # Initialize column
sorted_df[&#39;LaggedAvailable&#39;] = np.nan # Initialize column
last=df.loc[0] # intialize last row for first iteration. Doesn&#39;t really matter what it is
row_len = sorted_df.shape[0]
for index, row in sorted_df.iterrows():
    lag_date = row[&amp;quot;DateLagged&amp;quot;]
    # Check if it matches by comparing last weeks value to the compared date 
    # And if weekly sales aren&#39;t 0
    if((last[&#39;DateType&#39;]== lag_date) &amp;amp; (last[&#39;Weekly_Sales&#39;]&amp;gt;0)): 
        sorted_df.set_value(index, &#39;LaggedSales&#39;,last[&#39;Weekly_Sales&#39;])
        sorted_df.set_value(index, &#39;LaggedAvailable&#39;,1)
    else:
        sorted_df.set_value(index, &#39;LaggedSales&#39;,row[&#39;Median Sales&#39;]) # Fill with median
        sorted_df.set_value(index, &#39;LaggedAvailable&#39;,0)

    last = row #Remember last row for speed
    if(index%int(row_len/10)==0): #See progress by printing every 10% interval
        print(str(int(index*100/row_len))+&#39;% loaded&#39;)

print(sorted_df[[&#39;Dept&#39;, &#39;Store&#39;,&#39;DateType&#39;,&#39;LaggedSales&#39;,&#39;Weekly_Sales&#39;,&#39;Median Sales&#39;]].head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;9% loaded
19% loaded
29% loaded
39% loaded
49% loaded
59% loaded
69% loaded
79% loaded
89% loaded
99% loaded
     Dept    Store    DateType  LaggedSales  Weekly_Sales  Median Sales
0  Dept_1  Store_1  2010-02-05     23510.49      24924.50      23510.49
1  Dept_1  Store_1  2010-02-12     24924.50      46039.49      37887.17
2  Dept_1  Store_1  2010-02-19     46039.49      41595.55      23510.49
3  Dept_1  Store_1  2010-02-26     41595.55      19403.54      23510.49
4  Dept_1  Store_1  2010-03-05     19403.54      21827.90      21280.40
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Công việc đơn giản tiếp theo là merge dữ liệu vào data chính và tính độ lệch giữa 2 tuần bán&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Merge by store and department
df = df.merge(sorted_df[[&#39;Dept&#39;, &#39;Store&#39;,&#39;DateType&#39;,&#39;LaggedSales&#39;,&#39;LaggedAvailable&#39;]], how = &#39;inner&#39;, on = [&#39;Dept&#39;, &#39;Store&#39;,&#39;DateType&#39;])
df[&#39;Sales_dif&#39;] = df[&#39;Median Sales&#39;] - df[&#39;LaggedSales&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và bây giờ , thay vì ta ước lượng weekly sales, chúng ta sẽ ước lượng độ lệch giữa week sales và median sales (đây là một cách trong những cách để tính điểm dừng của dữ liệu time series)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;Difference&#39;] = df[&#39;Median Sales&#39;] - df[&#39;Weekly_Sales&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;huấn-luyện-mô-hình&#34;&gt;Huấn luyện mô hình&lt;/h1&gt;

&lt;p&gt;Lựa chọn các đặc trưng huấn luyện&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;selector = [
    #&#39;Month&#39;,
    &#39;CPI&#39;,
    &#39;Fuel_Price&#39;,
    &#39;MarkDown1&#39;,
    &#39;MarkDown2&#39;,
    &#39;MarkDown3&#39;,
    &#39;MarkDown4&#39;,
    &#39;MarkDown5&#39;,
    &#39;Size&#39;,
    &#39;Temperature&#39;,
    &#39;Unemployment&#39;,
    
    
    
    &#39;md1_present&#39;,
    &#39;md2_present&#39;,
    &#39;md3_present&#39;,
    &#39;md4_present&#39;,
    &#39;md5_present&#39;,

    &#39;IsHoliday_False&#39;,
    &#39;IsHoliday_True&#39;,
    &#39;Pre_christmas_no&#39;,
    &#39;Pre_christmas_yes&#39;,
    &#39;Black_Friday_no&#39;,
    &#39;Black_Friday_yes&#39;,    
    &#39;LaggedSales&#39;,
    &#39;Sales_dif&#39;,
    &#39;LaggedAvailable&#39;
    ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tách dữ liệu train và test riêng ra&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
train = df.loc[df[&#39;Split&#39;]==&#39;Train&#39;]
test = df.loc[df[&#39;Split&#39;]==&#39;Test&#39;]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lấy ngẫu nhiên 20% dữ liệu ở tập train để validation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed for reproducability 
np.random.seed(42)
X_train, X_val, y_train, y_val = train_test_split(train[selector], train[&#39;Difference&#39;], test_size=0.2, random_state=42)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Huấn luyện bằng neural network sử dụng lstm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
adam_regularized = Sequential()

    # First hidden layer now regularized
    model.add(Dense(32,activation=&#39;relu&#39;,
                    input_dim=X_train.shape[1],
                    kernel_regularizer = regularizers.l2(0.01)))

    # Second hidden layer now regularized
    adam_regularized.add(Dense(16,activation=&#39;relu&#39;,
                       kernel_regularizer = regularizers.l2(0.01)))

    # Output layer stayed sigmoid
    adam_regularized.add(Dense(1,activation=&#39;linear&#39;))

    # Setup adam optimizer
    adam_optimizer=keras.optimizers.Adam(lr=0.01,
                    beta_1=0.9, 
                    beta_2=0.999, 
                    epsilon=1e-08)

    # Compile the model
    adam_regularized.compile(optimizer=adam_optimizer,
                  loss=&#39;mean_absolute_error&#39;,
                  metrics=[&#39;acc&#39;])

    # Train
    history=adam_regularized.fit(X_train, y_train, # Train on training set
                                 epochs=10, # We will train over 1,000 epochs
                                 batch_size=2048, # Batch size 
                                 verbose=0) # Suppress Keras output
    print(&#39;eval&#39;,model.evaluate(x=X_val,y=y_val))

    # Plot network
    plt.plot(history.history[&#39;loss&#39;], label=&#39;Adam Regularized&#39;)
    plt.xlabel(&#39;Epochs&#39;)
    plt.ylabel(&#39;loss&#39;)
    plt.legend()
    plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eval:  [1457.0501796214685, 0.002312783168124545]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/walmart_keras_lstm_adam.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Độ lỗi trên tập train&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Độ lỗi trên tập train giảm xuống đến gần 1450 thì đừng hẳn, không thể giảm được nữa&lt;/p&gt;

&lt;p&gt;Giá trị độ lệch trên tập evaluation là 1457.0501796214685&lt;/p&gt;

&lt;p&gt;Thử huấn luyện bằng random forest&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;regr = RandomForestRegressor(n_estimators=20, criterion=&#39;mse&#39;, max_depth=None, 
                        min_samples_split=2, min_samples_leaf=1, 
                        min_weight_fraction_leaf=0.0, max_features=&#39;auto&#39;, 
                        max_leaf_nodes=None, min_impurity_decrease=0.0, 
                        min_impurity_split=None, bootstrap=True, 
                        oob_score=False, n_jobs=1, random_state=None, 
                        verbose=2, warm_start=False)

    #Train on data
    regr.fit(X_train, y_train.ravel())
    y_pred_random = regr.predict(X_val)

    y_val = y_val.to_frame()

    # Transform forest predictions to observe direction of change
    direction_true1= y_val.values
    direction_predict = y_pred_random

    y_val[&#39;Predicted&#39;] = y_pred_random
    df_out = pd.merge(train,y_val[[&#39;Predicted&#39;]],how = &#39;left&#39;,left_index = True, right_index = True,suffixes=[&#39;_True&#39;,&#39;_Pred&#39;])
    df_out = df_out[~pd.isnull(df_out[&#39;Predicted&#39;])]

    df_out[&#39;prediction&#39;] = df_out[&#39;Median Sales&#39;]-df_out[&#39;Predicted&#39;]

    print(&amp;quot;Medians: &amp;quot;+str(sum(abs(df_out[&#39;Difference&#39;]))/df_out.shape[0]))
    print(&amp;quot;Random Forest: &amp;quot;+str(sum(abs(df_out[&#39;Weekly_Sales&#39;]-df_out[&#39;prediction&#39;]))/df_out.shape[0])) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
9% loaded
19% loaded
29% loaded
39% loaded
49% loaded
59% loaded
69% loaded
79% loaded
89% loaded
99% loaded
     Dept    Store    DateType  LaggedSales  Weekly_Sales  Median Sales
0  Dept_1  Store_1  2010-02-05     23510.49      24924.50      23510.49
1  Dept_1  Store_1  2010-02-12     24924.50      46039.49      37887.17
2  Dept_1  Store_1  2010-02-19     46039.49      41595.55      23510.49
3  Dept_1  Store_1  2010-02-26     41595.55      19403.54      23510.49
4  Dept_1  Store_1  2010-03-05     19403.54      21827.90      21280.40
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
building tree 1 of 20
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.5s remaining:    0.0s
building tree 2 of 20
building tree 3 of 20
building tree 4 of 20
building tree 5 of 20
building tree 6 of 20
building tree 7 of 20
building tree 8 of 20
building tree 9 of 20
building tree 10 of 20
building tree 11 of 20
building tree 12 of 20
building tree 13 of 20
building tree 14 of 20
building tree 15 of 20
building tree 16 of 20
building tree 17 of 20
building tree 18 of 20
building tree 19 of 20
building tree 20 of 20
[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  2.2min finished
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.1s finished
Medians: 1545.7406070759525
Random Forest: 1356.4670052620745

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trung bình lệch của random forest là 1356, giá trị này nhỏ hơn so với giá trị output của lstm trả về.&lt;/p&gt;

&lt;p&gt;Thử huấn luyện bằng XGBoost&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
param_dist = { &#39;max_depth&#39;:5}

    model = XGBRegressor(**param_dist)

    #Train on data
    model.fit(X_train, y_train.ravel())
    y_pred_random = model.predict(X_val)

    y_val = y_val.to_frame()

    # Transform forest predictions to observe direction of change
    direction_true1= y_val.values
    direction_predict = y_pred_random

    y_val[&#39;Predicted&#39;] = y_pred_random
    df_out = pd.merge(train,y_val[[&#39;Predicted&#39;]],how = &#39;left&#39;,left_index = True, right_index = True,suffixes=[&#39;_True&#39;,&#39;_Pred&#39;])
    df_out = df_out[~pd.isnull(df_out[&#39;Predicted&#39;])]

    df_out[&#39;prediction&#39;] = df_out[&#39;Median Sales&#39;]-df_out[&#39;Predicted&#39;]

    print(&amp;quot;Medians: &amp;quot;+str(sum(abs(df_out[&#39;Difference&#39;]))/df_out.shape[0]))
    print(&amp;quot;XGB Regressor: &amp;quot;+str(sum(abs(df_out[&#39;Weekly_Sales&#39;]-df_out[&#39;prediction&#39;]))/df_out.shape[0])) 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
Medians: 1545.7406070759525
XGB Regressor: 1354.1976755192593

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả cũng gần như bằng Random forest :).&lt;/p&gt;

&lt;p&gt;Giờ mình sẽ dùng random forest để tạo file submission&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

rf_model = RandomForestRegressor(n_estimators=80, criterion=&#39;mse&#39;, max_depth=None, 
                      min_samples_split=2, min_samples_leaf=1, 
                      min_weight_fraction_leaf=0.0, max_features=&#39;auto&#39;, 
                      max_leaf_nodes=None, min_impurity_decrease=0.0, 
                      min_impurity_split=None, bootstrap=True, 
                      oob_score=False, n_jobs=1, random_state=None, 
                      verbose=0, warm_start=False)

#Train on data
rf_model.fit(train[selector], train[&#39;Difference&#39;])
final_y_prediction = rf_model.predict(test[selector])

testfile = pd.concat([test.reset_index(drop=True), pd.DataFrame(final_y_prediction)], axis=1)
testfile[&#39;prediction&#39;] = testfile[&#39;Median Sales&#39;]-testfile[0]

submission = pd.DataFrame({&#39;id&#39;:pd.Series([&#39;&#39;.join(list(filter(str.isdigit, x))) for x in testfile[&#39;Store&#39;]]).map(str) + &#39;_&#39; +
                           pd.Series([&#39;&#39;.join(list(filter(str.isdigit, x))) for x in testfile[&#39;Dept&#39;]]).map(str)  + &#39;_&#39; +
                           testfile[&#39;Date&#39;].map(str),
                          &#39;Weekly_Sales&#39;:testfile[&#39;prediction&#39;]})

submission.to_csv(&#39;submission.csv&#39;,index=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi submit mô hình, mình đạt được kết quả là 4455.96312 trên private board, và 4419.17292 trên publish board. Đây là một kết quả khá tệ (đứng hạng khoảng top 300). Sau khi mình nhìn lại mô hình thì phát hiện một số vấn đề.&lt;/p&gt;

&lt;p&gt;Các đặc trưng trong file features.csv nó không có mối tương quan gì hết với số bán như phân tích ở trên -&amp;gt; mình mạnh dạng bỏ luôn file features.csv, không quan tâm đến nó nữa, tập trung vào file chính.&lt;/p&gt;

&lt;p&gt;Bỏ mấy cái lag luôn, thử forecast chính vào cái số bán luôn xem sao&lt;/p&gt;

&lt;p&gt;Với cửa hàng nào thì xây dựng mô hình cho cửa hàng và sản phẩm đó, không xây dựng một mô hình tổng quát áp dụng cho toàn cửa hàng. với những cửa hàng không có trong tập train hoặc những sản phẩm mà cửa hàng đó chưa bán trước đây (nói chung là không có trong tập train) thì mới áp dụng mô hình của toàn cửa hàng cho nó.&lt;/p&gt;

&lt;p&gt;Kết quả là mình đạt được 2736 trên private board và 2657.40087 trên publish board (top 30), kết quả trên vẫn làm cho mình chưa hài lòng lắm.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>