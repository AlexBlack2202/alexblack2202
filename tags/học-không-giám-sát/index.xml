<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>học không giám sát on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/h%E1%BB%8Dc-kh%C3%B4ng-gi%C3%A1m-s%C3%A1t/</link>
    <description>Recent content in học không giám sát on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>alexblack2202@gmail.com (Phạm Duy Tùng)</managingEditor>
    <webMaster>alexblack2202@gmail.com (Phạm Duy Tùng)</webMaster>
    <copyright>&amp;copy; 2018 Phạm Duy Tùng. Website chia sẻ kiến thức của Phạm Duy Tùng và Đặng Thị Hằng. Vui lòng liên hệ email alexblack2202@gmail.com nếu bạn có thông tin cần trao đổi.</copyright>
    <lastBuildDate>Fri, 19 Apr 2019 00:12:00 +0300</lastBuildDate>
    <atom:link href="/tags/h%E1%BB%8Dc-kh%C3%B4ng-gi%C3%A1m-s%C3%A1t/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Phân nhóm các thuật toán học máy</title>
      <link>/blog/2019-04-19-deep-learning-view/</link>
      <pubDate>Fri, 19 Apr 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-19-deep-learning-view/</guid>
      <description>

&lt;p&gt;Ở bài trước mình đã trình bày định nghĩa và một số ứng dụng của Máy học (Machine Learning – ML), phân biệt ML với Trí tuệ nhân tạo (Artificial Intelligence – AI) cũng như mối quan hệ giữa AI, ML và Big Data. Từ bài viết này trở đi mình sẽ tập trung viết về ML, các thuật toán, cách sử dụng công cụ kèm theo một vài demo nhỏ giúp bạn đọc dễ hình dung và áp dụng. Để mở đầu cho chuỗi bài viết sắp tới, hôm nay mình sẽ trình bày cách phân nhóm các thuật toán ML.&lt;/p&gt;

&lt;p&gt;Với đa số mọi người, trước khi bắt đầu giải quyết một vấn đề nào đó, việc đầu tiên là chúng ta sẽ tìm hiểu xem liệu có ai đã gặp vấn đề này hoặc vấn đề tương tự như vậy hay không và cách họ giải quyết thế nào. Sau khi nắm được thông tin khái quát, công việc kế tiếp là chọn lựa và điều chỉnh giải pháp sao cho phù hợp với vấn đề của bản thân. Trong trường hợp vấn đề còn quá mới mẻ thì chúng ta mới phải bắt tay làm từ đầu, điều này hầu như rất hiếm, đặc biệt là trong thời đại công nghệ này, khi mà chỉ bằng một cú nhấp chuột, hàng ngàn thông tin, tư liệu về đề tài chúng ta quan tâm sẽ xuất hiện. Cũng giống như thế, ML hiện đã được nghiên cứu rộng khắp, rất nhiều công trình khoa học, thuật toán được cho ra đời. Với người mới bắt đầu mà nói thì chúng ta chưa cần phải làm gì cả ngoài việc nắm được các thuật toán cơ bản, đặc điểm của chúng để khi đối diện với một bài toán cụ thể trong thực tế chúng ta có thể biết được mình nên lựa chọn thuật toán nào cho phù hợp đã là điều rất tốt rồi.&lt;/p&gt;

&lt;p&gt;Mặc dù có rất nhiều thuật toán học nhưng dựa vào phương thức học (learning style) hoặc sự tương đồng (similarity) về hình thức hay chức năng mà chúng có thể được gom thành từng nhóm. Sau đây mình sẽ trình bày tổng quan cả hai cách phân nhóm thuật toán học này.&lt;/p&gt;

&lt;h1 id=&#34;1-phân-nhóm-dựa-trên-phương-thức-học&#34;&gt;1.    Phân nhóm dựa trên phương thức học&lt;/h1&gt;

&lt;p&gt;Xét theo phương thức học, các thuật toán ML được chia làm bốn nhóm, bao gồm “Học có giám sát” (Supervised Learning), “Học không giám sát” (Unsupervised Learning), “Học bán giám sát” (hay học kết hợp - Semi-supervised Learning) và “Học tăng cường” (Reinforcement Learning).&lt;/p&gt;

&lt;h2 id=&#34;a-học-có-giám-sát&#34;&gt;a.   Học có giám sát&lt;/h2&gt;

&lt;p&gt;Học có giám sát hay còn gọi là học có thầy là thuật toán dự đoán nhãn (label)/đầu ra (output) của một dữ liệu mới dựa trên tập dữ liệu huấn luyện mà trong đó mỗi mẫu dữ liệu đều đã được gán nhãn như minh hoạ ở Hình 1. Khi đó, thông qua một quá trình huấn luyện, một mô hình sẽ được xây dựng để cho ra các dự đoán và khi các dự đoán bị sai thì mô hình này sẽ được tinh chỉnh lại. Việc huấn luyện sẽ tiếp tục cho đến khi mô hình đạt được mức độ chính xác mong muốn trên dữ liệu huấn luyện. Điều này cũng giống như khi chúng ta đi học trên lớp, ta biết câu trả lời chính xác từ giáo viên (tập dữ liệu có nhãn) và từ đó ta sẽ sửa chữa nếu làm sai. Học có giám sát là nhóm phổ biến nhất trong các thuật toán ML.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/supervised-learning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 1: Supervised Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Một cách toán học, học có giám sát là khi chúng ra có một tập hợp biến đầu vào &lt;code&gt;$ X={x_1,x_2,…,x_N} $&lt;/code&gt; và một tập hợp nhãn tương ứng &lt;code&gt;$ Y={y_1,y_2,…,y_N} $&lt;/code&gt;, trong đó &lt;code&gt;$ x_i$&lt;/code&gt;, &lt;code&gt;$y_i $&lt;/code&gt; là các vector. Các cặp dữ liệu biết trước &lt;code&gt;$( x_i, y_i ) \in X \times Y $&lt;/code&gt; được gọi là tập dữ liệu huấn luyện (training data). Từ tập dữ liệu huấn luyện này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập X sang một phần tử (xấp xỉ) tương ứng của tập Y:&lt;/p&gt;

&lt;p&gt;$$ y_i \approx f(x_i), \forall i=1, 2, …, N $$&lt;/p&gt;

&lt;p&gt;Mục đích là xấp xỉ hàm số &lt;code&gt;$f$&lt;/code&gt; thật tốt để khi có một dữ liệu x mới, chúng ta có thể tính được nhãn tương ứng của nó &lt;code&gt;$y=f(x)$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Ví dụ: Trong nhận dạng chữ số viết tay, ta có ảnh của hàng nghìn trường hợp ứng với mỗi chữ số được viết bởi nhiều người khác nhau. Ta đưa các bức ảnh này vào một thuật toán học và chỉ cho nó biết “mỗi bức ảnh tương ứng với chữ số nào”. Sau khi thuật toán tạo ra một mô hình, tức là một hàm số nhận đầu vào là một bức ảnh và cho ra kết quả là một chữ số. Khi nhận được một bức ảnh mới mà mô hình “chưa từng gặp qua” và nó sẽ dự đoán xem bức ảnh đó tương ứng với chữ số nào.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mnist-900x506.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 2: Ảnh minh hoạ cho tập dữ liệu chữ số viết tay - MNIST&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Đối với những ai sử dụng mạng xã hội Facebook thì khá quen thuộc với tính năng phát hiện khuôn mặt trong một bức ảnh, bản chất của thuật toán dò tìm các khuôn mặt này là một thuật toán học có giám sát với tập huấn luyện là vô số ảnh đã được gán nhãn là mặt người hay không phải mặt người.&lt;/p&gt;

&lt;p&gt;Các thuật toán học có giám sát còn được phân ra thành hai loại chính là phân lớp (Classification) và hồi quy (Regression).&lt;/p&gt;

&lt;h3 id=&#34;phân-lớp&#34;&gt;Phân lớp&lt;/h3&gt;

&lt;p&gt;Một bài toán được gọi là phân lớp nếu các nhãn của dữ liệu đầu vào được chia thành một số hữu hạn lớp (miền giá trị là rời rạc). Chẳng hạn như tính năng xác định xem một email có phải là spam hay không của Gmail; xác định xem hình ảnh của con vật là chó hay mèo. Hoặc ví dụ nhận dạng ký số viết tay ở trên cũng thuộc bài toán phân lớp, bao gồm mười lớp ứng với các số từ 0 đến 9. Tương tự cho ví dụ nhận dạng khuôn mặt với hai lớp là phải và không phải khuôn mặt, …&lt;/p&gt;

&lt;h3 id=&#34;hồi-quy&#34;&gt;Hồi quy&lt;/h3&gt;

&lt;p&gt;Một bài toán được xem là hồi quy nếu nhãn không được chia thành các nhóm mà là một giá trị thực cụ thể (miền giá trị là liên tục). Hầu hết các bài toán dự báo (giá cổ phiếu, giá nhà, …) thường được xếp vào bài toán hồi quy. Ví như, nếu một căn nhà rộng 150 m^2, có 7 phòng và cách trung tâm thành phố 10 km sẽ có giá là bao nhiêu? Lúc này kết quả dự đoán sẽ là một số thực.&lt;/p&gt;

&lt;p&gt;Nếu như phát hiện khuôn mặt là bài toán phân lớp thì dự đoán tuổi là bài toán hồi quy. Tuy nhiên dự đoán tuổi cũng có thể coi là phân lớp nếu ta cho tuổi là một số nguyên dương N và khi đó ta sẽ có N lớp khác nhau tính từ 1.
Một số thuật toán nổi tiếng thuộc về nhóm học có giám sát như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Phân lớp: k-Nearest Neighbors, mạng nơron nhân tạo, SVM, …&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hồi quy: Linear Regression, Logistic Regression, …&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;b-học-không-giám-sát&#34;&gt;b. Học không giám sát&lt;/h2&gt;

&lt;p&gt;Trái với Supervised learning, học không giám sát hay học không thầy là thuật toán dự đoán nhãn của một dữ liệu mới dựa trên tập dữ liệu huấn luyện mà trong đó tất cả các mẫu dữ liệu đều chưa được gán nhãn hay nói cách khác là ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào như minh hoạ ở Hình 3. Điều này cũng giống như khi ta học mà không có thầy cô, sẽ không ai cho ta biết đáp án đúng là gì.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/unsupervisedlearning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 3: Unsupervised Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Khi đó, mục tiêu của thuật toán unsupervised learning không phải là tìm đầu ra chính xác mà sẽ hướng tới việc tìm ra cấu trúc hoặc sự liên hệ trong dữ liệu để thực hiện một công việc nào đó, ví như gom cụm (clustering) hoặc giảm số chiều của dữ liệu (dimension reduction) để thuận tiện trong việc lưu trữ và tính toán.&lt;/p&gt;

&lt;p&gt;Các bài toán Unsupervised learning tiếp tục được chia nhỏ thành hai loại là phân cụm (Clustering) và luật kết hợp (Association Rule).&lt;/p&gt;

&lt;h3 id=&#34;phân-cụm&#34;&gt;Phân cụm&lt;/h3&gt;

&lt;p&gt;Một bài toán phân cụm / phân nhóm toàn bộ dữ liệu X thành các nhóm/cụm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Chẳng hạn như phân nhóm khách hàng dựa vào độ tuổi, giới tính. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình dạng và màu sắc khác nhau, có thể là tam giác, vuông, tròn với màu xanh, đỏ, tím, vàng, sau đó yêu cầu trẻ phân chúng thành từng nhóm. Mặc dù ta không dạy trẻ mảnh nào tương ứng với hình nào hoặc màu nào, nhưng nhiều khả năng trẻ vẫn có thể phân loại các mảnh ghép theo màu sắc hoặc hình dạng.&lt;/p&gt;

&lt;h3 id=&#34;luật-kết-hợp&#34;&gt;Luật kết hợp&lt;/h3&gt;

&lt;p&gt;Là bài toán mà khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví như những khách hàng mua mặt hàng này sẽ mua thêm mặt hàng kia; hoặc khan giả xem phim này sẽ có xu hướng thích xem phim kia, dựa vào đó ta có thể xây dựng những hệ thống gợi ý khách hàng (Recommendation System) nhằm thúc đẩy nhu cầu mua sắm hoặc xem phim&amp;hellip;.&lt;/p&gt;

&lt;p&gt;Một số thuật toán thuộc nhóm học không giám sát như Apriori (Association Rule), k-Means (Clustering), …&lt;/p&gt;

&lt;h2 id=&#34;c-học-bán-giám-sát&#34;&gt;c.   Học bán giám sát&lt;/h2&gt;

&lt;p&gt;Là bài toán mà khi tập dữ liệu đầu vào X là hỗn hợp các mẫu có nhãn và không có nhãn, trong đó số lượng có nhãn chỉ chiếm một phần nhỏ như minh hoạ ở Hình 4.&lt;/p&gt;

&lt;p&gt;Phần lớn các bài toán thực tế của ML thuộc nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được, chẳng hạn như ảnh y học hoặc các cặp câu song ngữ. Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/semi-supervisedlearning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 4: Semi-supervised Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Với bài toán này, mô hình phải tìm hiểu các cấu trúc để tổ chức dữ liệu cũng như đưa ra dự đoán. Vì đặc điểm trung gian nên ta có thể sử dụng unsupervised learning để khám phá và tìm hiểu cấu trúc trong dữ liệu đầu vào, đồng thời sử dụng supervised learning để dự đoán cho dữ liệu không được gán nhãn. Sau đó đưa dữ liệu vừa dự đoán trở lại làm dữ liệu huấn luyện cho supervised learning và sử dụng mô hình sau khi huấn luyện để đưa ra dự đoán về dữ liệu mới.&lt;/p&gt;

&lt;p&gt;Một số thuật toán học tăng cường như: Self Training, Generative models, S3VMs, Graph-Based Algorithms, Multiview Algorithms, …&lt;/p&gt;

&lt;h2 id=&#34;d-học-tăng-cường&#34;&gt;d.   Học tăng cường&lt;/h2&gt;

&lt;p&gt;Học tăng tường hay học củng cố là bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất. Hiện tại, reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nước đi tiếp theo để đạt được điểm số cao nhất. Hình 5 là một ví dụ đơn giản sử dụng học tăng cường.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/reinforcementlearning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 5: Minh hoạ cho học tăng cường được áp dụng trong lý thuyết trò chơi.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AlphaGo - một phần mềm chơi cờ vây trên máy tính được xây dựng bởi Google DeepMind hay chương trình dạy máy tính chơi game Mario là những ứng dụng sử dụng học tăng cường.&lt;/p&gt;

&lt;p&gt;Cờ vậy được xem là trò chơi có độ phức tạp cực kỳ cao với tổng số nước đi là xấp xỉ 1076110761, so với cờ vua là 1012010120, vì vậy thuật toán phải chọn ra một nước đi tối ưu trong số hàng tỉ tỉ lựa chọn. Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục tiêu cuối cùng của AlphaGo không phải là chơi như con người mà phải thắng được con người. Vì vậy, sau khi học xong các ván cờ của con người, AlphaGo tự chơi với chính nó thông qua hàng triệu ván cờ để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning.&lt;/p&gt;

&lt;p&gt;Đơn giản hơn cờ vây, tại một thời điểm cụ thể, người chơi game Mario chỉ cần bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào ứng với một chướng ngại vật cố định ở một vị trí cố định. Khi đó thuật toán trong ứng dụng dạy máy tính chơi game Mario sẽ nhận đầu vào là sơ đồ của màn hình tại thời điểm hiện hành, nhiệm vụ của thuật toán là tìm ra tổ hợp phím nên được bấm ứng với đầu vào đó. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa với thời gian bao lâu trong game, càng xa và càng nhanh thì điểm thưởng đạt được càng cao, tất nhiên điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra. Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa.&lt;/p&gt;

&lt;p&gt;Có nhiều cách khác nhau để thuật toán có thể mô hình hóa một vấn đề dựa trên sự tương tác của nó với dữ liệu đầu vào. Phân loại hoặc cách tổ chức thuật toán học máy này rất hữu ích vì nó buộc chúng ta phải suy nghĩ về vai trò của dữ liệu đầu vào và quy trình chuẩn bị mô hình và chọn một thuật toán phù hợp nhất cho vấn đề của chúng ta để có kết quả tốt nhất.&lt;/p&gt;

&lt;h1 id=&#34;2-phân-nhóm-dựa-trên-sự-tương-đồng&#34;&gt;2.    Phân nhóm dựa trên sự tương đồng&lt;/h1&gt;

&lt;p&gt;Dựa vào sự tương đồng về chức năng hay cách thức hoạt động mà các thuật toán sẽ được gom nhóm với nhau. Sau đây là danh sách các nhóm và các thuật toán theo từng nhóm.&lt;/p&gt;

&lt;h2 id=&#34;a-các-thuật-toán-hồi-quy-regression-algorithms&#34;&gt;a.   Các thuật toán hồi quy (Regression Algorithms)&lt;/h2&gt;

&lt;p&gt;Hồi quy là quá trình tìm mối quan hệ phụ thuộc của một biến (được gọi là biến phụ thuộc hay biến được giải thích, biến được dự báo, biến được hồi quy, biến phản ứng, biến nội sinh) vào một hoặc nhiều biến khác (được gọi là biến độc lập, biến giải thích, biến dự báo, biến hồi quy, biến tác nhân hay biến kiểm soát, biến ngoại sinh) nhằm mục đích ước lượng hoặc tiên đoán giá trị kỳ vọng của biến phụ thuộc khi biết trước giá trị của biến độc lập. Hình 6 tượng trưng cho ý tưởng của các thuật toán hồi quy.&lt;/p&gt;

&lt;p&gt;Ví dụ như, dự đoán rằng nếu tăng lãi suất tiền gửi thì sẽ huy động được lượng tiền gửi nhiều hơn, khi đó ngân hàng A cần biết mối quan hệ giữa lượng tiền gửi và lãi suất tiền gửi, cụ thể hơn họ muốn biết khi tăng lãi suất thêm 0.1% thì lượng tiền gửi sẽ tăng trung bình là bao nhiêu.&lt;/p&gt;

&lt;p&gt;Các thuật toán hồi quy phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Linear Regression&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Logistic Regression&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Locally Estimated Scatterplot Smoothing (LOESS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multivariate Adaptive Regression Splines (MARS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ordinary Least Squares Regression (OLSR)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stepwise Regression&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/regression-algorithn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 6: Regression Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;b-thuật-toán-dựa-trên-mẫu-instance-based-algorithms&#34;&gt;b.   Thuật toán dựa trên mẫu (Instance-based Algorithms)&lt;/h2&gt;

&lt;p&gt;Mô hình học tập dựa trên mẫu hay thực thể là bài toán ra quyết định dựa vào các trường hợp hoặc các mẫu dữ liệu huấn luyện được coi là quan trọng hay bắt buộc đối với mô hình.&lt;/p&gt;

&lt;p&gt;Nhóm thuật toán này thường xây dựng cơ sở dữ liệu về dữ liệu mẫu và so sánh dữ liệu mới với cơ sở dữ liệu bằng cách sử dụng thước đo tương tự để tìm kết quả phù hợp nhất và đưa ra dự đoán. Khi đó trọng tâm được đặt vào đại diện của các thể hiện được lưu trữ như minh hoạ ở Hình 7.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/instance-based-algorithms.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 7: Instance-based Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán dựa trên thực thể phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;k-Nearest Neighbor (kNN – k láng giềng gần nhất)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learning Vector Quantization (LVQ)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Locally Weighted Learning (LWL)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Self-Organizing Map (SOM)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;c-thuật-toán-chuẩn-hoá-regularization-algorithms&#34;&gt;c.   Thuật toán chuẩn hoá (Regularization Algorithms)&lt;/h2&gt;

&lt;p&gt;Các thuật toán chuẩn hoá ra đời từ sự mở rộng các phương pháp đã có (điển hình là các phương pháp hồi quy) bằng cách xử phạt các mô hình dựa trên mức độ phức tạp của chúng. Việc ưu tiên các mô hình đơn giản hơn cũng tốt hơn trong việc khái quát hóa. Hình 8 tượng trưng cho ý tưởng của thuật toán chuẩn hoá.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/regularization-algorithms.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 8: Regularization Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán chính quy phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Elastic Net&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Least Absolute Shrinkage and Selection Operator (LASSO)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Least-Angle Regression (LARS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ridge Regression&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;d-thuật-toán-cây-quyết-định-decision-tree-algorithms&#34;&gt;d.   Thuật toán cây quyết định (Decision Tree Algorithms)&lt;/h2&gt;

&lt;p&gt;Đây là phương pháp xây dựng mô hình ra quyết định dựa trên các giá trị thực của những thuộc tính trong dữ liệu. Sự quyết định được rẽ nhánh trong cấu trúc cây cho đến khi quyết định dự đoán được đưa ra cho một mẫu nhất định như minh hoạ ở Hình 9. Phương pháp này được sử dụng trong việc huấn luyện dữ liệu cho bài toán phân lớp và hồi quy. Vì sự nhanh chóng, chính xác nên phương pháp này rất được ưa chuộng trong ML.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/decision-tree-algorithm.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 9: Decision Tree Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán cây quyết định phổ biến nhất bao gồm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Chi-squared Automatic Interaction Detection (CHAID)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Classification và Regression Tree – CART&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Conditional Decision Trees&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;C4.5 và C5.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Decision Stump&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Iterative Dichotomiser 3 (ID3)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;M5&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;e-thuật-toán-bayes-bayesian-algorithms&#34;&gt;e.   Thuật toán Bayes (Bayesian Algorithms)&lt;/h2&gt;

&lt;p&gt;Đây là nhóm các thuật toán áp dụng Định lý Bayes cho bài toán phân loại và hồi quy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/bayessian-algorith.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 10: Bayesian Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Averaged One-Dependence Estimators (AODE)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bayesian Belief Network (BBN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bayesian Network (BN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gaussian Naive Bayes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multinomial Naive Bayes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Naive Bayes&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;f-thuật-toán-phân-cụm-clustering-algorithms&#34;&gt;f.   Thuật toán phân cụm (Clustering Algorithms)&lt;/h2&gt;

&lt;p&gt;Tất cả các phương pháp đều sử dụng các cấu trúc vốn có trong dữ liệu để tổ chức tốt nhất dữ liệu thành các nhóm có mức độ phổ biến tối đa dựa vào trọng tâm (centroid) và thứ bậc (hierarchal) như thể hiện ở Hình 11.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/clustering-algorithm.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 11: Clustering Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán phân cụm phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Expectation Maximisation (EM – cực đại hoá kỳ vọng)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hierarchical Clustering&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;k-Means&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;k-Medians&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;g-các-thuật-toán-luật-kết-hợp-association-rule-learning-algorithms&#34;&gt;g.   Các thuật toán luật kết hợp (Association Rule Learning Algorithms)&lt;/h2&gt;

&lt;p&gt;Đây là những thuật toán sẽ rút trích ra các quy tắc giải thích tốt nhất mối quan hệ giữa các biến trong dữ liệu. Các quy tắc này có thể giúp khám phá ra các tính chất quan trọng và hữu ích trong các tập dữ liệu lớn và cao chiều trong thương mại cùng các lĩnh vực khác. Hình 12 minh hoạ cho ý tưởng của thuật toán luật kết hợp.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/association-rule.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 12: Association Rule Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán luật kết hợp phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Apriori algorithm&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Eclat algorithm&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;FP-Growth algorithm&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;h-thuật-toán-mạng-nơron-nhân-tạo-artificial-neural-network-algorithms&#34;&gt;h.   Thuật toán mạng nơron nhân tạo (Artificial Neural Network Algorithms)&lt;/h2&gt;

&lt;p&gt;Mạng nơron nhân tạo là các mô hình được lấy cảm hứng từ cấu trúc và chức năng của mạng lưới thần kinh sinh học. Hình 13 minh hoạ cho một mạng truyền thẳng.
Nhóm thuật toán này có thể được sử dụng cho bài toán phân lớp và hồi quy với rất nhiều biến thể khác nhau cho hầu hết các vấn đề. Tuy nhiên, trong bài viết này mình chỉ trình bày các thuật toán cổ điển và phổ biến nhất:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Back-Propagation (mạng lan truyền ngược)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Perceptron (Mạng lan truyền thẳng)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multi-layer perceptron (Mạng truyền thẳng đa lớp)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hopfield Network&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Radial Basis Function Network (RBFN)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/neural-network-alg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 13: Artificial Neural Network Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;i-thuật-toán-học-sâu-deep-learning-algorithms&#34;&gt;i.   Thuật toán học sâu (Deep Learning Algorithms)&lt;/h2&gt;

&lt;p&gt;Thực chất Deep Learning là một bản cập nhật hiện đại cho Artificial Neural Networks nhằm khai thác khả năng tính toán của máy tính, tuy nhiên vì sự phát triển lớn mạnh của chúng nên mình tách ra thành một nhóm riêng.&lt;/p&gt;

&lt;p&gt;Deep Learning quan tâm đến việc xây dựng các mạng thần kinh lớn hơn, phức tạp hơn nhiều, và làm sao để khai thác hiệu quả các bộ dữ liệu lớn chứa rất ít dữ liệu đã được gán nhãn. Hình 14 minh hoạ cho ý tưởng của Deep learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/deep-learning-alg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 14: Deep Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán học sâu phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Convolutional Neural Network (CNN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep Belief Networks (DBN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep Boltzmann Machine (DBM)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stacked Auto-Encoders&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;j-nhóm-thuật-toán-giảm-chiều-dữ-liệu-dimensionality-reduction-algorithms&#34;&gt;j.   Nhóm thuật toán Giảm chiều dữ liệu (Dimensionality Reduction Algorithms)&lt;/h2&gt;

&lt;p&gt;Giống như các phương pháp phân cụm, giảm không gian tìm kiếm và khai thác cấu trúc vốn có trong dữ liệu nhưng theo cách không giám sát hoặc để tóm tắt hay mô tả dữ liệu sử dụng ít thông tin hơn là mục tiêu của nhóm phương pháp này. Hình 15 minh hoạ cho việc giảm chiều dữ liệu.&lt;/p&gt;

&lt;p&gt;Điều này có thể hữu ích để trực quan hóa dữ liệu hoặc đơn giản hóa dữ liệu mà sau đó có thể được sử dụng trong phương pháp học có giám sát. Nhiều trong số các phương pháp này có thể được điều chỉnh để sử dụng trong phân lớp và hồi quy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/demension-reducion.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 15: Dimensional Reduction Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán Giảm chiều dữ liệu phổ biến như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Flexible Discriminant Analysis (FDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Linear Discriminant Analysis (LDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mixture Discriminant Analysis (MDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multidimensional Scaling (MDS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Partial Least Squares Regression (PLSR)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Principal Component Analysis (PCA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Principal Component Regression (PCR)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Projection Pursuit&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Quadratic Discriminant Analysis (QDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sammon Mapping&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;k-thuật-toán-tập-hợp-ensemble-algorithms&#34;&gt;k.   Thuật toán tập hợp (Ensemble Algorithms)&lt;/h2&gt;

&lt;p&gt;Ensemble methods là những phương pháp kết hợp các mô hình yếu hơn được huấn luyện độc lập và phần dự đoán của chúng sẽ được kết hợp theo một cách nào đó để đưa ra dự đoán tổng thể như minh họa ở Hình 16.&lt;/p&gt;

&lt;p&gt;Nhóm thuật toán này khá mạnh và được nghiên cứu nhiều, đặc biệt là về cách để kết hợp các mô hình với nhau.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/ensemble-alg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 16: Ensemble Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Một số thuật toán phổ biến như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AdaBoost&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Boosting&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bootstrapped Aggregation (Bagging)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient Boosting Machines (GBM)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient Boosted Regression Trees (GBRT)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Random Forest&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stacked Generalization (blending)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;l-các-thuật-toán-khác&#34;&gt;l.   Các thuật toán khác&lt;/h2&gt;

&lt;p&gt;Còn rất nhiều các thuật toán khác không được liệt kê ở đây, chẳng hạn như Support Vector Machines (SVM), mình đang phân vân rằng liệu thuật toán này nên được đưa vào nhóm nào đó hay đứng một mình. Nếu dựa vào danh sách các biến thể và mức độ phát triển thì SVM có thể được tách thành một nhóm riêng – nhóm thuật toán sử dụng véctơ hỗ trợ.&lt;/p&gt;

&lt;p&gt;Thêm vào đó, các thuật toán được hình thành từ các nhiệm vụ đặc biệt, hoăc các thuật toán từ những nhánh con đặc biệt của ML cũng không được liệt kê vào các nhóm, chẳng hạn như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Feature selection algorithms&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Algorithm accuracy evaluation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Performance measures&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Có dịp mình sẽ bổ sung hoặc đề cập đến những thuật toán này ở một bài viết khác.&lt;/p&gt;

&lt;p&gt;Mặc dù rất hữu ích (dựa vào nhóm, người dùng sẽ dễ dàng nhớ được bản chất của thuật toán) nhưng phương pháp phân nhóm này chưa hoàn hảo ở điểm có những thuật toán có thể phù hợp với nhiều danh mục như Learning Vector Quantization, vừa là phương pháp lấy cảm hứng từ mạng thần kinh (neural network), vừa là phương pháp dựa trên cá thể (instance-based). Hoặc là thuật toán có cùng tên mô tả bài toán và nhóm thuật toán như Hồi quy (Regression) và Phân cụm (Clustering). Đối với những trường hợp này ta có thể giải quyết bằng cách liệt kê các thuật toán hai lần hoặc bằng cách chọn nhóm một cách chủ quan. Để tránh trùng lặp các thuật toán và giữ cho mọi thứ đơn giản thì có lẽ chọn nhóm theo cách chủ quan sẽ phù hợp hơn.&lt;/p&gt;

&lt;p&gt;Để giúp các bạn dễ nhớ cũng như tổng kết cho phần này mình đã vẽ một sơ đồ các thuật toán phân theo nhóm và sắp xếp theo alphabet, các bạn có thể xem thểm ở Hình 17 bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/machine-learning-branch.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 17: Sơ đồ phân nhóm thuật toán theo sự tương đồng&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hy vọng bài viết này sẽ mang lại hữu ích cho bạn đọc, nhất là giúp bạn có dược cái nhìn tổng quan về những gì hiện có và một số ý tưởng về cách liên kết các thuật toán với nhau.&lt;/p&gt;

&lt;p&gt;Danh sách các nhóm và thuật toán được liệt kê trong bài viết chỉ đảm bảo được yếu tố phổ biến tuy nhiên sẽ không đầy đủ. Vậy nên nếu bạn biết thêm thuật toán hoặc nhóm nào chưa được liệt kê ở đây hoặc kể cả cách phân nhóm thuật toán khác, cũng như sau khi đọc mà các bạn có bất kỳ góp ý, câu hỏi giúp cải thiện bài viết tốt hơn, các bạn có thể để lại bình luận nhằm chia sẻ cùng mình và những bạn đọc khác nhé.&lt;/p&gt;

&lt;p&gt;Tài liệu tham khảo:
A Tour of Machine Learning Algorithms by Jason Brownlee  in Understand Machine Learning Algorithms&lt;/p&gt;

&lt;p&gt;Semi-Supervised Learning Tutorial by Xiaojin Zhu&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms&#34;&gt;https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Top 10 algorithms in data mining by Xindong Wu · Vipin Kumar · J. Ross Quinlan · Joydeep Ghosh · Qiang Yang · Hiroshi Motoda · Geoffrey J. McLachlan · Angus Ng · Bing Liu · Philip S. Yu · Zhi-Hua Zhou · Michael Steinbach · David J. Hand · Dan Steinberg.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>