<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deeplearning on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/deeplearning/</link>
    <description>Recent content in Deeplearning on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>alexblack2202@gmail.com (Phạm Duy Tùng)</managingEditor>
    <webMaster>alexblack2202@gmail.com (Phạm Duy Tùng)</webMaster>
    <copyright>&amp;copy; 2018 Phạm Duy Tùng. Website chia sẻ kiến thức của Phạm Duy Tùng và Đặng Thị Hằng. Vui lòng liên hệ email alexblack2202@gmail.com nếu bạn có thông tin cần trao đổi.</copyright>
    <lastBuildDate>Fri, 13 Dec 2019 00:19:00 +0300</lastBuildDate>
    <atom:link href="/tags/deeplearning/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tìm hiểu Non-maximum Suppression (NMS)</title>
      <link>/blog/2019-12-25-nms/</link>
      <pubDate>Fri, 13 Dec 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-12-25-nms/</guid>
      <description>

&lt;h2 id=&#34;đặt-vấn-đề&#34;&gt;Đặt vấn đề&lt;/h2&gt;

&lt;p&gt;Sau khi thực hiện object detection feed một ảnh qua mạng neural, chúng ta sẽ thu được rất nhiều proposals (như hình ở dưới). Ở trạng thái này, có rất nhiều proposals là boding box cho một object duy nhất, điều này dẫn tới việc dư thừa. Chúng ta sử dụng thuật toán Non-maximum suppression (NMS) để giải quyết bài toán này.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/proposals.JPG&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 1: Proposals box, hình được cắt từ bài báo&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;thuật-toán-nms&#34;&gt;Thuật toán NMS&lt;/h2&gt;

&lt;p&gt;Đầu vào:&lt;/p&gt;

&lt;p&gt;Tập danh sách các proposals box ký hiệu là B với B ={b1,b2,&amp;hellip;,bn}, với bi là proposal thứ i.&lt;/p&gt;

&lt;p&gt;Tập điểm của mỗi proposal box ký hiệu là S với S={s1,s2,&amp;hellip;,sn}, si là điểm confidence của box bi&lt;/p&gt;

&lt;p&gt;Giá trị ngưỡng overlap threshold N.&lt;/p&gt;

&lt;p&gt;Cả hai giá trị bi và si đều là output của mạng neural network.&lt;/p&gt;

&lt;p&gt;Đầu ra:&lt;/p&gt;

&lt;p&gt;Một tập các proposals box D là tập các proposals đã loại bỏ dư thừa tương ứng với từng object trong hình.&lt;/p&gt;

&lt;p&gt;Thuật toán:&lt;/p&gt;

&lt;p&gt;Bước 1: Khởi tạo tập output  D = {}&lt;/p&gt;

&lt;p&gt;Bước 2: Chọn ra proposal box có điểm confidence cao nhất trong tập S, loại box đó ra khỏi tập S, B và thêm nó vào tập D.&lt;/p&gt;

&lt;p&gt;Bước 3: Tính giá trị IOU giữa proposal box mới vừa loại ra ở bước 2 với toàn bộ proposal box trong tập B. Nếu có bất kỳ box nào đó có giá trị IOU lớn hơn giá trị ngưỡng N thì loại box đó ra khỏi B, S.&lt;/p&gt;

&lt;p&gt;Bước 4: Lặp lại bước 2 đến khi nào không còn box nào có trong tập B.&lt;/p&gt;

&lt;p&gt;Điểm yếu của thuật toán:&lt;/p&gt;

&lt;p&gt;Nếu bạn đọc kỹ thuật toán, bạn sẽ thấy rằng toàn bộ quá trình loai bỏ những box dư thừa đều phụ thuộc vào giá trị ngưỡng N. Việc chọn lựa giá trị N chính là chìa khóa thành công của mô hình. Tuy nhiên, việc chọn giá trị ngưỡng này trong các bài toán khá khó. Và với việc chỉ sử dụng giá trị N, chúng ta sẽ gặp trường hợp dưới đây.&lt;/p&gt;

&lt;p&gt;Giả sửa giá trị ngưỡng N bạn chọn là 0.5. Có nghĩa là nếu box có giá trị lớn IOU đều bị loại bỏ, ngay cả với trường hợp điểm score si của nó  có giá trị cao. Ngược lại, giả sử box có điểm score si thấp nhưng IOU của nó nhỏ hơn 0.5, ví dụ o.49, thì nó lại được nhận.&lt;/p&gt;

&lt;p&gt;Và để giải quyết bài toán này Navaneeth Bodla đã đề xuất một cải tiến nhỏ và đặt tên thuật toán là Soft-NMS. ý tưởng được đề ra như sau: Thay vì phải loại bỏ hoàn toàn proposal, chúng ta sẽ giảm giá trị confidence của box đi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/soft_mns.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;soft-nms, hình được cắt từ bài báo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Với giá trị si được cập nhật lại như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/soft_nms_si.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;soft-nms, hình được cắt từ bài báo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi bài viết. Hẹn gặp lại các bạn ở những bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Tham khảo&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@yusuken/object-detction-1-nms-ed00d16fdcf9&#34;&gt;https://medium.com/@yusuken/object-detction-1-nms-ed00d16fdcf9&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c&#34;&gt;https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1704.04503.pdf&#34;&gt;https://arxiv.org/pdf/1704.04503.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1705.02950.pdf&#34;&gt;https://arxiv.org/pdf/1705.02950.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân loại hoa sử dụng pretrain model</title>
      <link>/blog/2019-04-15-phan-loai-hoa/</link>
      <pubDate>Mon, 15 Apr 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-15-phan-loai-hoa/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Ở trong bài viết này, chúng ta sẽ sử dụng tập dữ liệu là tập dữ liệu ở ở link &lt;a href=&#34;https://www.kaggle.com/alxmamaev/flowers-recognition&#34;&gt;https://www.kaggle.com/alxmamaev/flowers-recognition&lt;/a&gt;. Tập dữ liệu này bao gồm 4242 hình cảnh của 5 loại  hoa hồng (rose), hoa mặt trời (sunflower), hoa bồ công anh (dandelion), hoa cúc (daisy) và hoa tulip. Nhóm tác giả đã thu thập dữ liệu dựa trên các trang web  flicr, google images, yandex. Tập hình ảnh được chia thành 5 lớp, mỗi lớp có khoảng 800 hình, có kích thước xấp xỉ 320x320 pixel. Các hình ảnh có kích thước không đồng nhất với nhau.&lt;/p&gt;

&lt;h2 id=&#34;thực-hiện&#34;&gt;Thực hiện&lt;/h2&gt;

&lt;p&gt;Dữ liệu sau khi giản nén có dạng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir/classname1/*.*
data_dir/classname2/*.*
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cấu trúc lưu trũ như này đúng với mô hình của mình nên chúng ta cần nên chúng ta không thay đổi gì về câu trúc nữa, tiến hành viết code&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ load dataset lên và tranform nó để đưa vào huấn luyện.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import os
from collections import defaultdict
import numpy as np
import scipy.misc


def preprocess_input(x0):
    x = x0 / 255.
    x -= 0.5
    x *= 2.
    return x


def reverse_preprocess_input(x0):
    x = x0 / 2.0
    x += 0.5
    x *= 255.
    return x


def dataset(base_dir, n):
    print(&amp;quot;base dir: &amp;quot;+base_dir)
    print(&amp;quot;n: &amp;quot;+str(n))
    n = int(n)
    d = defaultdict(list)
    for root, subdirs, files in os.walk(base_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            assert file_path.startswith(base_dir)
            
            suffix = file_path[len(base_dir):]
            
            suffix = suffix.lstrip(&amp;quot;/&amp;quot;)
            suffix = suffix.lstrip(&amp;quot;\\&amp;quot;)
            if(suffix.find(&#39;/&#39;)&amp;gt;-1): #linux
                label = suffix.split(&amp;quot;/&amp;quot;)[0]
            else: #window
                label = suffix.split(&amp;quot;\\&amp;quot;)[0]
            d[label].append(file_path)
    print(&amp;quot;walk directory complete&amp;quot;)
    tags = sorted(d.keys())

    processed_image_count = 0
    useful_image_count = 0

    X = []
    y = []

    for class_index, class_name in enumerate(tags):
        filenames = d[class_name]
        for filename in filenames:
            processed_image_count += 1
            if processed_image_count%100 ==0:
                print(class_name+&amp;quot;\tprocess: &amp;quot;+str(processed_image_count)+&amp;quot;\t&amp;quot;+str(len(d[class_name])))
            img = scipy.misc.imread(filename)
            height, width, chan = img.shape
            assert chan == 3
            aspect_ratio = float(max((height, width))) / min((height, width))
            if aspect_ratio &amp;gt; 2:
                continue
            # We pick the largest center square.
            centery = height // 2
            centerx = width // 2
            radius = min((centerx, centery))
            img = img[centery-radius:centery+radius, centerx-radius:centerx+radius]
            img = scipy.misc.imresize(img, size=(n, n), interp=&#39;bilinear&#39;)
            X.append(img)
            y.append(class_index)
            useful_image_count += 1
    print(&amp;quot;processed %d, used %d&amp;quot; % (processed_image_count, useful_image_count))

    X = np.array(X).astype(np.float32)
    #X = X.transpose((0, 3, 1, 2))
    X = preprocess_input(X)
    y = np.array(y)

    perm = np.random.permutation(len(y))
    X = X[perm]
    y = y[perm]

    print(&amp;quot;classes:&amp;quot;,end=&amp;quot; &amp;quot;)
    for class_index, class_name in enumerate(tags):
        print(class_name, sum(y==class_index),end=&amp;quot; &amp;quot;)
    print(&amp;quot;X shape: &amp;quot;,X.shape)

    return X, y, tags
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đoạn code trên khá đơn giản và dễ hiểu. Lưu ý ở đây là với những bức ảnh có tỷ lệ width và height &amp;gt; 2 thì mình sẽ loại chúng ra khỏi tập dữ liệu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ xây dựng mô hình dựa trên mô hình Resnet50 có sẵn của kares, do sử dụng pretrain model, nên n-1 lớp trước đó sẽ không được huấn luyện và chúng ta sẽ sử dụng dụng các weight có sẵn đã được huấn luyện trên tập ImageNet rút đặc trưng cho mô hình. Chúng ta chỉ cần thêm một lớp full connected và softmax để phân lớp các loại hoa, công việc của chúng ta hiện tại là tìm ra trọng số của lớp full connected cuối cùng (thay vì huấn luyện lại hết toàn bộ mô hình).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# create the base pre-trained model
def build_model(nb_classes):
    base_model = ResNet50(weights=&#39;imagenet&#39;, include_top=False)

    # add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    # let&#39;s add a fully-connected layer
    x = Dense(1024, activation=&#39;relu&#39;)(x)
    # and a logistic layer
    predictions = Dense(nb_classes, activation=&#39;softmax&#39;)(x)

    # this is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)

    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional ResNet50 layers
    for layer in base_model.layers:
        layer.trainable = False

    return model
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visualize một chút xíu về kiến trúc inceptionV3 mình đang dùng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, None, None, 6 9472        conv1_pad[0][0]
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, None, None, 6 0           bn_conv1[0][0]
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, None, None, 6 0           activation_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           pool1_pad[0][0]
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, None, None, 6 4160        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, None, None, 6 0           bn2a_branch2a[0][0]
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, None, None, 6 36928       activation_2[0][0]
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, None, None, 6 0           bn2a_branch2b[0][0]
__________________________________________________________________________________________________
res2a_branch2c (Conv2D)         (None, None, None, 2 16640       activation_3[0][0]
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, None, None, 2 16640       max_pooling2d_1[0][0]
__________________________________________________________________________________________________
bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]
                                                                 bn2a_branch1[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, None, None, 2 0           add_1[0][0]
__________________________________________________________________________________________________
res2b_branch2a (Conv2D)         (None, None, None, 6 16448       activation_4[0][0]
__________________________________________________________________________________________________
bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, None, None, 6 0           bn2b_branch2a[0][0]
__________________________________________________________________________________________________
res2b_branch2b (Conv2D)         (None, None, None, 6 36928       activation_5[0][0]
__________________________________________________________________________________________________
bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, None, None, 6 0           bn2b_branch2b[0][0]
__________________________________________________________________________________________________
res2b_branch2c (Conv2D)         (None, None, None, 2 16640       activation_6[0][0]
__________________________________________________________________________________________________
bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]
                                                                 activation_4[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, None, None, 2 0           add_2[0][0]
__________________________________________________________________________________________________
res2c_branch2a (Conv2D)         (None, None, None, 6 16448       activation_7[0][0]
__________________________________________________________________________________________________
bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, None, None, 6 0           bn2c_branch2a[0][0]
__________________________________________________________________________________________________
res2c_branch2b (Conv2D)         (None, None, None, 6 36928       activation_8[0][0]
__________________________________________________________________________________________________
bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, None, None, 6 0           bn2c_branch2b[0][0]
__________________________________________________________________________________________________
res2c_branch2c (Conv2D)         (None, None, None, 2 16640       activation_9[0][0]
__________________________________________________________________________________________________
bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]
                                                                 activation_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, None, None, 2 0           add_3[0][0]
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, None, None, 1 32896       activation_10[0][0]
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, None, None, 1 0           bn3a_branch2a[0][0]
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, None, None, 1 147584      activation_11[0][0]
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, None, None, 1 0           bn3a_branch2b[0][0]
__________________________________________________________________________________________________
res3a_branch2c (Conv2D)         (None, None, None, 5 66048       activation_12[0][0]
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, None, None, 5 131584      activation_10[0][0]
__________________________________________________________________________________________________
bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]
                                                                 bn3a_branch1[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, None, None, 5 0           add_4[0][0]
__________________________________________________________________________________________________
res3b_branch2a (Conv2D)         (None, None, None, 1 65664       activation_13[0][0]
__________________________________________________________________________________________________
bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, None, None, 1 0           bn3b_branch2a[0][0]
__________________________________________________________________________________________________
res3b_branch2b (Conv2D)         (None, None, None, 1 147584      activation_14[0][0]
__________________________________________________________________________________________________
bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, None, None, 1 0           bn3b_branch2b[0][0]
__________________________________________________________________________________________________
res3b_branch2c (Conv2D)         (None, None, None, 5 66048       activation_15[0][0]
__________________________________________________________________________________________________
bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]
__________________________________________________________________________________________________
add_5 (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]
                                                                 activation_13[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, None, None, 5 0           add_5[0][0]
__________________________________________________________________________________________________
res3c_branch2a (Conv2D)         (None, None, None, 1 65664       activation_16[0][0]
__________________________________________________________________________________________________
bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, None, None, 1 0           bn3c_branch2a[0][0]
__________________________________________________________________________________________________
res3c_branch2b (Conv2D)         (None, None, None, 1 147584      activation_17[0][0]
__________________________________________________________________________________________________
bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, None, None, 1 0           bn3c_branch2b[0][0]
__________________________________________________________________________________________________
res3c_branch2c (Conv2D)         (None, None, None, 5 66048       activation_18[0][0]
__________________________________________________________________________________________________
bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]
__________________________________________________________________________________________________
add_6 (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]
                                                                 activation_16[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, None, None, 5 0           add_6[0][0]
__________________________________________________________________________________________________
res3d_branch2a (Conv2D)         (None, None, None, 1 65664       activation_19[0][0]
__________________________________________________________________________________________________
bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, None, None, 1 0           bn3d_branch2a[0][0]
__________________________________________________________________________________________________
res3d_branch2b (Conv2D)         (None, None, None, 1 147584      activation_20[0][0]
__________________________________________________________________________________________________
bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, None, None, 1 0           bn3d_branch2b[0][0]
__________________________________________________________________________________________________
res3d_branch2c (Conv2D)         (None, None, None, 5 66048       activation_21[0][0]
__________________________________________________________________________________________________
bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]
__________________________________________________________________________________________________
add_7 (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, None, None, 5 0           add_7[0][0]
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, None, None, 2 131328      activation_22[0][0]
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, None, None, 2 0           bn4a_branch2a[0][0]
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, None, None, 2 590080      activation_23[0][0]
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, None, None, 2 0           bn4a_branch2b[0][0]
__________________________________________________________________________________________________
res4a_branch2c (Conv2D)         (None, None, None, 1 263168      activation_24[0][0]
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, None, None, 1 525312      activation_22[0][0]
__________________________________________________________________________________________________
bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]
__________________________________________________________________________________________________
add_8 (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]
                                                                 bn4a_branch1[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, None, None, 1 0           add_8[0][0]
__________________________________________________________________________________________________
res4b_branch2a (Conv2D)         (None, None, None, 2 262400      activation_25[0][0]
__________________________________________________________________________________________________
bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, None, None, 2 0           bn4b_branch2a[0][0]
__________________________________________________________________________________________________
res4b_branch2b (Conv2D)         (None, None, None, 2 590080      activation_26[0][0]
__________________________________________________________________________________________________
bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, None, None, 2 0           bn4b_branch2b[0][0]
__________________________________________________________________________________________________
res4b_branch2c (Conv2D)         (None, None, None, 1 263168      activation_27[0][0]
__________________________________________________________________________________________________
bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]
__________________________________________________________________________________________________
add_9 (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]
                                                                 activation_25[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, None, None, 1 0           add_9[0][0]
__________________________________________________________________________________________________
res4c_branch2a (Conv2D)         (None, None, None, 2 262400      activation_28[0][0]
__________________________________________________________________________________________________
bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, None, None, 2 0           bn4c_branch2a[0][0]
__________________________________________________________________________________________________
res4c_branch2b (Conv2D)         (None, None, None, 2 590080      activation_29[0][0]
__________________________________________________________________________________________________
bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, None, None, 2 0           bn4c_branch2b[0][0]
__________________________________________________________________________________________________
res4c_branch2c (Conv2D)         (None, None, None, 1 263168      activation_30[0][0]
__________________________________________________________________________________________________
bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]
__________________________________________________________________________________________________
add_10 (Add)                    (None, None, None, 1 0           bn4c_branch2c[0][0]
                                                                 activation_28[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, None, None, 1 0           add_10[0][0]
__________________________________________________________________________________________________
res4d_branch2a (Conv2D)         (None, None, None, 2 262400      activation_31[0][0]
__________________________________________________________________________________________________
bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, None, None, 2 0           bn4d_branch2a[0][0]
__________________________________________________________________________________________________
res4d_branch2b (Conv2D)         (None, None, None, 2 590080      activation_32[0][0]
__________________________________________________________________________________________________
bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, None, None, 2 0           bn4d_branch2b[0][0]
__________________________________________________________________________________________________
res4d_branch2c (Conv2D)         (None, None, None, 1 263168      activation_33[0][0]
__________________________________________________________________________________________________
bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]
__________________________________________________________________________________________________
add_11 (Add)                    (None, None, None, 1 0           bn4d_branch2c[0][0]
                                                                 activation_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, None, None, 1 0           add_11[0][0]
__________________________________________________________________________________________________
res4e_branch2a (Conv2D)         (None, None, None, 2 262400      activation_34[0][0]
__________________________________________________________________________________________________
bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, None, None, 2 0           bn4e_branch2a[0][0]
__________________________________________________________________________________________________
res4e_branch2b (Conv2D)         (None, None, None, 2 590080      activation_35[0][0]
__________________________________________________________________________________________________
bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, None, None, 2 0           bn4e_branch2b[0][0]
__________________________________________________________________________________________________
res4e_branch2c (Conv2D)         (None, None, None, 1 263168      activation_36[0][0]
__________________________________________________________________________________________________
bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]
__________________________________________________________________________________________________
add_12 (Add)                    (None, None, None, 1 0           bn4e_branch2c[0][0]
                                                                 activation_34[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, None, None, 1 0           add_12[0][0]
__________________________________________________________________________________________________
res4f_branch2a (Conv2D)         (None, None, None, 2 262400      activation_37[0][0]
__________________________________________________________________________________________________
bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, None, None, 2 0           bn4f_branch2a[0][0]
__________________________________________________________________________________________________
res4f_branch2b (Conv2D)         (None, None, None, 2 590080      activation_38[0][0]
__________________________________________________________________________________________________
bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, None, None, 2 0           bn4f_branch2b[0][0]
__________________________________________________________________________________________________
res4f_branch2c (Conv2D)         (None, None, None, 1 263168      activation_39[0][0]
__________________________________________________________________________________________________
bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]
__________________________________________________________________________________________________
add_13 (Add)                    (None, None, None, 1 0           bn4f_branch2c[0][0]
                                                                 activation_37[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, None, None, 1 0           add_13[0][0]
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, None, None, 5 524800      activation_40[0][0]
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, None, None, 5 0           bn5a_branch2a[0][0]
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_41[0][0]
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, None, None, 5 0           bn5a_branch2b[0][0]
__________________________________________________________________________________________________
res5a_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_42[0][0]
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, None, None, 2 2099200     activation_40[0][0]
__________________________________________________________________________________________________
bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]
__________________________________________________________________________________________________
add_14 (Add)                    (None, None, None, 2 0           bn5a_branch2c[0][0]
                                                                 bn5a_branch1[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, None, None, 2 0           add_14[0][0]
__________________________________________________________________________________________________
res5b_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_43[0][0]
__________________________________________________________________________________________________
bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, None, None, 5 0           bn5b_branch2a[0][0]
__________________________________________________________________________________________________
res5b_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_44[0][0]
__________________________________________________________________________________________________
bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, None, None, 5 0           bn5b_branch2b[0][0]
__________________________________________________________________________________________________
res5b_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_45[0][0]
__________________________________________________________________________________________________
bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]
__________________________________________________________________________________________________
add_15 (Add)                    (None, None, None, 2 0           bn5b_branch2c[0][0]
                                                                 activation_43[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, None, None, 2 0           add_15[0][0]
__________________________________________________________________________________________________
res5c_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_46[0][0]
__________________________________________________________________________________________________
bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, None, None, 5 0           bn5c_branch2a[0][0]
__________________________________________________________________________________________________
res5c_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_47[0][0]
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, None, None, 5 0           bn5c_branch2b[0][0]
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_48[0][0]
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]
__________________________________________________________________________________________________
add_16 (Add)                    (None, None, None, 2 0           bn5c_branch2c[0][0]
                                                                 activation_46[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, None, None, 2 0           add_16[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 2048)         0           activation_49[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 5)            5125        dense_1[0][0]
==================================================================================================
Total params: 25,691,013
Trainable params: 2,103,301
Non-trainable params: 23,587,712
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phần train lại sẽ có khoảng hơn 2 triệu tham số, phần layer ở trước đó không train là khoảng 23 triệu tham số.&lt;/p&gt;

&lt;p&gt;Chia tập dữ liệu ra thành 5 phần, 4 phần làm tập train, 1 phần làm tập validation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y, tags = dataset.dataset(data_directory, n)
nb_classes = len(tags)


sample_count = len(y)
train_size = sample_count * 4 // 5
X_train = X[:train_size]
y_train = y[:train_size]
Y_train = np_utils.to_categorical(y_train, nb_classes)
X_test  = X[train_size:]
y_test  = y[train_size:]
Y_test = np_utils.to_categorical(y_test, nb_classes)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;chúng ta tiến hành thực hiện ImageDataGenerator để có được nhiều dữ liệu mẫu hơn và chống overfit, trong keras đã có sẵn hàm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;datagen = ImageDataGenerator(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        zca_whitening=False,
        rotation_range=45,
        width_shift_range=0.25,
        height_shift_range=0.25,
        horizontal_flip=True,
        vertical_flip=False,
        channel_shift_range=0.5,
        zoom_range=[0.5, 1.5],
        brightness_range=[0.5, 1.5],
        fill_mode=&#39;reflect&#39;)
        
datagen.fit(X_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cuối cùng, chúng ta sẽ xây dựng mô hình và tiến hành huấn luyện, lưu mô hình. Quá trình này tốn hơi nhiều thời gian.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = net.build_model(nb_classes)
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&amp;quot;accuracy&amp;quot;])

# train the model on the new data for a few epochs

print(&amp;quot;training the newly added dense layers&amp;quot;)

samples_per_epoch = X_train.shape[0]//batch_size*batch_size
steps_per_epoch = samples_per_epoch//batch_size
validation_steps = X_test.shape[0]//batch_size*batch_size

model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),
            samples_per_epoch=samples_per_epoch,
            epochs=nb_epoch,
            steps_per_epoch = steps_per_epoch,
            validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size),
            validation_steps=validation_steps,
            )


net.save(model, tags, model_file_prefix)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử download một vài hình ảnh trên mạng về rồi test thử xem sao&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../post_image/flower-classifition_demo.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả khá tốt phải không các bạn.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lựa chọn siêu tham số cho mô hình LSTM đơn giản sử dụng Keras</title>
      <link>/blog/2019-02-06-choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras/</link>
      <pubDate>Wed, 06 Feb 2019 00:20:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-06-choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras/</guid>
      <description>

&lt;h2 id=&#34;mở-đầu&#34;&gt;Mở đầu&lt;/h2&gt;

&lt;p&gt;Việc xây dựng một mô hình machine learning chưa bao giờ thật sự dễ dàng. Rất nhiều bài báo chỉ &amp;ldquo;show hàng&amp;rdquo; những thứ cao siêu, những thứ chỉ nằm trong sự tưởng tượng của chính các nhà báo. Còn khi đọc các bài báo khoa học về machine learning, tác giả công bố cho chúng ta những mô hình rất tốt, giải quyết một domain nhỏ vấn đề của họ. Tuy nhiên, có một thứ họ không/ chưa công bố. Đó là cách thức họ lựa chọn số lượng note ẩn, số lượng layer trong mô hình neural network. Trong bài viết này, chúng ta sẽ xây dựng mô hình LSTM đơn giản để dự đoán giới tính khi biết tên một người, và thử tìm xem công thức để chọn ra tham số &amp;ldquo;đủ tốt&amp;rdquo; là như thế nào.&lt;/p&gt;

&lt;h2 id=&#34;chẩn-bị-dữ-liệu&#34;&gt;Chẩn bị dữ liệu&lt;/h2&gt;

&lt;p&gt;Tập dữ liệu ở đây có khoảng 500000 tên kèm giới tính. Đầu tiên mình sẽ làm sạch dữ liệu bằng cách chỉ lấy giới tính là &amp;rsquo;m&amp;rsquo; và &amp;lsquo;f&amp;rsquo;, loại bỏ những tên quá ngắn (có ít hơn 3 ký tự)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filepath = &#39;firstnames.csv&#39;
max_rows = 500000 # Reduction due to memory limitations

df = (pd.read_csv(filepath, usecols=[&#39;name&#39;, &#39;gender&#39;],sep=&amp;quot;;&amp;quot;)
        .dropna(subset=[&#39;name&#39;, &#39;gender&#39;])
        .assign(name = lambda x: x.name.str.strip())
        .assign(gender = lambda x: x.gender.str.lower())
        .head(max_rows))

df= df[df.gender.isin([&#39;m&#39;,&#39;f&#39;])]

# In the case of a middle name, we will simply use the first name only
df[&#39;name&#39;] = df[&#39;name&#39;].apply(lambda x: str(x).split(&#39; &#39;, 1)[0])

# Sometimes people only but the first letter of their name into the field, so we drop all name where len &amp;lt;3
df.drop(df[df[&#39;name&#39;].str.len() &amp;lt; 3].index, inplace=True)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sử dụng một kỹ thuật khá cũ trong NLP là one-hot encoding. Mỗi ký tự được biểu diễn bởi một vector nhị phân. Ví dụ có 26 ký tự trong bảng chữ cái tiếng anh, vector đại diện cho chữ a là [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], ký tự b được biểu diễn là [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], &amp;hellip; tương tự cho đến z.&lt;/p&gt;

&lt;p&gt;Một từ được encode là một tập các vector. Ví dụ chữ hello được biểu diễn là&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #h,
 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #e,
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #l,
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #l,
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #o]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đọc đến đây, chắc các bạn đã mườn tượng ra rằng một từ sẽ được encode như thế nào rồi phải không. Tiếp theo, chúng ta sẽ xây dựng hàm encode cho tập dữ liệu&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; # Define a mapping of chars to integers
char_to_int = dict((c, i) for i, c in enumerate(accepted_chars))
int_to_char = dict((i, c) for i, c in enumerate(accepted_chars))

# Removes all non accepted characters
def normalize(line):
    return [c.lower() for c in line if c.lower() in accepted_chars]

# Returns a list of n lists with n = word_vec_length
def name_encoding(name):

    # Encode input data to int, e.g. a-&amp;gt;1, z-&amp;gt;26
    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i &amp;lt; word_vec_length]
    
    # Start one-hot-encoding
    onehot_encoded = list()
    
    for value in integer_encoded:
        # create a list of n zeros, where n is equal to the number of accepted characters
        letter = [0 for _ in range(char_vec_length)]
        letter[value] = 1
        onehot_encoded.append(letter)
        
    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array
    for _ in range(word_vec_length - len(name)):
        onehot_encoded.append([0 for _ in range(char_vec_length)])
        
    return onehot_encoded

# Encode the output labels
def lable_encoding(gender_series):
    labels = np.empty((0, 2))
    for i in gender_series:
        if i == &#39;m&#39;:
            labels = np.append(labels, [[1,0]], axis=0)
        else:
            labels = np.append(labels, [[0,1]], axis=0)
    return labels
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và tiến hành chia tập dữ liệu thành train, val, và test set&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
# Split dataset in 60% train, 20% test and 20% validation
train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])

# Convert both the input names as well as the output lables into the discussed machine readable vector format
train_x =  np.asarray([np.asarray(name_encoding(normalize(name))) for name in train[predictor_col]])
train_y = lable_encoding(train.gender)

validate_x = np.asarray([name_encoding(normalize(name)) for name in validate[predictor_col]])
validate_y = lable_encoding(validate.gender)

test_x = np.asarray([name_encoding(normalize(name)) for name in test[predictor_col]])
test_y = lable_encoding(test.gender)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vậy là chúng ta đã có chuẩn bị xong dữ liệu đầy đủ rồi đó. Bây giờ chúng ta xây dựng mô hình thôi.&lt;/p&gt;

&lt;h2 id=&#34;xây-dựng-mô-hình&#34;&gt;Xây dựng mô hình&lt;/h2&gt;

&lt;p&gt;Có rất nhiều cách để chọn tham số cho mô hình, ví dụ như ở &lt;a href=&#34;https://stats.stackexchange.com/questions/95495/guideline-to-select-the-hyperparameters-in-deep-learning&#34;&gt;https://stats.stackexchange.com/questions/95495/guideline-to-select-the-hyperparameters-in-deep-learning&lt;/a&gt;
 liệt kê ra 4 cách là Manual Search, Grid Search, Random Search, Bayesian Optimization. Tuy nhiên,  những cách trên đều khá tốn thời gian và đòi hỏi người kỹ sư phải có am hiểu nhất định.&lt;/p&gt;

&lt;p&gt;Ở đây, chúng ta sử dụng một công thức được đưa ra trong link &lt;a href=&#34;https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw/136542#136542&#34;&gt;https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw/136542#136542&lt;/a&gt;, cụ thể&lt;/p&gt;

&lt;p&gt;$$ N_h = \frac{N_s}{(\alpha * (N_i + N_o))}$$&lt;/p&gt;

&lt;p&gt;Trong đó Ni là số lượng input neural, No là số lượng output neural, Ns là số lượng element trong tập dữ liệu train. alpha là một con số trade-off đại diện cho tỷ lệ thuộc đoạn [2-10].&lt;/p&gt;

&lt;p&gt;Một lưu ý ở đây là bạn có thể dựa vào công thức và số alpha mà ước lượng xem rằng bạn đã có đủ dữ liệu mẫu hay chưa. Một ví dụ đơn giản là giả sử bạn có 10,000 mẫu dữ liệu, input số từ 0 đến 9, output là 64, chọn alpha ở mức nhỏ nhất là 2, vậy theo công thức số neural ẩn là 10000/(2*64*10) = 7.8 ~ 8. Nếu bạn tăng số alpha lên thì số hidden layer còn ít nữa. Điều trên chứng tỏ rằng số lượng mẫu của bạn chưa đủ, còn thiếu quá nhiều.  Nếu bạn tăng gấp 100 lần số dữ liệu mẫu, thì con số có vẻ hợp lý hơn.&lt;/p&gt;

&lt;p&gt;Trong tập dữ liệu, mình có:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;The input vector will have the shape  {17} x {82}
Train len:  (21883, 17, 82) 36473

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tổng cộng N_s là 21883, Ni là 17, No là 82, chọn alpha là 2 thì mình có 21883/(2*17*82) = 7.8 ~ 8. Một con số khá nhỏ, chứng tỏ dữ liệu của mình còn quá ít.&lt;/p&gt;

&lt;p&gt;Đối với tập dữ liệu nhỏ như thế này, mình thường sẽ áp dụng công thức sau:&lt;/p&gt;

&lt;p&gt;$$ N_h= \beta* (N_i + N_o) $$&lt;/p&gt;

&lt;p&gt;Với beta là một con số thực thuộc nửa đoạn (0,1]. Thông thường sẽ là &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;. Kết quả là số lượng neural của mình khoảng 929.333 node. Thông thường, mình sẽ chọn số neural là một con số là bội số của 2, ở đây 929 gần với 2^10 nhất, nên mình chọn số neural là 2^10.&lt;/p&gt;

&lt;p&gt;Tóm lại, mình sẽ theo quy tắc&lt;/p&gt;

&lt;p&gt;Nếu dữ liệu nhiều:&lt;/p&gt;

&lt;p&gt;$$ N_h = \frac{N_s}{(\alpha * (N_i + N_o))}$$&lt;/p&gt;

&lt;p&gt;Nếu dữ liệu ít&lt;/p&gt;

&lt;p&gt;$$ N_h= \frac{2}{3}* (N_i + N_o) $$&lt;/p&gt;

&lt;p&gt;Làm tròn lên bằng với bội số của 2 mũ gần nhất.&lt;/p&gt;

&lt;p&gt;Một lưu ý nhỏ là số lượng node càng nhiều thì tỷ lệ overfit càng cao, và thời gian huấn luyện càng lâu. Do đó, bạn nên trang bị máy có cấu hình kha khá một chút, tốt hơn hết là nên có GPU đi kèm. Ngoài ra, bạn nên chuẩn bị càng nhiều dữ liệu càng tốt. Một kinh nghiệm của mình rút ra trong quá trình làm Machine Learning là nếu không có nhiều dữ liệu, thì đừng cố thử áp dụng các phương pháp ML trên nó.&lt;/p&gt;

&lt;p&gt;Mô hình mình xây dựng như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
hidden_nodes = 1024


# Build the model
print(&#39;Build model...&#39;)
model = Sequential()
model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))
model.add(Dropout(0.2))
model.add(Dense(units=output_labels))
model.add(Activation(&#39;softmax&#39;))
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;acc&#39;])

batch_size=1000
model.fit(train_x, train_y, batch_size=batch_size, epochs=50, validation_data=(validate_x, validate_y))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Do bài viết chỉ tập trung vào vấn đề lựa chọn số lượng node, nên mình sẽ bỏ qua những phần phụ như là early stoping, save each epochs &amp;hellip;, Các vấn đề trên ít nhiều mình đã đề cập ở các bài viết trước.&lt;/p&gt;

&lt;p&gt;Kết quả của việc huấn luyện mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 21883/21883 [==============================] - 34s 2ms/step - loss: 0.6602 - acc: 0.6171 - val_loss: 0.6276 - val_acc: 0.7199
Epoch 2/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5836 - acc: 0.7056 - val_loss: 0.5625 - val_acc: 0.7193
Epoch 3/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5531 - acc: 0.7353 - val_loss: 0.5506 - val_acc: 0.7389
Epoch 4/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.5480 - acc: 0.7446 - val_loss: 0.5664 - val_acc: 0.7313
Epoch 5/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5406 - acc: 0.7420 - val_loss: 0.5247 - val_acc: 0.7613
Epoch 6/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5077 - acc: 0.7686 - val_loss: 0.4918 - val_acc: 0.7790
Epoch 7/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4825 - acc: 0.7837 - val_loss: 0.4939 - val_acc: 0.7740
Epoch 8/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.4611 - acc: 0.7887 - val_loss: 0.4407 - val_acc: 0.8037
Epoch 9/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4421 - acc: 0.7987 - val_loss: 0.4657 - val_acc: 0.8005
Epoch 10/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4293 - acc: 0.8055 - val_loss: 0.4183 - val_acc: 0.8141
Epoch 11/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.4129 - acc: 0.8128 - val_loss: 0.4171 - val_acc: 0.8212
Epoch 12/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4153 - acc: 0.8141 - val_loss: 0.4031 - val_acc: 0.8188
Epoch 13/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3978 - acc: 0.8191 - val_loss: 0.3918 - val_acc: 0.8280
Epoch 14/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3910 - acc: 0.8268 - val_loss: 0.3831 - val_acc: 0.8276
Epoch 15/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3848 - acc: 0.8272 - val_loss: 0.3772 - val_acc: 0.8314
Epoch 16/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3751 - acc: 0.8354 - val_loss: 0.3737 - val_acc: 0.8363
Epoch 17/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3708 - acc: 0.8345 - val_loss: 0.3717 - val_acc: 0.8374
Epoch 18/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3688 - acc: 0.8375 - val_loss: 0.3768 - val_acc: 0.8330
Epoch 19/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3704 - acc: 0.8375 - val_loss: 0.3621 - val_acc: 0.8392
Epoch 20/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3608 - acc: 0.8444 - val_loss: 0.3656 - val_acc: 0.8422
Epoch 21/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3548 - acc: 0.8459 - val_loss: 0.3670 - val_acc: 0.8417
Epoch 22/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3521 - acc: 0.8452 - val_loss: 0.3555 - val_acc: 0.8462
Epoch 23/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3432 - acc: 0.8504 - val_loss: 0.3591 - val_acc: 0.8402
Epoch 24/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3415 - acc: 0.8524 - val_loss: 0.3471 - val_acc: 0.8470
Epoch 25/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3355 - acc: 0.8555 - val_loss: 0.3577 - val_acc: 0.8436
Epoch 26/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3320 - acc: 0.8552 - val_loss: 0.3602 - val_acc: 0.8430
Epoch 27/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3294 - acc: 0.8578 - val_loss: 0.3565 - val_acc: 0.8485
Epoch 28/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3235 - acc: 0.8602 - val_loss: 0.3427 - val_acc: 0.8514
Epoch 29/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3138 - acc: 0.8651 - val_loss: 0.3523 - val_acc: 0.8470
Epoch 30/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3095 - acc: 0.8683 - val_loss: 0.3457 - val_acc: 0.8487
Epoch 31/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3064 - acc: 0.8701 - val_loss: 0.3538 - val_acc: 0.8531
Epoch 32/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2985 - acc: 0.8717 - val_loss: 0.3555 - val_acc: 0.8455
Epoch 33/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2930 - acc: 0.8741 - val_loss: 0.3430 - val_acc: 0.8525
Epoch 34/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2901 - acc: 0.8786 - val_loss: 0.3457 - val_acc: 0.8503
Epoch 35/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2852 - acc: 0.8776 - val_loss: 0.3458 - val_acc: 0.8510
Epoch 36/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2817 - acc: 0.8811 - val_loss: 0.3445 - val_acc: 0.8568
Epoch 37/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2780 - acc: 0.8816 - val_loss: 0.3356 - val_acc: 0.8540
Epoch 38/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2734 - acc: 0.8852 - val_loss: 0.3442 - val_acc: 0.8559
Epoch 39/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2579 - acc: 0.8904 - val_loss: 0.3552 - val_acc: 0.8540
Epoch 40/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2551 - acc: 0.8927 - val_loss: 0.3677 - val_acc: 0.8532
Epoch 41/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2558 - acc: 0.8921 - val_loss: 0.3496 - val_acc: 0.8588
Epoch 42/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2472 - acc: 0.8963 - val_loss: 0.3534 - val_acc: 0.8587
Epoch 43/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2486 - acc: 0.8948 - val_loss: 0.3490 - val_acc: 0.8537
Epoch 44/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2503 - acc: 0.8965 - val_loss: 0.3594 - val_acc: 0.8552
Epoch 45/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2391 - acc: 0.8993 - val_loss: 0.3793 - val_acc: 0.8566
Epoch 46/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2244 - acc: 0.9048 - val_loss: 0.3815 - val_acc: 0.8543
Epoch 47/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2203 - acc: 0.9095 - val_loss: 0.3848 - val_acc: 0.8554
Epoch 48/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2221 - acc: 0.9051 - val_loss: 0.3892 - val_acc: 0.8558
Epoch 49/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2117 - acc: 0.9124 - val_loss: 0.3654 - val_acc: 0.8544
Epoch 50/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2141 - acc: 0.9118 - val_loss: 0.3726 - val_acc: 0.8547


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Độ chính xác trên tập train là hơn 90%, trên tập val là hơn 85%. Nhìn kỹ hơn vào những từ sai ta thấy rằng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;             name gender predicted_gender
6750       Chiaki      f                m
28599      Naheed      f                m
11448  Espiridión      m                f
895       Akmaral      f                m
33778         Ros      f                m

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Có một sự nhập nhằng ở ngôn ngữ giữa tên nam và tên nữ ở những từ này. Có lẽ một tập dữ liệu với đầy đủ họ và tên sẽ cho ra một kết quả có độ chính xác cao hơn. Ví dụ, ở Việt Nam, tên Ngọc thì có thể đặt được cho cả Nam lẫn Nữ.&lt;/p&gt;

&lt;p&gt;Mình sẽ cố gắng kiếm một bộ dataset tên tiếng việt và thực hiện việc xây dựng mô hình xác định giới tính thông qua tên người dựa vào mô hình LSTM.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Giảm bộ nhớ sử dụng trong python</title>
      <link>/blog/2019-02-06-how-to-reduce-memory-consumption-by-half-by-adding-just-one-line-of-code/</link>
      <pubDate>Wed, 06 Feb 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-06-how-to-reduce-memory-consumption-by-half-by-adding-just-one-line-of-code/</guid>
      <description>

&lt;h2 id=&#34;mở-đầu&#34;&gt;Mở đầu&lt;/h2&gt;

&lt;p&gt;Bắt đầu bằng một class đơn giản như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DataItem(object):
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bạn nghĩ một đối tượng của class trên sẽ chiếm bao nhiêu bộ nhớ. Chúng ta cùng tiến hành một vài thí nghiệm nho nhỏ bên dưới.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dx = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;-&amp;quot;)
print (&amp;quot;sys.getsizeof(dx):&amp;quot;, sys.getsizeof(dx))
&amp;gt;&amp;gt; sys.getsizeof(dx): 56
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả ra là &lt;em&gt;56 bytes&lt;/em&gt;, khá hợp lý phải không các bạn. Thử với một ví dụ khác xem sao nhỉ.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dy = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;I am working at MWG&amp;quot;)
print (&amp;quot;sys.getsizeof(dy):&amp;quot;, sys.getsizeof(dy))
&amp;gt;&amp;gt; sys.getsizeof(dy): 56
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả vẫn là &lt;em&gt;56 bytes&lt;/em&gt;. Có cái gì đó sai sai ở đây không nhỉ?&lt;/p&gt;

&lt;p&gt;Chúng ta thực nghiệm một vài thí nghiệm khác để chứng thực.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print (sys.getsizeof(&amp;quot;&amp;quot;))
&amp;gt;&amp;gt; 49
print (sys.getsizeof(&amp;quot;1&amp;quot;))
&amp;gt;&amp;gt; 50
print (sys.getsizeof(1))
&amp;gt;&amp;gt; 28
print (sys.getsizeof(dict()))
&amp;gt;&amp;gt; 240
print (sys.getsizeof({}))
&amp;gt;&amp;gt; 240
print (sys.getsizeof(list()))
&amp;gt;&amp;gt; 64
print (sys.getsizeof([]))
&amp;gt;&amp;gt; 64
print (sys.getsizeof(()))
&amp;gt;&amp;gt; 48
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một điều cực kỳ bất ngờ đã xuất hiện ở đây. Một chuỗi rỗng chiếm đến tận &lt;em&gt;49 bytes&lt;/em&gt;, một dictionary rỗng, không chứa phần tử nào chiếm đến &lt;em&gt;240 bytes&lt;/em&gt;, và một list rỗng chiếm tới &lt;em&gt;64 bytes&lt;/em&gt;. Rõ ràng, python đã lưu một số thứ gì đó ngoài dữ liệu của mình.&lt;/p&gt;

&lt;p&gt;Đi sâu vào thử tìm hiểu những thứ &amp;lsquo;linh kiện&amp;rsquo; linh tinh mà python đã kèm theo cho chúng ta là gì nhé.&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ cần một hàm in ra những thứ mà python đã &amp;lsquo;nhúng&amp;rsquo; thêm vào class DataItem chúng ta khai báo ở trên.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def dump(obj):
  for attr in dir(obj):
    print(&amp;quot;  obj.%s = %r&amp;quot; % (attr, getattr(obj, attr)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;và dump biến dy ra thôi&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dump(dy)

obj.__class__ = &amp;lt;class &#39;__main__.DataItem&#39;&amp;gt;
  obj.__delattr__ = &amp;lt;method-wrapper &#39;__delattr__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__dict__ = {&#39;name&#39;: &#39;Alex Black&#39;, &#39;age&#39;: 42, &#39;address&#39;: &#39;i am working at MWG&#39;}
  obj.__dir__ = &amp;lt;built-in method __dir__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__doc__ = None
  obj.__eq__ = &amp;lt;method-wrapper &#39;__eq__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__format__ = &amp;lt;built-in method __format__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__ge__ = &amp;lt;method-wrapper &#39;__ge__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__getattribute__ = &amp;lt;method-wrapper &#39;__getattribute__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__gt__ = &amp;lt;method-wrapper &#39;__gt__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__hash__ = &amp;lt;method-wrapper &#39;__hash__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__init__ = &amp;lt;bound method DataItem.__init__ of &amp;lt;__main__.DataItem object at 0x000001A64A6DD0F0&amp;gt;&amp;gt;
  obj.__init_subclass__ = &amp;lt;built-in method __init_subclass__ of type object at 0x000001A64A5DE738&amp;gt;
  obj.__le__ = &amp;lt;method-wrapper &#39;__le__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__lt__ = &amp;lt;method-wrapper &#39;__lt__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__module__ = &#39;__main__&#39;
  obj.__ne__ = &amp;lt;method-wrapper &#39;__ne__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__new__ = &amp;lt;built-in method __new__ of type object at 0x000000005C2DC580&amp;gt;
  obj.__reduce__ = &amp;lt;built-in method __reduce__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__reduce_ex__ = &amp;lt;built-in method __reduce_ex__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__repr__ = &amp;lt;method-wrapper &#39;__repr__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__setattr__ = &amp;lt;method-wrapper &#39;__setattr__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__sizeof__ = &amp;lt;built-in method __sizeof__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__str__ = &amp;lt;method-wrapper &#39;__str__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__subclasshook__ = &amp;lt;built-in method __subclasshook__ of type object at 0x000001A64A5DE738&amp;gt;
  obj.__weakref__ = None
  obj.address = &#39;i am working at MWG&#39;
  obj.age = 42
  obj.name = &#39;Alex Black&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow, có vẻ khá là đồ sộ nhỉ.&lt;/p&gt;

&lt;p&gt;Trên github, có một hàm có sẵn tính toán số lượng bộ nhớ mà object chiếm được dựa vào cách truy xuất trực tiếp từng trường dữ liệu của đối tượng và tính toán kích thước&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys

def get_size(obj, seen=None):
    &amp;quot;&amp;quot;&amp;quot;Recursively finds size of objects&amp;quot;&amp;quot;&amp;quot;
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    # Important mark as seen *before* entering recursion to gracefully handle
    # self-referential objects
    seen.add(obj_id)
    if isinstance(obj, dict):
        size += sum([get_size(v, seen) for v in obj.values()])
        size += sum([get_size(k, seen) for k in obj.keys()])
    elif hasattr(obj, &#39;__dict__&#39;):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, &#39;__iter__&#39;) and not isinstance(obj, (str, bytes, bytearray)):
        size += sum([get_size(i, seen) for i in obj])
    return size
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;thử với 2 biến dx và dy của chúng ta xem sao&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; print (&amp;quot;get_size(d1):&amp;quot;, get_size(dx))
get_size(d1): 466
&amp;gt;&amp;gt;&amp;gt; print (&amp;quot;get_size(d1):&amp;quot;, get_size(dy))
get_size(d1): 484
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng tốn lần lượt là 466 và 484 bytes. Có vẻ đúng đó nhỉ.&lt;/p&gt;

&lt;p&gt;Điều chúng ta quan tâm lúc này là có cách nào để giảm bộ nhớ tiêu thụ của một object hay không?&lt;/p&gt;

&lt;h2 id=&#34;giảm-bộ-nhớ-tiêu-thụ-của-một-đối-tượng-trong-python&#34;&gt;Giảm bộ nhớ tiêu thụ của một đối tượng trong python&lt;/h2&gt;

&lt;p&gt;Tất nhiên là sẽ có cách giảm. Python là một ngôn ngữ thông dịch, và nó cho phép chúng ta mở rộng lớp bất kể lúc nào bằng cách thêm một/ nhiều trường dữ liệu.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dz = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;-&amp;quot;)
dz.height = 1.80
print ( get_size(dz))
&amp;gt;&amp;gt; 484
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chính vì lý do này, trình biên dịch sẽ tốn thêm một đống bộ nhớ tạm để chúng ta có thể dễ dàng mở rộng một lớp trong tương lai. Nếu chúng ta &amp;ldquo;ép buộc&amp;rdquo; trình biên dịch, nói rằng chúng ta chỉ có nhiêu đó trường, và bỏ phần dư thừa đi.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DataItem(object):
    __slots__ = [&#39;name&#39;, &#39;age&#39;, &#39;address&#39;]
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và thử lại&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
dz = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;i am working at MWG&amp;quot;)
print (&amp;quot;sys.getsizeof(dz):&amp;quot;, get_size(dz))

&amp;gt;&amp;gt;sys.getsizeof(dz): 64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Các bạn thấy gì không, bộ nhớ tiêu thụ chỉ là &amp;ldquo;64 bytes&amp;rdquo;. Dung lượng đã giảm đi hơn &amp;ldquo;7 lần&amp;rdquo; so với model class ban đầu. Tuy nhiên, chúng ta sẽ không thể mở rộng class dễ dàng như xưa nữa.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; dz.height = 1.80
Traceback (most recent call last):
  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;
AttributeError: &#39;DataItem&#39; object has no attribute &#39;height&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử tạo một đối tượng có 1000 phần tử và kiểm tra thử.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DataItem(object):
    __slots__ = [&#39;name&#39;, &#39;age&#39;, &#39;address&#39;]
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address


data = []

tracemalloc.start()
start =datetime.datetime.now()
for p in range(100000):
    data.append(DataItem(&amp;quot;Alex&amp;quot;, 42, &amp;quot;middle of nowhere&amp;quot;))
    
end =datetime.datetime.now()
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics(&#39;lineno&#39;)
total = sum(stat.size for stat in top_stats)
print(&amp;quot;Total allocated size: %.1f MB&amp;quot; % (total / (1024*1024)))
print(&amp;quot;Total execute time:&amp;quot;,(end-start).microseconds)

&amp;gt;&amp;gt; Total allocated size: 6.9 MB
&amp;gt;&amp;gt; Total execute time: 232565
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bỏ dòng &lt;strong&gt;slots&lt;/strong&gt; = [&amp;lsquo;name&amp;rsquo;, &amp;lsquo;age&amp;rsquo;, &amp;lsquo;address&amp;rsquo;] đi thử&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
class DataItem(object):
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address


data = []

tracemalloc.start()
start =datetime.datetime.now()
for p in range(100000):
    data.append(DataItem(&amp;quot;Alex&amp;quot;, 42, &amp;quot;middle of nowhere&amp;quot;))
end =datetime.datetime.now()
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics(&#39;lineno&#39;)
total = sum(stat.size for stat in top_stats)
print(&amp;quot;Total allocated size: %.1f MB&amp;quot; % (total / (1024*1024)))
print(&amp;quot;Total execute time:&amp;quot;,(end-start).microseconds)

&amp;gt;&amp;gt; Total allocated size: 16.8 MB
&amp;gt;&amp;gt; Total execute time: 240772
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So sánh thử, chúng ta thấy rằng số lượng RAM giảm đi khá nhiều, thời gian thực thi khá tương đương nhau (có giảm một chút).&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 mẹo hay sử dụng python</title>
      <link>/blog/2019-02-05-5-python-tricks-you-need-to-know-today/</link>
      <pubDate>Tue, 05 Feb 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-05-5-python-tricks-you-need-to-know-today/</guid>
      <description>

&lt;h2 id=&#34;mở-đầu&#34;&gt;Mở đầu&lt;/h2&gt;

&lt;p&gt;Hiện nay, có rất nhiều thư viện do cộng đồng đóng góp và xây dựng. Ví dụ như biopython trong tin sinh học, pandas (data science), keras/tensorflow (machine learning), astropy ( cho thiên văn học - astronomy). Trước khi bắt đầu đọc bài viết này, bạn đên đọc &amp;ldquo;Python Tricks Book&amp;rdquo; của Dan Bader trước (&lt;a href=&#34;https://dbader.org/products/python-tricks-book/&#34;&gt;https://dbader.org/products/python-tricks-book/&lt;/a&gt;). Trong sách, anh ấy đã chia sẻ một số lời khuyên và mẹo về các code python hiệu quả hơn.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-số-1-sức-mạnh-của-một-dòng&#34;&gt;Mẹo số 1: Sức mạnh của một dòng&lt;/h2&gt;

&lt;p&gt;Khi bạn đọc một đoạn giải thuật với nhiều dòng code, có thể bạn sẽ bị quên thông tin những dòng trước đó đã viết gì, đặc biệt là trong những câu lệnh điều kiện. Ví dụ:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
if alpha &amp;gt; 7:
     beta = 999
elif alpha == 7:
    beta = 99
else:
   beta =0

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chóng ta có thể viết đơn giản hơn chỉ với một dòng code như sau.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;beta = 999 if alpha &amp;gt; 7 else 99 if alpha == 7 else 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;thật đơn giản phải không. Bạn chỉ cần nhìn đúng một dòng là nằm được nội dung ý nghĩa của đoạn code bạn cần. Một ví dụ khác về vòng lặp for.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lst = [1, 2, 3, 4] 
lst_double = []

for num in lst:
    lst_double.append(num * 2)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đoạn code trên có thể viết lại dưới dạng 1 dòng như sau.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lst_double = [num * 2 for num in lst]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tất nhiên, bạn không nên &amp;ldquo;lạm dụng&amp;rdquo; one line một cách thái quá, ví dụ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pprint; pprint.pprint(zip((&#39;Byte&#39;, &#39;KByte&#39;, &#39;MByte&#39;, &#39;GByte&#39;, &#39;TByte&#39;), (1 &amp;lt;&amp;lt; 10*i for i in xrange(5))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trông nó có vẻ hơi &amp;ldquo;lố bịch&amp;rdquo; phải không.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-2-các-thao-tác-nhanh-trên-chuỗi&#34;&gt;Mẹo 2: Các thao tác nhanh trên chuỗi&lt;/h2&gt;

&lt;p&gt;Python cung cấp cho chúng ta một số cách viết ngắn gọn giúp chúng ta có thể dể dàng thao tác trên chuỗi. Để reverse một chuỗi, chúng ta sử dụng toán tử ::-1&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
str = &#39;i am alex&#39;
print(str[::-1])
&amp;gt;&amp;gt; xela ma i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mẹo trên cũng có thể sử dụng đối với list số nguyên.&lt;/p&gt;

&lt;p&gt;Để nối các phần tử trong một list thành một chuỗi, chúng ta có thể dùng hàm join()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
str1 = [&amp;quot;pig&amp;quot;, &amp;quot;year&amp;quot; , &amp;quot;2019&amp;quot;]
str2 = &amp;quot;happy &amp;quot;
str3 = &amp;quot;new &amp;quot;


print( &#39; &#39;.join(str1))
&amp;gt;&amp;gt; pig year 2019

print(str2+str3+&#39; &#39;.join(str1))
&amp;gt;&amp;gt; happy new year 2019
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thật tuyệt vời phải không các bạn.&lt;/p&gt;

&lt;p&gt;Ngoài ra các bạn có thể sử dụng biếu thức chính quy để tìm kiếm chuỗi và pattern. Về biểu thức chính quy trong python, các bạn có thể tìm hiểu ở &lt;a href=&#34;https://docs.python.org/3/library/re.html&#34;&gt;https://docs.python.org/3/library/re.html&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-số-3-chuỗi-lồng-nhau&#34;&gt;Mẹo số 3: Chuỗi lồng nhau&lt;/h2&gt;

&lt;p&gt;Thử tưởng tượng rằng bạn có hàng tá các list, và sau một mớ các thao tác, kết quả của bạn là một list các list. Chúng ta sẽ sử dụng itertools - một thư viện được cung cấp sẵn trong python để giải quyết vấn đề này giúp chúng ta.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import itertools
flatten = lambda x: list(itertools.chain.from_iterable(x))
s =[[&amp;quot;this&amp;quot;,&amp;quot;is&amp;quot;],[&amp;quot;the&amp;quot;,&amp;quot;year&amp;quot;], [&amp;quot;of&amp;quot;, &amp;quot;pig&amp;quot;], [&amp;quot;in&amp;quot;], [&amp;quot;Việt&amp;quot;, &amp;quot;Nam&amp;quot;]]

print(&#39; &#39;,join(flatten(s)))
&amp;gt;&amp;gt; this is the year of pig in Việt Nam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nếu bạn chạy dòng code trên bị lỗi, rất có thể là do terminal của bạn không hỗ trợ tiếng việt font unicode. Hãy chuyển qua font unicode trên terminal hoặc dùng terminal của ubuntu, bash (trên window 10).&lt;/p&gt;

&lt;p&gt;Ngoài ra, itertools còn hỗ trợ rất nhiều hàm khác để giúp chúng ta thao tác trên chuỗi lồng dễ dàng hơn. Các bạn có thể tham khảo thêm ở &lt;a href=&#34;https://docs.python.org/2/library/itertools.html&#34;&gt;https://docs.python.org/2/library/itertools.html&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-4-cấu-trúc-dữ-liệu-đơn-giản&#34;&gt;Mẹo 4: Cấu trúc dữ liệu đơn giản.&lt;/h2&gt;

&lt;p&gt;Chúng ta có thể xây dựng một cây đơn giản chỉ với một dòng mã lệnh:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tree(): return defaultdict(tree)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một ví dụ đơn giản khác là hàm tạo số nguyên chỉ với 1 dòng code ngắn gọn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reduce( (lambda r,x: r-set(range(x**2,N,x)) if (x in r) else r), 
        range(2,N), set(range(2,N)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python có hỗ trợ nhiều thư viện rất mạnh trong việc giải quyết các vấn đề trong thế giới thực. Ví dụ thư viện Collections&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter
myList = [1,1,2,3,4,5,3,2,3,4,2,1,2,3]
print(Counter(myList))
Counter({2: 4, 3: 4, 1: 3, 4: 2, 5: 1})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một lưu ý nhỏ là các thư viện này chỉ nên sử dụng khi tập dữ liệu của bạn nhỏ, nếu tập dữ liệu lớn, ví dụ bạn cần đếm số lần xuất hiện của các từ trong tập văn bản với 100GB dữ liệu. Bạn hãy dùng cách khác, ví dụ hadoop, hoặc tăng bộ nhớ ram của bạn lên, ví dụ 1 Tb chẳng hạn :)&lt;/p&gt;

&lt;h2 id=&#34;mẹo-5-xuất-dữ-liệu-ra-command-line-dễ-dàng&#34;&gt;Mẹo 5: Xuất dữ liệu ra command line dễ dàng&lt;/h2&gt;

&lt;p&gt;Để xuất dữ liệu của một list int ra command line, theo như mẹo ở trên, ta sẽ dùng hàm .join() và vòng lặp.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python`&#34;&gt;lst_row = [1,2,3,4,5]
print(&#39;,&#39;.join([str(x) for x in lst_row])
1,2,3,4,5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cách đơn giản hơn chỉ với một dòng code (Ước gì mình biết cách này sớm hơn, hix).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(*lst_row, sep=&#39;,&#39;)
1,2,3,4,5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một mẹo khác là trong một số trường hợp duyệt mảng, bạn cần lấy giá trị và chỉ số của mảng đó để làm một số thao tác khác&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
lst_arr = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]

int_index = 0

for item in lst_arr:
    print(int_index, item)
    int_index = int_index + 1
    
&amp;gt;&amp;gt; 0 a
1 b
2 c
3 d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hoặc cách viết giống c/c++&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
for int_index in len(lst_arr):
    print(int_index, lst_arr[int_index])
    
&amp;gt;&amp;gt; 0 a
1 b
2 c
3 d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một cách khác là sử dụng hàm có sẵn enumerate của python&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for int_index, item in enumerate(lst_arr):
    print(int_index, item)
    
&amp;gt;&amp;gt; 0 a
1 b
2 c
3 d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Có rất nhiều mẹo hay để đơn giản hoá việc xuất dữ liệu ra terminal. Hãy thông tin cho mình biết nếu bạn có nhiều mẹo hay khác cần chia sẻ nhé.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hệ thống gợi ý khoá học cho website DonorChoose.org</title>
      <link>/blog/2019-01-03-donor-project-matching-with-recommender-systems/</link>
      <pubDate>Tue, 11 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-01-03-donor-project-matching-with-recommender-systems/</guid>
      <description>

&lt;h2 id=&#34;đặt-vấn-đề&#34;&gt;Đặt vấn đề&lt;/h2&gt;

&lt;p&gt;DonorsChoose.org được thành lập vào năm 2000 bởi một giáo viên lịch sử tại Mỹ tên là Bronx và đã huy động được 685 triệu đô la cho các lớp học. &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt; các giáo viên ở các trường công lập ở Hoa Kỳ đã sử dụng Donor để gửi các yêu cầu bài tập cho học sinh. Từ đó, Donor trở thành nền tảng giáo dục hàng đầu hỗ trợ cho các vấn đề giáo dục công cộng.&lt;/p&gt;

&lt;p&gt;Đến nay, hơn 3 triệu người dùng và đối tác đã đóng góp hơn 1,1 triệu dự án cho Donor. Nhưng các giáo viên vẫn phải tốn hàng tỷ đô tiền túi để chuẩn bị các dụng cụ học tập trên lớp (để truyền tải kiến thức cho học sinh).&lt;/p&gt;

&lt;p&gt;Giải pháp được đưa ra ở đây là xây dựng một chiến dịch gợi ý cho các nhà tại trợ.&lt;/p&gt;

&lt;h3 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h3&gt;

&lt;p&gt;Chúng ta có các file sau:&lt;/p&gt;

&lt;p&gt;File Donations.csv.  Với mỗi dự án (Project ID), sẽ có 1 hoặc nhiều nhà quyên góp (Donor ID) mỗi cặp (dự án - nhà quyên góp sẽ định dang bằng 1 mã chung (Donation ID) và có các cột thông tin liên quan đến việc quyên góp đó). File có xấp xỉ 4.67 triệu dòng (chính xác là 4687844 dòng) và 7 cột. (Project ID - Định danh dự án, Donation ID - Định danh khoảng đóng góp (tưởng tượng như khoá tự tăng của bảng này đó các bạn), Donor ID - Mã định danh người đóng góp, Donation Included Option - hỗ trợ website donoschoose 15% giá trị quyên góp, Donation Amount - Số tiền quyên góp, Donor Cart Sequence - Thứ tự của dự án trọng bảng danh sách quyên góp,Donation Received Date - Ngày giờ quyên góp).&lt;/p&gt;

&lt;p&gt;File Donors.csv. File định danh người quyên góp. Chứa tổng cộng hơn 2 triệu dòng( chính xác là 2122640 dòng)
File có kích thước 2122640 x 5 với các thông tin cột là Donor ID (khoá chính, không trùng), Donor City (tên thành phố nhà đầu tư đang sinh sống), Donor State (tiểu bang mà người quyên góp đang sống), Donor is teacher, Donor Zip (3 ký tự đầu của mã bưu điện nhà từ thiện).&lt;/p&gt;

&lt;p&gt;File Teacher.csv. File có tổng cộng 402900 dòng với các cột TeachId, Teacher Prefix (Mr, Mrs, Ms), Teacher First Project Posted Date.&lt;/p&gt;

&lt;p&gt;File Schools.csv. File có tổng cộng 72994 dòng với các cột là SchoolID, SchoolName (tên trường có thể trùng nhau), School Metro Type ( phân loại trường thuộc 1 trong 5 nhóm : suburnban - ngoại ô, rural - nông thôn, uban - thành thị, town - thị trấn, unknow), School Percentage Free Lunch ( Số nguyên, mô tả tỷ lệ phần trăm số học sinh đủ điều kiện ăn trưa miễn phí hoặc ăn trưa giảm phí. Dữ liệu thu được cung cấp bởi một đối tác thống kê độc lập là NCES. Nếu trường nào không có giá trị do NCES cung cấp, chúng ta sẽ lấy số phần trăm này là trung bình phần trăm của các trường cùng huyện), School State (Trường đang toạ lạc ở bang nào (vd cali, Florida, Virginia, &amp;hellip;)), School Zip (mã bưu chính), School City, School County&lt;/p&gt;

&lt;p&gt;File Resources.csv. Với mỗi dự án, chúng ta cần các loại tài nguyên khác nhau. Các cột là Project ID (mã dự án), Resource Item Name (tên tài nguyên cần cho dự án đó vd project 000009891526c0ade7180f8423792063 cần &amp;lsquo;chair move and store cart&amp;rsquo;), Resource Quantity (số lượng tài nguyên cần, vd cần 1 cái ghế, 2 cái bảng v.v),
Resource Unit Price (đơn giá cho 1 đơn vị tài nguyên, vd cái ghế giá 7 ngàn, cái bảng giá 10 ngàn, nếu 1 unit là ghế + bảng thì là 17 ngàn), Resource Vendor Name(nhà cung cấp, vd: Amazon Business, Woodwind and Brasswind).&lt;/p&gt;

&lt;p&gt;File Projects.csv&lt;/p&gt;

&lt;h3 id=&#34;xây-dựng-chiến-lược-tiếp-cận-bài-toán&#34;&gt;Xây dựng chiến lược tiếp cận bài toán&lt;/h3&gt;

&lt;p&gt;Hãy xem đây như là bài toán gợi ý. Và Donors chính là hệ thống cung cấp các sản phẩm. Ví dụ đơn giản là bạn có website nghe nhạc mp3.zing.vn, alice vào nghe một hoặc một vài bài nhạc. Chúng ta sẽ xây dựng một hệ gợi ý những bài nhạc tiếp theo alice nên nghe dựa vào những bài nhạc đã nghe trước đó của alice. Tương tự vậy, hệ thống Donor như là website mp3.zing, bài nhạc tương tự như các project đang có, người dùng tương tự như các nhà tự thiện. Một khi một nhà từ thiện đã quyên góp cho 1 hoặc 1 nhón các dự án, chúng ta sẽ lên kế hoạch và gợi ý cho khác hàng dự án tiếp theo khách hàng nên tìm hiểu kỹ để xét xem có nên donate hay không.&lt;/p&gt;

&lt;p&gt;Dựa vào các chiến lược trên, chúng ta có 3 cách có thể tiếp cận vấn đề:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Content-based filltering.&lt;/li&gt;
&lt;li&gt;Collaborative Filtering&lt;/li&gt;
&lt;li&gt;Hybrid methods&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;1-tiền-xử-lý-dữ-liệu&#34;&gt;1. Tiền xử lý dữ liệu&lt;/h4&gt;

&lt;p&gt;Trước khi bắt đầu xây dựng chương trình gợi ý, chúng ta cần phải load dữ liệu lên bộ nhớ chính và làm sạch dữ liệu.&lt;/p&gt;

&lt;p&gt;Trước tiên, chúng ta sẽ import các thư viện cần thiết. Nếu thiếu các thư viện nào, các bạn cứ pip install tên thư viện trong cmd/terminal là được&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import scipy
import pandas as pd
import math
import random
import sklearn
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.linalg import svds
import matplotlib.pyplot as plt
import os
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load 3 file Projects.csv, Donations.csv, Donors.csv lên và merge donations với donors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set up test mode to save some time
test_mode = True

# Read datasets
projects = pd.read_csv(&#39;../input/Projects.csv&#39;)
donations = pd.read_csv(&#39;../input/Donations.csv&#39;)
donors = pd.read_csv(&#39;../input/Donors.csv&#39;)

#this piece of code converts Project_ID which is a 32-bit Hex int digits 10-1010
# create column &amp;quot;project_id&amp;quot; with sequential integers
f=len(projects)
projects[&#39;project_id&#39;] = np.nan
g = list(range(10,f+10))
g = pd.Series(g)
projects[&#39;project_id&#39;] = g.values

# Merge datasets
donations = donations.merge(donors, on=&amp;quot;Donor ID&amp;quot;, how=&amp;quot;left&amp;quot;)
df = donations.merge(projects,on=&amp;quot;Project ID&amp;quot;, how=&amp;quot;left&amp;quot;)

# only load a few lines in test mode
if test_mode:
    df = df.head(10000)

donations_df = df
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở giai đoạn xây dựng code và debug, mình chỉ load 10000 dữ liệu lên để test thử  (để đảm bảo rằng mình code đúng - bằng cách set test_mode = True). Khi chạy thật mình sẽ set lại test_mode = False.&lt;/p&gt;

&lt;p&gt;Thực hiện một vài bước phân tích kỹ thuật đơn giản để nắm rõ hơn về dữ liệu.&lt;/p&gt;

&lt;p&gt;Thử đo mối quan hệ giữa các dự án và các &amp;ldquo;mạnh thường quân&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Deal with missing values
donations[&amp;quot;Donation Amount&amp;quot;] = donations[&amp;quot;Donation Amount&amp;quot;].fillna(0)

# Define event strength as the donated amount to a certain project
donations_df[&#39;eventStrength&#39;] = donations_df[&#39;Donation Amount&#39;]

def smooth_donor_preference(x):
    return math.log(1+x, 2)
    
donations_full_df = donations_df \
                    .groupby([&#39;Donor ID&#39;, &#39;Project ID&#39;])[&#39;eventStrength&#39;].sum() \
                    .apply(smooth_donor_preference).reset_index()
        
# Update projects dataset
project_cols = projects.columns
projects = df[project_cols].drop_duplicates()

print(&#39;# of projects: %d&#39; % len(projects))
print(&#39;# of unique user/project donations: %d&#39; % len(donations_full_df))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# of projects: 1889
# of unique user/project donations: 8648
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dựa vào kết quả trên tập test, chúng ta có thể đưa ra một vài nhận xét như sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hầu hết các mạnh thường quân chỉ donate cho 1 project (tỷ lệ 86,48%)&lt;/li&gt;
&lt;li&gt;Sẽ có trường hợp 1 mạnh thường quân sẽ donate cho nhiều dự án, và cũng có trường hợp 1 mạnh thường quân donate nhiều lần cho 1 dự án. Trường hợp này chiếm phần ít.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Để đánh giá mô hình, chúng ta sẽ chia tập dữ liệu thành 2 phần là train và test. Ở đây, chúng ta sẽ set tỷ lệ train/test là 20%.&lt;/p&gt;

&lt;h4 id=&#34;2-xây-dựng-mô-hình-content-based-filtering&#34;&gt;2. Xây dựng mô hình Content-Based Filtering&lt;/h4&gt;

&lt;p&gt;Cách tiếp cận đầu tiên, chúng ta sẽ tìm những project gần giống với những project mà donor đã donated. Đơn giản nhất là với mỗi project, chúng ta sẽ định nghĩa các vector đặc trưng của chúng và đo độ giống nhau giữa hai vector đó. Vector đặc trưng chúng ta có thể xây dựng trên các thuộc tính như project type, project catefory, grade level, resource category, cost, school zip code, &amp;hellip; hoặc các bạn có thể từ các vector cơ bản do tập dữ liệu cung cấp bổ sung thêm các vector cấp cao hơn, ví dụ như là rút trích các feature từ tên project hoặc mô tả của project, loại bỏ stopwords &amp;hellip;&lt;/p&gt;

&lt;p&gt;Ở đây, chúng ta sẽ sử dụng kỹ thuật TF-IDF để rút trích thông tin đặc trưng của dự án dựa trên project tittle và description. Về TF-IDF, các bạn có thể đọc ở một bài viết nào đó của google, mình không tiện nhắc đến nó chi tiết ở bài viết này.&lt;/p&gt;

&lt;h5 id=&#34;a-xây-dựng-tập-đặc-trưng&#34;&gt;a. Xây dựng tập đặc trưng&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Preprocessing of text data
textfeats = [&amp;quot;Project Title&amp;quot;,&amp;quot;Project Essay&amp;quot;]
for cols in textfeats:
    projects[cols] = projects[cols].astype(str) 
    projects[cols] = projects[cols].astype(str).fillna(&#39;&#39;) # FILL NA
    projects[cols] = projects[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently
 
text = projects[&amp;quot;Project Title&amp;quot;] + &#39; &#39; + projects[&amp;quot;Project Essay&amp;quot;]
vectorizer = TfidfVectorizer(strip_accents=&#39;unicode&#39;,
                             analyzer=&#39;word&#39;,
                             lowercase=True, # Convert all uppercase to lowercase
                             stop_words=&#39;english&#39;, # Remove commonly found english words (&#39;it&#39;, &#39;a&#39;, &#39;the&#39;) which do not typically contain much signal
                             max_df = 0.9, # Only consider words that appear in fewer than max_df percent of all documents
                             # max_features=5000 # Maximum features to be extracted                    
                            )                        
project_ids = projects[&#39;Project ID&#39;].tolist()
tfidf_matrix = vectorizer.fit_transform(text)
tfidf_feature_names = vectorizer.get_feature_names()


## build profile

def get_project_profile(project_id):
    idx = project_ids.index(project_id)
    project_profile = tfidf_matrix[idx:idx+1]
    return project_profile

def get_project_profiles(ids):
    project_profiles_list = [get_project_profile(x) for x in np.ravel([ids])]
    project_profiles = scipy.sparse.vstack(project_profiles_list)
    return project_profiles

def build_donors_profile(donor_id, donations_indexed_df):
    donations_donor_df = donations_indexed_df.loc[donor_id]
    donor_project_profiles = get_project_profiles(donations_donor_df[&#39;Project ID&#39;])
    donor_project_strengths = np.array(donations_donor_df[&#39;eventStrength&#39;]).reshape(-1,1)
    #Weighted average of project profiles by the donations strength
    donor_project_strengths_weighted_avg = np.sum(donor_project_profiles.multiply(donor_project_strengths), axis=0) / (np.sum(donor_project_strengths)+1)
    donor_profile_norm = sklearn.preprocessing.normalize(donor_project_strengths_weighted_avg)
    return donor_profile_norm

from tqdm import tqdm

def build_donors_profiles(): 
    donations_indexed_df = donations_full_df[donations_full_df[&#39;Project ID&#39;].isin(projects[&#39;Project ID&#39;])].set_index(&#39;Donor ID&#39;)
    donor_profiles = {}
    for donor_id in tqdm(donations_indexed_df.index.unique()):
        donor_profiles[donor_id] = build_donors_profile(donor_id, donations_indexed_df)
    return donor_profiles

donor_profiles = build_donors_profiles()
print(&amp;quot;# of donors with profiles: %d&amp;quot; % len(donor_profiles))

mydonor1 = &amp;quot;6d5b22d39e68c656071a842732c63a0c&amp;quot;
mydonor2 = &amp;quot;0016b23800f7ea46424b3254f016007a&amp;quot;
mydonor1_profile = pd.DataFrame(sorted(zip(tfidf_feature_names, 
                        donor_profiles[mydonor1].flatten().tolist()), 
                        key=lambda x: -x[1])[:10],
                        columns=[&#39;token&#39;, &#39;relevance&#39;])
mydonor2_profile = pd.DataFrame(sorted(zip(tfidf_feature_names, 
                        donor_profiles[mydonor2].flatten().tolist()), 
                        key=lambda x: -x[1])[:10],
                        columns=[&#39;token&#39;, &#39;relevance&#39;])

print(&#39;feature of user &#39; + str(mydonor1))
print(mydonor1_profile)

print(&#39;feature of user &#39; + str(mydonor2))
print(mydonor2_profile)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mã nguồn ở trên cũng có chú thích đầy đủ, và đọc cũng dễ hiểu, nên mình không nói thêm gì nhiều. Mình tóm gọn một chút là chúng ta sẽ convert toàn bộ project tittle và description về dạng chữ thường, tách từ dựa vào khoảng trắng, loại bỏ những english stopwords. Sau đó xây dựng profile cho từng donor.&lt;/p&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature of  user 6d5b22d39e68c656071a842732c63a0c
        token  relevance
0       music   0.450057
1  auditorium   0.355256
2        cart   0.272809
3       chair   0.223861
4   equipment   0.211338
5   musicians   0.179244
6        time   0.172908
7      moving   0.137749
8        ohms   0.134065
9     prepare   0.131274
feature of  user 0016b23800f7ea46424b3254f016007a
         token  relevance
0  pollinators   0.670222
1       plants   0.305398
2       module   0.223407
3  pollination   0.211870
4        seeds   0.180609
5      writing   0.166816
6        books   0.137455
7      reading   0.115003
8       weaved   0.111704
9         bees   0.101842
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nhìn kết quả trên, ta thấy rằng donor 1 có vẻ thích những thứ liên quan đến âm nhạc (music, auditorim), trong khi đó donor 2 thích những thứ liên quan đến trồng trọt (pollinators - thụ phấn, plants - cây cối)&lt;/p&gt;

&lt;h5 id=&#34;b-xây-dựng-mô-hình&#34;&gt;b. Xây dựng mô hình&lt;/h5&gt;

&lt;p&gt;Việc xây dựng mô hình đến đây là khá đơn giản. Chúng ta chỉ việc tính khoảng cách cosin giữa vector cần dự đoán và toàn bộ vector có trong tập train rồi show top K prject có liên quan cao nhất&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

class ContentBasedRecommender:
    
    MODEL_NAME = &#39;Content-Based&#39;
    
    def __init__(self, projects_df=None):
        self.project_ids = project_ids
        self.projects_df = projects_df
        
    def get_model_name(self):
        return self.MODEL_NAME
        
    def _get_similar_projects_to_donor_profile(self, donor_id, topn=1000):
        #Computes the cosine similarity between the donor profile and all project profiles
        cosine_similarities = cosine_similarity(donor_profiles[donor_id], tfidf_matrix)
        #Gets the top similar projects
        similar_indices = cosine_similarities.argsort().flatten()[-topn:]
        #Sort the similar projects by similarity
        similar_projects = sorted([(project_ids[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1])
        return similar_projects
        
    def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10, verbose=False):
        similar_projects = self._get_similar_projects_to_donor_profile(donor_id)
        #Ignores projects the donor has already donated
        similar_projects_filtered = list(filter(lambda x: x[0] not in projects_to_ignore, similar_projects))
        
        recommendations_df = pd.DataFrame(similar_projects_filtered, columns=[&#39;Project ID&#39;, &#39;recStrength&#39;]).head(topn)

        recommendations_df = recommendations_df.merge(self.projects_df, how = &#39;left&#39;, 
                                                    left_on = &#39;Project ID&#39;, 
                                                    right_on = &#39;Project ID&#39;)[[&#39;recStrength&#39;, &#39;Project ID&#39;, &#39;Project Title&#39;, &#39;Project Essay&#39;]]


        return recommendations_df


cbr_model = ContentBasedRecommender(projects)


print(&#39;recommend for user &#39; + str(mydonor1))
print(cbr_model.recommend_projects(mydonor1))

print(&#39;recommend for user &#39; + str(mydonor2))
print(cbr_model.recommend_projects(mydonor2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recommend for user 6d5b22d39e68c656071a842732c63a0c
   recStrength                        ...                                                              Project Essay
0     1.000000                        ...                          the music students in our classes perform freq...
1     0.390997                        ...                          i have spent 12 years as an educator rebuildin...
2     0.338676                        ...                          &amp;quot;music is what feelings sound like.&amp;quot; -g. cates...
3     0.331034                        ...                          true music is created not by the teacher but b...
4     0.324355                        ...                          every morning my first grade students come to ...
5     0.322923                        ...                          in today&#39;s fast paced environment, students ne...
6     0.315910                        ...                          &amp;quot;music is a moral law.  it gives soul to the u...
7     0.314845                        ...                          i walk in the door so excited to get the stude...
8     0.310103                        ...                          some students have never put their hands on a ...
9     0.297516                        ...                          my students do not have money, but they do hav...

[10 rows x 4 columns]
recommend for user 0016b23800f7ea46424b3254f016007a
   recStrength                        ...                                                              Project Essay
0     1.000000                        ...                          my students are creative, curious, and excited...
1     0.211962                        ...                          our school is a title 1 school.  100% of stude...
2     0.189111                        ...                          my students are active and eager learners who ...
3     0.188095                        ...                          being a small rural school we do a lot of trad...
4     0.173520                        ...                          &amp;quot;science is a way of life...science is the pro...
5     0.159015                        ...                          my second grade students love to come to schoo...
6     0.158071                        ...                          i teach 28 fourth graders in a neighborhood sc...
7     0.150389                        ...                          in my classroom we are working hard to become ...
8     0.144724                        ...                          as a teacher in a diverse, low-income, high-po...
9     0.139937                        ...                          have you ever been told you need to read, but ...

[10 rows x 4 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mình dùng cmd nên bị giới hạn kết quả, các bạn có thể write log vào file hoặc dùng jupiter để show kết quả rõ hơn.&lt;/p&gt;

&lt;p&gt;Ở đây, chúng ta nhận thấy rằng các recommend cho donor 1 thường là những project liên quan tới âm nhạc (nhìn tập feature ta cũng có thể đoán được). Và recommend cho donor 2 là những thứ liên quan đến chủ đề làm vườn và reading.&lt;/p&gt;

&lt;h4 id=&#34;3-collaborative-filtering-model&#34;&gt;3. Collaborative Filtering Model&lt;/h4&gt;

&lt;p&gt;Lý thuyết về Collaborative Filtering Model các bạn có thể xem ở các bài viết khác của mình hoặc tham khảo thêm trên mạng. Ở đây, mình sẽ sử dụng Singular Value Decomposition (SVD) để xây dựng ma trận đặc trưng.&lt;/p&gt;

&lt;h5 id=&#34;a-xây-dựng-ma-trận-donor-project&#34;&gt;a. Xây dựng ma trận donor - project&lt;/h5&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ xây dựng ma trận mối quan hệ giữa donor và project. Nếu donor i có donated cho 1 project j thì dòng i cột j của ma trận sẽ được đánh dấu là 1, ngược lại là 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#### create matrix
#Creating a sparse pivot table with donors in rows and projects in columns
donors_projects_pivot_matrix_df = donations_full_df.pivot(index=&#39;Donor ID&#39;, 
                                                          columns=&#39;Project ID&#39;, 
                                                          values=&#39;eventStrength&#39;).fillna(0)

# Transform the donor-project dataframe into a matrix
donors_projects_pivot_matrix = donors_projects_pivot_matrix_df.as_matrix()

# Get donor ids
donors_ids = list(donors_projects_pivot_matrix_df.index)

print(donors_projects_pivot_matrix[:5]) # print first 5 row
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;b-singular-value-decomposition&#34;&gt;b. Singular Value Decomposition&lt;/h5&gt;

&lt;p&gt;Sau khi có ma trận trên, ta có một nhận xét rằng nó rất thưa, số lượng 0 thì nhiều mà 1 thì ít. Sau khi áp dụng SVD, ma trận kết quả sẽ ít thưa hơn (có thể đạt được đến mức không còn thưa nữa).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Performs matrix factorization of the original donor-project matrix
# Here we set k = 20, which is the number of factors we are going to get
# In the definition of SVD, an original matrix A is approxmated as a product A ≈ UΣV 
# where U and V have orthonormal columns, and Σ is non-negative diagonal.
U, sigma, Vt = svds(donors_projects_pivot_matrix, k = 20)
sigma = np.diag(sigma)

# Reconstruct the matrix by multiplying its factors
all_donor_predicted_ratings = np.dot(np.dot(U, sigma), Vt) 

#Converting the reconstructed matrix back to a Pandas dataframe
cf_preds_df = pd.DataFrame(all_donor_predicted_ratings, 
                           columns = donors_projects_pivot_matrix_df.columns, 
                           index=donors_ids).transpose()
                           
print(cf_preds_df.head())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;                                  0003aba06ccf49f8c44fc2dd3b582411                ...                 ffff088c35d3455779a30898d1327b76
Project ID                                                                        ...

000009891526c0ade7180f8423792063                     -3.423182e-34                ...-4.577244e-34
00000ce845c00cbf0686c992fc369df4                     -3.061322e-36                ...-6.492305e-36
00002d44003ed46b066607c5455a999a                      1.368936e-33                ...-2.239156e-32
00002eb25d60a09c318efbd0797bffb5                      1.784576e-33                ...1.163684e-32
0000300773fe015f870914b42528541b                      4.314216e-34                ...-4.666110e-34

[5 rows x 8015 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;c-xây-dựng-collaborative-filtering-model&#34;&gt;c. Xây dựng Collaborative Filtering Model&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

class CFRecommender:
    
    MODEL_NAME = &#39;Collaborative Filtering&#39;
    
    def __init__(self, cf_predictions_df, projects_df=None):
        self.cf_predictions_df = cf_predictions_df
        self.projects_df = projects_df
        
    def get_model_name(self):
        return self.MODEL_NAME
        
    def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10):
        # Get and sort the donor&#39;s predictions
        sorted_donor_predictions = self.cf_predictions_df[donor_id].sort_values(ascending=False) \
                                    .reset_index().rename(columns={donor_id: &#39;recStrength&#39;})

        # Recommend the highest predicted projects that the donor hasn&#39;t donated to
        recommendations_df = sorted_donor_predictions[~sorted_donor_predictions[&#39;Project ID&#39;].isin(projects_to_ignore)] \
                               .sort_values(&#39;recStrength&#39;, ascending = False) \
                               .head(topn)

 
        recommendations_df = recommendations_df.merge(self.projects_df, how = &#39;left&#39;, 
                                                          left_on = &#39;Project ID&#39;, 
                                                          right_on = &#39;Project ID&#39;)[[&#39;recStrength&#39;, &#39;Project ID&#39;, &#39;Project Title&#39;, &#39;Project Essay&#39;]]


        return recommendations_df

cfr_model = CFRecommender(cf_preds_df, projects)
print(cfr_model.recommend_projects(mydonor1))

print(cfr_model.recommend_projects(mydonor2))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[5 rows x 8015 columns]
    recStrength                        ...                                                              Project Essay
0  3.015461e-17                        ...                          Our students are some of the hardest working k...
1  2.237275e-17                        ...                          As Service Learning Coordinators at our elemen...
2  2.188501e-17                        ...                          We are trying to engage more students in scien...
3  1.768711e-17                        ...                          We are a brand new charter school that has onl...
4  1.344489e-17                        ...                          Sitting at a desk for a sustained period of ti...
5  9.957278e-18                        ...                          Our students come from a Title I school in Jer...
6  6.932330e-18                        ...                          In my school 50% of the students are socioecon...
7  8.589640e-19                        ...                          Have you ever been told you need to read, but ...
8  6.698040e-19                        ...                          &amp;quot;I cannot say good-bye to those whom I have gr...
9  5.733941e-19                        ...                          I have students in class who are squinting and...

[10 rows x 4 columns]
    recStrength                        ...                                                              Project Essay
0  3.015461e-17                        ...                          Our students are some of the hardest working k...
1  2.237275e-17                        ...                          As Service Learning Coordinators at our elemen...
2  2.188501e-17                        ...                          We are trying to engage more students in scien...
3  1.768711e-17                        ...                          We are a brand new charter school that has onl...
4  1.344489e-17                        ...                          Sitting at a desk for a sustained period of ti...
5  9.957278e-18                        ...                          Our students come from a Title I school in Jer...
6  6.932330e-18                        ...                          In my school 50% of the students are socioecon...
7  8.589640e-19                        ...                          Have you ever been told you need to read, but ...
8  6.698040e-19                        ...                          &amp;quot;I cannot say good-bye to those whom I have gr...
9  5.733941e-19                        ...                          I have students in class who are squinting and...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả trả về có vẻ không được đẹp như ở phương pháp trên. Ở đây, thuật toán dựa vào hành vi donated của những người khác có điểm tương đồng với user donor 1 và 2. Bởi vậy gợi ý những project sẽ khác những gợi ý ở phương pháp 1.&lt;/p&gt;

&lt;h4 id=&#34;4-hybrid-method&#34;&gt;4. Hybrid Method&lt;/h4&gt;

&lt;p&gt;Phương pháp lai này kết hợp cả 2 hướng tiếp cận của hai phương pháp ở trên. Ở đây, chúng ta sẽ xây dựng một mô hình nhỏ, nhân điểm của content based và collaborative filtering lại với nhau, sau đó xếp hạng để được điểm hybrid. Đây là 1 cách đơn giản, các bạn có thể tìm đọc nhiều cách tiếp cận khác và ứng dụng vào bài toán.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HybridRecommender:
    
    MODEL_NAME = &#39;Hybrid&#39;
    
    def __init__(self, cb_rec_model, cf_rec_model, projects_df):
        self.cb_rec_model = cb_rec_model
        self.cf_rec_model = cf_rec_model
        self.projects_df = projects_df
        
    def get_model_name(self):
        return self.MODEL_NAME
        
    def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10):
        #Getting the top-1000 Content-based filtering recommendations
        cb_recs_df = self.cb_rec_model.recommend_projects(donor_id, projects_to_ignore=projects_to_ignore, 
                                                           topn=1000).rename(columns={&#39;recStrength&#39;: &#39;recStrengthCB&#39;})
        
        #Getting the top-1000 Collaborative filtering recommendations
        cf_recs_df = self.cf_rec_model.recommend_projects(donor_id, projects_to_ignore=projects_to_ignore,  
                                                           topn=1000).rename(columns={&#39;recStrength&#39;: &#39;recStrengthCF&#39;})
        
        #Combining the results by Project ID
        recs_df = cb_recs_df.merge(cf_recs_df,
                                   how = &#39;inner&#39;, 
                                   left_on = &#39;Project ID&#39;, 
                                   right_on = &#39;Project ID&#39;)
        
        #Computing a hybrid recommendation score based on CF and CB scores
        recs_df[&#39;recStrengthHybrid&#39;] = recs_df[&#39;recStrengthCB&#39;] * recs_df[&#39;recStrengthCF&#39;]
        
        #Sorting recommendations by hybrid score
        recommendations_df = recs_df.sort_values(&#39;recStrengthHybrid&#39;, ascending=False).head(topn)

        recommendations_df = recommendations_df.merge(self.projects_df, how = &#39;left&#39;, 
                                                    left_on = &#39;Project ID&#39;, 
                                                    right_on = &#39;Project ID&#39;)[[&#39;recStrengthHybrid&#39;, 
                                                                              &#39;Project ID&#39;, &#39;Project Title&#39;, 
                                                                              &#39;Project Essay&#39;]]


        return recommendations_df
    
hybrid_model = HybridRecommender(cbr_model, cfr_model, projects)


print(hybrid_model.recommend_projects(mydonor1))

print(hybrid_model.recommend_projects(mydonor2))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   recStrengthHybrid                        ...                                                              Project Essay
0       1.574375e-18                        ...                          we are trying to engage more students in scien...
1       1.221807e-18                        ...                          in my school 50% of the students are socioecon...
2       1.214293e-18                        ...                          our students are some of the hardest working k...
3       4.037232e-19                        ...                          sitting at a desk for a sustained period of ti...
4       6.661794e-20                        ...                          “music expresses that which cannot be put into...
5       4.872264e-20                        ...                          i walk in the door so excited to get the stude...
6       4.410098e-20                        ...                          i have spent 12 years as an educator rebuildin...
7       2.907349e-20                        ...                          &amp;quot;music is what feelings sound like.&amp;quot; -g. cates...
8       2.121616e-20                        ...                          &amp;quot;i cannot say good-bye to those whom i have gr...
9       1.353927e-20                        ...                          our band program is one of the largest in our ...

[10 rows x 4 columns]
   recStrengthHybrid                        ...                                                              Project Essay
0       2.811124e-18                        ...                          in this modern, digital age, i would like to u...
1       1.249967e-18                        ...                          we are a brand new charter school that has onl...
2       6.055628e-19                        ...                          my students are african american and hispanic....
3       5.912367e-19                        ...                          the a. community and its students are a very s...
4       2.541749e-19                        ...                          do you want to go on an adventure and learn ab...
5       2.494812e-19                        ...                          the average day in my class involves students ...
6       2.323313e-19                        ...                          i teach ela (reading component) to self-contai...
7       1.271629e-19                        ...                          hi there! do you want to help to instill a lif...
8       1.044990e-19                        ...                          having writing utensils is essential for stude...
9       1.004780e-19                        ...                          there&#39;s no such thing as a kid who hates readi...

[10 rows x 4 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả trả ra tốt hơn nhiều so với cách 2, donor1 có music, donor2 có cây trồng và sách.&lt;/p&gt;

&lt;h4 id=&#34;5-đánh-giá-mô-hình&#34;&gt;5. Đánh giá mô hình&lt;/h4&gt;

&lt;p&gt;Có rất nhiều cách khác nhau để đánh giá mô hình recommend system. Một trong các cách mình sử dụng ở đây là sử dụng độ đo top K accuracy. Độ đo này được tính như sau:&lt;/p&gt;

&lt;p&gt;Với mỗi user:
    Với mỗi item user đã pick trong test set
        Lấy mẫu 1000 item khác mà người dùng chưa bao giờ pick&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo. Cố lên.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Một số mẹo để lựa chọn mô hình object detection</title>
      <link>/blog/2018-12-11-a-bunch-of-tips-and-tricks-for-training-deep-neural-networks/</link>
      <pubDate>Tue, 11 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-11-a-bunch-of-tips-and-tricks-for-training-deep-neural-networks/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Việc huấn luyện một mô hình neural network khá đơn giản, chỉ việc download code mẫu về, quăng tập data của mình vào, rồi cho chạy, xong. Nhưng khó khăn ở đây là làm cách nào để nâng độ chính xác của mô hình lên. Ở bài viết này, chúng ta sẽ tìm hiểu một số cách giúp tăng độ chính xác của mô hình.&lt;/p&gt;

&lt;h3 id=&#34;kiểm-tra-dữ-liệu&#34;&gt;Kiểm tra dữ liệu&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Thực chất, chúng ta phải hiểu rõ kỹ chúng ta đang có những gì trong tay, thì chúng ta mới dạy cho máy học đủ và đúng được. Các bạn hãy kiểm tra thật kỹ để đảm bảo rằng tập nhãn được gán chính xác, bouding box của đối tượng được vẽ không quá dư thừa, không có missing value, v.v. Một ví dụ nhỏ là tập MNIST, có nhiều hình bị nhập nhằng giữa những con số, chúng ta không thể phân biệt được chính xác hình đó là con số nào bằng mắt thường.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tiếp theo, các bạn hãy quyết định xem rằng mình có nên sử dụng các pre-train model hay không.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nếu tập dữ liệu của bạn gần giống với tập dữ liệu ImageNet, hãy dùng pre-train model. Có các mô hình đã được huấn luyện sẵn là VGG net, ResNet, DenseNet, Xception. Với các kiến trúc khác nhau như VGG(16 và 19 layer), ResNet (50, 101, 152 layer), DenseNet(201,169,121 layer). Ban đầu, đừng sử dụng các kiến trúc có số lượng nhiều (ResNet152, DenseNet201) bởi vì nó rất tốn chi phí tính toán. Chúng ta nên bắt đầu bởi các mô hình nhỏ như VGG16, ResNet50. Hãy chọn một mô hình mà bạn nghĩ là sẽ có kết quả tốt. Sau khi huấn luyện, nếu kết quả không được như ý muốn, hãy tăng số lớp lên (ví dụ ban đầu chọn Resnet50, sau đó nâng lên Resnet101, &amp;hellip;).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu bạn có ít dữ liệu, bạn nãy &amp;ldquo;đóng băng&amp;rdquo; lại trọng số của pre-train model, chỉ huấn luyện phần phân lớp. Bạn cũng có thể thêm phần Dropout để tránh overfit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu tập dữ liệu của bạn không giống một tí nào so với taapk ImageNet, không nên dùng pre-train model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Luôn luôn sử dụng lớp chuẩn hoá trong mô hình. Nếu bạn huấn luyện mô hình với batch-size lớn ( ví dụ lớn hơn 10), hãy sử dụng BatchNormalization Layer trong keras. Nếu bạn sử dụng batch-size nhỏ (ví dụ 1), thì hãy sử dụng InstanceNormalization. Hai layer này đã có sẵn trong Keras, trong các framework khác thì mình không rõ lắm. Có nhiều tác giả đã chỉ ra rằng sử dụng BatchNormalization  sẽ cho kết quả tốt hơn nếu tăng batch-size và hiệu năng sẽ giảm khi batch-size nhỏ, và trong trường hợp batch-size nhỏ thì kết quả sẽ tốt hơn một tí khi sử dụng InstanceNormalization thay cho BatchNormalization. Ngoài ra, các bạn cũng có thể sử dụng GroupNormalization (mình chưa kiểm chứng GroupNormalization có làm tăng độ chính xác hay không).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu bạn sử dụng concatenation layer để kết hợp các feature từ nhiều convolution layers (Li), và những Li  trên rút trích thông tin từ cùng một input (F), thì bạn jay sử dụng SpatialDropout ngay sau concatenation layer trên (Xem hình bên dưới). Khi các convolution layer rút trích thông tin từ cùng một nguồn, các đặc trưng của chúng thường sẽ có mức tương quan với nhau rất lớn. SpatialDropout sẽ loại bỏ những đặc trưng có mức độ liên quan cao này và giúp bạn chống lại hiện tượng overfiting. Thông thường người ta chỉ sử dụng SpatialDropout ở các lớp gần input layer, và không sử dụng chúng ở các lớp cao bên trên.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/spatialdropoutusecase.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Theo andrej Karpathy, để xác định khả năng lưu trữ thông tin của mô hình, hãy rút một phần nhỏ dữ liệu trong tập train của bạn đem đi huấn luyện. Nếu mô hình không overfit, chúng ta tăng số lượng node/layer lên. Nếu mô hình bị overfit, sử dụng các kỹ thuật như L1, L2, Dropout hoăc các kỹ thuật khác để chống lại việc overfit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Các kỹ thuật chuẩn hoá thường sẽ ràng buộc hoặc tinh gọn các trọng số của mô hình. Nó cũng đồng thời giúp chúng ta chống lại việc gradient explosion (gradient mang giá trị lớn khi tính backpropagation) (lý do là các trọng số sẽ bị giới hạn trong đoạn nào đó, ví dụ L2 giới hạn căn bậc 2 tổng bình phương các trọng số =1 chẳng hạn). Ví dụ dưới sử dụng kares và giới hạn max của L2 là 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.constraints import max_norm
# add to Dense layers
model.add(Dense(64, kernel_constraint=max_norm(2.)))
# or add to Conv layers
model.add(Conv2D(64, kernel_constraint=max_norm(2.)))
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Việc sử dụng mean subtraction đôi khi cho kết quả khá tệ, đặc biệt là khi sử dụng trong ảnh xám (grayscale image), hoặc các bài toán phân đoạn ảnh.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Luôn nhớ đến việc xáo trộn dữ liệu (nếu bạn có thể). Nếu được, hãy thực hiện xáo trộn dữ liệu trong quá trình huấn luyện. Việc xáo trộn ảnh sẽ giúp bạn cải thiện độ chính xác.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu bài toán của bạn thuộc nhóm dense prediction (ví dụ phân đoạn ngữ nghĩa - semantic segmentation). Hãy sử dụng pre-train model là Dilated Residual Networks. Mô hình trên cực kỳ hiệu quả cho bài toán này.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Để xác định thông tin ngữ cảnh xung quanh các đối tượng, hãy sử dụng module multi-scale feature pooling. Module này sẽ giúp bạn tăng độ chính xác và thường được sử dụng trong bài toán phân đoạn ngữ nghĩa (semantic segmentation) hoặc bài toán phân đoạn nền (foreground segmentation).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Khi bạn tính độ lỗi hoặc độ chính xác, nếu có vùng nào không trả về nhãn, hoặc nhãn trả về không chắc chắn, hãy bỏ qua việc tính toán chúng đi. Hành động này sẽ giúp mô hình của bạn chắc chắn hơn khi đưa ra quyết định.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng trọng số cho từng class trong quá trình training nếu dữ liệu của bạn có tính bất cân bằng cao. Hãy đặt trọng số lớn cho những lớp có ít dữ liệu, và trọng số nhỏ cho những lớp có nhiều dữ liệu. Trọng số của các lớp có thể được tính toán một cách dễ dàng bằng các sử dụng thư viện skearn trong python. Ngoài ra, bạn có thể sử dụng các kỹ thuật như OverSampling hoặc UnderSampling đối với tập dữ liệu nhỏ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chọn đúng hàm tối ưu. Có rất nhiều hàm tối ưu như Adam, Adagrad, Adadellta, RMSprop, &amp;hellip; Trong các paper người ta thường sử dụng tổ hợp SGD + momentun. Có hai vấn đề cần được xem xét ở đây: Một là nếu bạn muốn mô hình có độ hội tụ nhanh, hãy dùng Adam ( và có khả năng cao là mô hình sẽ bị kẹt ở điểm cực tiểu cục bộ -&amp;gt; không có tính tổng quát hoá cao). Hai là sử dujg SGD + momentun để tìm cực tiểu toàn cục, mô hình này phụ thuộc rất nhiều vào giá trị khởi tạo ban đầu và mô hình thường sẽ hội tụ rất chậm. (Xem hình bên dưới)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/optimal.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Thông thường, chúng ta sẽ chọn learning-rate là (1e-1, 1e-3, 1e-6). Nếu bạn sử dụng pre-train model, hãy sử dụng learning rate nhỏ hơn 1e-3 (ví dụ 1e-4). Nếu bạn không sử dụng pre-train model, hãy sử dụng learning-rate lớn hơn 1e-3. Bạn có thể grid search giá trị learning-rate và chọn ra mô hình cho kết quả tốt nhất. Bạn có thể sử dụng Learing Rate Schedulers giảm giá trịn learning rate trong quá trình huấn luyện mô hình.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bên cạnh việc sử dụng Learing Rate Schedulers để giảm giá trị learning rate, bạn có thể sử dụng một kỹ thuật khác để giảm giá trị learning-rate. Ví dụ sau 5 epochs, độ lỗi trên tập validation không thay đổi, bạn giảm learning-rate đi 10 lần (vd từ 1e-3 thành 1e-4). Trong keras, bạn có thể dễ dàng implement công thức trên bằng việc sử dụng callbacs ReduceLROnPlateau.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reduce = keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.1, patience=5, mode=&#39;auto&#39;)
early = keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=1e-4, patience=10, mode=&#39;auto&#39;)
model.fit(X, Y, callbacks=[reduce, early])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ví dụ trên, chúng ta sẽ giảm learning-rate đi 10 lần khi độ lỗi trên tập validation không thay đổi qua 5 lần lặp liên tiếp, và sẽ dừng việc huấn luyện khi độ lỗi không giảm qua 10 lần lặp liên tiếp.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Nếu bài toán của bạn thuộc nhóm dense prediction như phân đoạn ảnh, phân đoạn ngữ nghĩa, bạn nên sử dụng skip connection để chống lại việc các biên của đối tượng hoặc các thông tin đặc trưng hữu ích của đối tượng bị mất trong max-pooling hoặc strided convolution. Skip connection cũng giúp mô hình học features map từ feature space và image space dễ dàng hơn, và nó cũng giúp cho bạn giảm bị vanish gradient ( giá trị gradient nhỏ dần và gần xấp xỉ bằng 0, nên trọng số không thay đổi nhiều, dẫn đến không hội tụ).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nên sử dụng data augmentation, như là horizontally flipping, rotating, zoom-croping&amp;hellip; để tăng dữ liệu của bạn lên. Việc có nhiều dữ liệu sẽ giúp mô hình có mức tổng quát hoá cao hơn.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng Max-pooling trước Relu để giảm thiểu mức độ tính toán thay vì làm ngược lại. chúng ta biết rằng ReLU trả ra giá trị có ngưỡng cực tiểu là 0 do f(x)=max(0,x), và max-pooling tính max cho các đặc trưng f(x) = max(x1,x2,&amp;hellip;,xi). Nếu ta sử dụng &lt;em&gt;Conv &amp;gt; ReLU &amp;gt; Max-pooling&lt;/em&gt;, ta sẽ tốn i lần tính ReLu, và 1 lần tính max. Nếu ta sử dụng &lt;em&gt;Conv -&amp;gt; max-pooling &amp;gt; ReLU&lt;/em&gt;, ta tốn 1 lần tính max, 1 lần tính ReLU.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu có thể, hãy thử sử dụng Depthwise Separable Convolution. Nó giúp mô hình giảm số lượng tham số so với các convolution khác, ngoài ra nó giúp mô hình chạy nhanh hơn.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Điều cuối cùng là đừng bao giờ từ bỏ. Hãy tin tưởng rằng bạn có thể làm được. Nếu bạn vẫn không thể đạt được độ chính xác như mong đợi, hãy điều chỉnh lại các tham số, kiến trúc mô hình, tập dữ liệu huấn luyện đến khi bạn đạt được mô hình với độ chính xác như bạn đề ra.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo. Cố lên.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lựa chọn mô hình object detectors</title>
      <link>/blog/2018-12-10-design-choices-lessons-learned-and-trends-for-object-detections/</link>
      <pubDate>Mon, 10 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-10-design-choices-lessons-learned-and-trends-for-object-detections/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Các thuật toán phát hiện đối tượng, như các thuật toán thuộc nhóm region proposal hoặc single shot đầu bắt đầu bởi những ý tưởng khác nhau, nhưng sau qua một vài quá trình cập nhật và nâng cấp cho đến thời điểm hiện tại, mô hình chung của chúng đã gần gần giống nhau hơn. Và hai thuật toán trên là hai thuật toán tiêu biểu cạnh tranh nhau danh hiệu thuật toán phát hiện đối tượng nhanh nhất và thuật toán nhận diện chính xác nhất.
Trong bài viết này, chúng ta sẽ đề cập đến một số chiến lược lựa chọn mô hình cho bài toán object detector và một số benchmarks do team Google Research thực hiện.&lt;/p&gt;

&lt;h2 id=&#34;box-encoding-và-loss-function&#34;&gt;Box encoding và loss function&lt;/h2&gt;

&lt;p&gt;Có rất nhiều hàm lỗi và box encoding được sử dụng trong các thuật toán phát hiện đối tượng. Ví dụ, SSD trả ra căn bậc hai của Width và height để giảm tỷ lệ độ lỗi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/box_encoding_architerch.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Các bạn có thể để ý kỹ hơn SSD phiên bản custom không sử dụng cặp toạ độ trái trên - phải dưới mà là cặp tâm - căn bậc hai của with, căn bậc hai của height. Một số thuật toán lại dùng log width, log height, một số lại dùng tâm là Wc/Wa, Wy/ha, với Wc và Wy là toạ độ tâm của đối tượng, wa và ha là chiều dài và rộng của anchor khớp nhất (matching anchor). Các bạn có thể tham khảo thêm ở &lt;a href=&#34;https://arxiv.org/pdf/1611.10012.pdf&#34;&gt;https://arxiv.org/pdf/1611.10012.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Để huấn luyện mô hình tốt hơn, Các nhà nghiên cứu sử dụng các trọng số khác nhau cho các hàm lỗi, YOLO và một ví dụ minh hoạ.&lt;/p&gt;

&lt;h2 id=&#34;feature-extraction&#34;&gt;Feature extraction&lt;/h2&gt;

&lt;p&gt;Trong thực tế, Feature extraction ảnh hưởng lớn trên 2 phần tradeoff là độ chính xác và tốc độ. Nhóm thuật toán ResNet và Inception đi theo tiêu chí là độ chính xác quan trọng hơn tốc độ (và quả thật nhóm thuật toán thuộc họ này có độ chính xác khá cao). MobileNet cung cấp cho chúng ta một mô hình khá nhỏ gọn, sử dụng SSD, mục tiêu của nhóm này là có thể xử lý được trên các thiết bị di động và thời gian xử lý là realtime.&lt;/p&gt;

&lt;h2 id=&#34;feature-extractor-accuracy&#34;&gt;Feature extractor accuracy&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/feature_extraction_accuracy.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn vào hình trên, chúng ta có thể thấy rõ ràng rằng Faster R-CNN và R-FCN đều cho độ chính xác khá tốt trên feature extraction. Ngược lại SSD có kết quả khá tệ.&lt;/p&gt;

&lt;h2 id=&#34;non-max-suppression-nms&#34;&gt;Non-max suppression (nms)&lt;/h2&gt;

&lt;p&gt;Sau khi thu được vị trí của các đối tượng, chúng ta sẽ merge lại các vị trí bị phát hiện trùng lắp. Các thuật toán thuộc nhóm single shot thường cho ra output overlap khá nhiều.&lt;/p&gt;

&lt;h2 id=&#34;data-augmentation&#34;&gt;Data augmentation&lt;/h2&gt;

&lt;p&gt;Ngày nay, hầu hết các thuật toán đều sử dụng Data augmentation. Việc augment data bằng cách cắt xét ảnh, quay ảnh một góc ngẫu nhiên nào đó, giúp cho tránh được overfit trong quá trình huấn luyện, do đó gián tiếp tăng độ chính xác của mô hình.&lt;/p&gt;

&lt;h2 id=&#34;feature-map-strides&#34;&gt;Feature map strides&lt;/h2&gt;

&lt;p&gt;Thuật toán thuộc nhóm single shot thường có tuỳ chọn layter feature map nào được sử dụng để nhận dạng đối tượng. Feature map có stride là 2 nếu chúng ta thực hiện giảm 2 lần độ phân giải. Feature map có độ phân giải thấp thường giữ lại những thông tin đặc trưng tốt của đối tượng và giúp cho detector thực hiện tốt hơn. Tuy nhiên, những đối tượng có kính thước nhỏ sẽ bị mất thông tin trầm trọng và khó để phát hiện ra chúng.&lt;/p&gt;

&lt;h2 id=&#34;speed-v-s-accuracy&#34;&gt;Speed v.s. accuracy&lt;/h2&gt;

&lt;p&gt;Thật khó để trả lời rằng thuật toán nhận dạng đối tượng nào tốt hơn, mà câu trả lời phụ thuộc vào bài toán của bạn đang gặp. Nếu bài toán cần độ chính xác cao, hãy sử dụng ResNet hoặc Inception, nếu bạn cần chạy realtime và độ chính xác tạm chấp nhận, hãy sử dụng MobileNet hoặc YOLO. Không có (chưa có - ít nhất đến thời điểm hiện tại) có thuật toán nào đáp ứng cả 2 tiêu chí là vừa có độ chính xác cao, vừa chạy nhanh cả. Đó là một tradeoff giữa Speed và Accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/gpu-time-resnet-inception-mobilenet.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;object-size&#34;&gt;Object size&lt;/h2&gt;

&lt;p&gt;Với những hình ảnh có kích thước lớn, SSD thực hiện rút trích đặc trưng rất tốt (nên nhớ rằng mô hình rút trích đặc trưng của SSD rất đơn giản). Với những hình ảnh dạng này, SSD có thể so sánh với các thuật toán khác khác về độ chính xác.&lt;/p&gt;

&lt;p&gt;Với nhưng hình ảnh có kích thước nhỏ, chúng ta không nên/không bao giờ xài SSD.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/object_size_compatiple.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn hình ở trên, chúng ta thấy rõ độ chính xác của SSD và các thuật oán khác trên các tập dữ liệu có kích thước khác nhau. Và phụ thuộc vào kích thước dữ liệu của bạn để chọn ra mô hình tối ưu nhất.&lt;/p&gt;

&lt;h2 id=&#34;input-image-resolution&#34;&gt;Input image resolution&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/resolution_reduce.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn hình trên các bạn cũng có thể nhìn thấy rõ. Ảnh có độ phân giải lớn giúp nhận dạng đối tượng tốt hơn rất nhiều so với ảnh có độ phân giải nhỏ. Khi giảm 2 lần độ phân giải trên mỗi chiều (từ 600x600 xuống còn 300x300), trung bình độ chính xác giảm 15.88% trong quá trình huấn luyện, và trung bình giảm 27.4% trong inference.&lt;/p&gt;

&lt;h2 id=&#34;number-of-proposals&#34;&gt;Number of proposals&lt;/h2&gt;

&lt;p&gt;Số lượng proposal được sinh ra ảnh hưởng trực tiếp đến tốc độ của nhóm R-CNN. Ví dụ, Faster R-CNN có thể tăng tốc độ nhận dạng đối tượng gấp 3 lần nếu ta chỉ sử dụng 50 proposal thay vì 300 proposal. Độ chính xác chỉ giảm 4%&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/number-proposal-f-rcnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hình trên, đường nét liền mô tả độ chính xác khi tăng số lượng proposal. Đường nét đứt thể hiện thời gian xử láy tăng khi tăng số lượng proposal.&lt;/p&gt;

&lt;h2 id=&#34;điểm-danh-danh-lại-các-bước-phát-triển-của-object-detection&#34;&gt;Điểm danh danh lại các bước phát triển của object detection&lt;/h2&gt;

&lt;p&gt;Các thuật toán object detection đã phát triển trong một khoảng thời gian dài. Ý tưởng  đầu tiên, đơn giản nhất là chúng ta sẽ sử dụng cửa sổ trượt.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Sliding windows
for window in windows
    patch = get_patch(image, window)
    results = detector(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để tăng tốc, chúng ta sẽ
1. Giảm số lượng windows (R-CNN giảm còn khoảng 2000)
2. Giảm các phép tính trong việc tìm ROI (Fast R-CNN sử dụng feature map thay vì toàn bộ image patchs).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fast R-CNN
feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Việc tìm region_proposal cũng tốn khá nhiều thời gian. Faster R-CNN sử dụng một convolution network thay thế cho region proposal ở bước này (làm giảm thời gian từ 2.3s xuống còn 0.3 giây). Faster R-CNN cũng giới thiệu 1 khái nhiệm là anchor giúp cải thiện độ chính xác và việc huấn luyện trở nên dễ dàng hơn.&lt;/p&gt;

&lt;p&gt;R-FCN đưa ra một điều chỉnh nhỏ, là tiến hành tìm position và sensitive score map trên mỗi ROIS độc lập. Và tính trung bình xác suất xuất hiện đối tượng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# R-FCN
feature_maps = process(image)
ROIs = region_proposal(feature_maps)         
score_maps = compute_score_map(feature_maps)
for ROI in ROIs
    V = pool(score_maps, ROI)     
    class_scores = average(V)         
    class_probabilities = softmax(class_scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R-FCN chạy khá nhanh, nhưng độ chính xác thì thấp hơn một hút so với Faster R-CNN. Để ý kỹ đoạn mã giả ở trên, chúng ta phải trải qua 2 lần tính toán, một lần là tìm các ROIs, một lần là object detection. Thuật toán Single shot detector được đề xuất để sử dụng 1 lần tính toán.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
results = detector3(feature_maps) # No more separate step for ROIs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thuật toán SSD và YOLO đều thuộc nhóm single shot detectors. Cả hai đều sử dụng convolution layer để rút trích đặc trưng và một convolution filter để đưa quyết định. Cả hai đều dùng feature map có độ phân giải thấp (low resolution feature map) để dò tìm đối tượng =&amp;gt; chỉ phát hiện được các đối tượng có kích thước lớn. Một cách tiếp cận là sử dụng các feature map có độ phân giải cao (higher resolution feature map). Nhưng độ chính xác sẽ giảm do thông tin đặc trưng của đối tượng quá hỗn loạn. FPN đưa ra ý tưởng sử dụng feature map trung gian merge giữa feature map high resolution và low resolution. Việc này giúp cho chúng ta vẫn giữ được thông tin đặc trưng hữu ích của đối tượng, đồng thời cũng giữ được thông tin của các đối tượng có kích thước nhỏ. Do đó, độ chính xác cũng tăng lên và phát hiện các đối tượng có các tỷ lệ khác nhau (different scale) tốt hơn.&lt;/p&gt;

&lt;p&gt;Trong quá trình huấn luyện, chúng ta sẽ nhận ra 1 vấn đề rằng backgroup sẽ chiếm 1 phần rất lớn trong bức ảnh. Hoặc một đối tượng nào đó có số mẫu nhiều hơn so với các đối tượng khác. Thuật toán Focal loss được sinh ra để giải quyết vấn đề này.&lt;/p&gt;

&lt;h2 id=&#34;lesson-learned&#34;&gt;Lesson learned&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Feature Pyramid Networks sử dụng các feature map nhiều thông tin hơn để cải thiện độ chính xác.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng các mô hình như ResNet hoặc Inception ResNet nếu mô hình bạn cần độ chính xác và không quan tâm lắm về tốc độ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng các thuật toán thuộc nhóm Single shot detectors như MobileNet nếu bạn cần tốc độ tính toán và có thể chạy được trên mobilenet, yêu cầu về độ chính xác tạm chấp nhận được.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng batch normaliation, nói chung là đều phải chuẩn hoá dữ liệu trước khi sử dụng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lựa chọn anchors cẩn thận (Cái này khá khó, đòi hỏi bạn phải am hiểu khá kỹ về dữ liệu, và nếu set nhầm thì sẽ đi tong).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng data augmentation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch và tham khảo từ nguồn &lt;a href=&#34;https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff&#34;&gt;https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu single shot object detectors</title>
      <link>/blog/2018-12-06-what-do-we-learn-from-single-shot-object-detection/</link>
      <pubDate>Thu, 06 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-06-what-do-we-learn-from-single-shot-object-detection/</guid>
      <description>

&lt;h2 id=&#34;single-shot-detectors&#34;&gt;Single Shot detectors&lt;/h2&gt;

&lt;p&gt;Ở bài trước, chúng ta đã tìm hiểu về region proposal và ứng dụng của nó vào Faster R-CNN. Các thuật toán thuộc nhóm region proposal tuy cho kết quả có độ chính xác cao, nhưng chúng có một nhược điểm rất lớn là thời gian huấn luyện và đưa quyết định rất chậm. Faster R-CNN xử lý khoảng 7 &lt;em&gt;FPS&lt;/em&gt; trên tập dữ liệu PASCAL VOC 2007. Một cách để tăng tốc quá trình tính toán là giảm số lượng tính toán trên mỗi ROI.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_align(feature_maps, ROI)
    results = detector2(patch)    # Giảm khối lượng tính toán ở đây
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một ý tưởng khác, là chúng ta sẽ bỏ qua bước tìm region proposal, mà trực tiếp rút trích boundary boxes và classes trực tiếp từ feature map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
results = detector3(feature_maps) # Không cần tìm ROI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dựa trên ý tưởng sử dụng cửa sổ trượt. Chúng ta sẽ trượt trên feature máp để nhận diện các đối tượng. Với mỗi loại đối tượng khác nhau, chúng ta sửa dụng các cửa sổ trượt có kích thước khác nhau. Cách này thoạt đầu trông có vẻ khá tốt, nhưng điểm yếu của nó là đã sử dụng cửa sổ trượt làm final boundary box. Do đó, giả sử chúng ta có nhiều đối tượng, và mỗi đối tượng có kích thước khác nhau, chúng ta sẽ có rất nhiều cửa sổ trượt để bao phủ hết toàn bộ đối tượng.&lt;/p&gt;

&lt;p&gt;Một ý tưởng cải tiến là chúng ta sẽ định nghĩa trước các cửa sổ trượt, sau đó sẽ tiến hành dự đoán lớp và boundary box ( và Ý tưởng này, nhóm nghiên cứu phát triển thuật toán và đặt tên thuật toán là single shot detectors). Ý tưởng này tương tự như việc sử dụng anchors trong Faster R-CNN, nhưng single shot detectors thực hiện dự đoán boundary box và class đồng thời cùng nhau.&lt;/p&gt;

&lt;p&gt;Ví dụ, giả sử chúng ta có một feature map 8x8 và chúng ta đưa ra k = 4 dự đoán.  Vậy ta có tổng cộng 8x8x4 = 256 dự đoán.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-img-1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Xét hình bên trên, ta có 4 anchors đã được định nghĩa trước ( màu xanh lá cây), và có 4 prediction( màu xanh nước biển) tương ứng với từng anchor trên.&lt;/p&gt;

&lt;p&gt;Với thuật toán Faster R-CNN, chúng ta sử dụng một convolution filter trả ra 5 kết quả dự đoán: 4 giá trị là toạ độ của boundary box, và giá trị còn lại là xác suất xuất hiện đối tượng. Tổng quát hơn, ta có input là D feature map 8x8, output là 8x8x5, số convolution filter trong Faster R-CNN là 3x3xDx8.&lt;/p&gt;

&lt;p&gt;Với single shot detector, input của ta cũng tương tự là 8x8xD, output là 8x8x (4 + C) ( với 4 tương ứng với 4 điểm boundary box, và C là số lượng lớp đối tượng), vậy ta cần một convolution filter là 3x3xDx(4+C)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-architech.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thuật toán Single shot detect chạy khá nhanh, nhưng độ chính xác của nó không cao lắm (không bằng region proposal). Thuật toán có vấn đề về việc nhận dạng các đối tượng có kích thước nhỏ. Ví dụ như hình bên dưới, chúng ta có tổng cộng 9 ông già noel, nhưng thuật toán chỉ nhận diện được có 5 ông.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-img-2.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;ssd&#34;&gt;SSD&lt;/h2&gt;

&lt;p&gt;SSD là mô hình single shot detector sử dụng mạng VGG16 để rút trích đặc trưng. Mô hình như hình bên dưới. Trong đó, những conv có màu xanh nước biển nhạt là những custom convolution layter (ta có thể thêm bớt bao nhiêu tuỳ thích). Convolutional filter layter (là cục màu xanh lá cây) có nhiệm vụ tổng hợp các thông tin lại để đưa quyết định.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-vgg19-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Khi sử dụng mô hình như hình ở trên, chúng ta thấy rằng các custom convolution layter có nhiệm vụ làm giảm chiều và giảm độ phân giải của bức ảnh. Cho nên, mô hình chỉ có khả năng nhận ra các đối tượng có kích thước lớn. Để giải quyết vấn đề này, chúng ta sẽ sử dụng các object detector khác nhau trên mỗi feature maps (xem output của mỗi custom convolution là một feature map).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-vgg19-model1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ảnh bên dưới là sơ đồ số chiều của các feature maps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-vgg19-diagram.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SSD sử dụng các layter có kích thước giảm dần theo độ sâu để nhận dạng đối tượng. Nhìn vào hình vẽ sơ đồ bên dưới của SSD, chúng ra dễ dàng nhận thấy rằng độ phân giải giảm đáng kể qua mỗi layer và có lẽ (chắc chắn) sẽ bỏ sót những đối tượng có kích thước nhỏ ở những lớp có độ phân giải thấp. Nếu trong dự án thực tế của bạn có xảy ra vấn đề này, bạn nên tăng độ phân giải của ảnh đầu vào.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-SSD1-diagram.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;yolo&#34;&gt;YOLO&lt;/h2&gt;

&lt;p&gt;YOLO cũng là một thuật toán sử dụng single shot detector để dò tìm vị trí của các đối tượng trong ảnh. YOLO sử dụng DarkNet để tạo các feature cho bức ảnh (SSD sử dụng VGG16). Mô hình của YOLLO như ảnh ở bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-darknet-diagram.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Khác với kiến trúc mạng SSD ở trên, YOLLO không sử dụng multiple scale feature map (SSD sử dụng các custom convolution layter, qua mỗi layter thì feature maps sẽ có kích thước giảm xuống - các output của custom convolution layer chính là các feature map chúng ta thu được). Thay vào đó, YOLLO sẽ làm phẳng hoá (flatten - vd ma trận 3x3  sẽ biến thành vector 1x9, ma trận 4x5 sẽ biến thành vector 1x20 &amp;hellip;, làm phẳng nghĩa là chúng ta sẽ không dùng bộ lọc nào hết, mà sử dụng các phép biến đổi, nên không làm thay đổi giá trị, chỉ làm thay đổi hình dạng) một phần output của convolution layer và kết hợp với  convolution layer ở trong DarkNet tạo thành feature map (Xem hình ở trên sẽ rõ hơn). Ví dụ ở custom convolution layer chúng ta thu được output có kích thước 28x28x512, chúng ta sẽ flatten thành layter có kích thước 14x14x2048, kết hợp với 1 layter có kích thước 14x14x1024 ở trong darknet, chúng ta thu được feature maps có kích thước là 14x14x3072. Đem feature maps này đi đự đoán.&lt;/p&gt;

&lt;p&gt;YOLOv2 đã thêm vào rất nhiều các cải tiền để cải tăng mAP từ 63.4 trong mô hình đầu tiên (YOLOv1) lên 78.6. Các cải tiền bao gồm thêm batch norm, anchor boxes,  hi-res classifier &amp;hellip; Các bạn có thể xem ở hình bên dưới. YOLO9000 có thể nhận dạng 9000 đối tượng khác nhau.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/yollo-v2-improment.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;YOLOv2 có thể nhận diện các đối tượng với ảnh đầu vào có độ phân giải bất kỳ. Với ảnh có độ phân giải thấp thì mô hình chạy khá nhanh, có FPS cao nhưng mAP lại thấp (tradeoff giữa FPS và mAP).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/yollo-v2-acc.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;yolov3&#34;&gt;YOLOv3&lt;/h2&gt;

&lt;p&gt;YOLOv3 sử dụng darknet với kiến trúc phức hơn để rút trích đặc trưng của bức ảnh. YOLOv3 thêm vào đặc trưng Pyramid để dò tìm các đối tượng có kích thước nhỏ.&lt;/p&gt;

&lt;p&gt;Hình bên dưới so sánh tradeoff giữa thời gian thực thi và độ chính xác giữa các mô hình. Ta thấy rằng thời gian thực thi của YOLOv3 rất nhanh, cùng phân mức mAP 28.8, thời gian YOLOv3 thực thi chỉ tốn 22ms, trong khi đó SSD321 tốn đến 61ms - gấp 3 lần.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-compare.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;feature-pyramid-networks-fpn&#34;&gt;Feature Pyramid Networks (FPN)&lt;/h2&gt;

&lt;p&gt;Dò tìm các đối tượng có kích thước nhỏ là một vấn đề đáng được giải quyết để nâng cao độ chính xác. Và FPN là mô hình mạng được thiết kế ra dựa trên khái niệm pyramid để giải quyết vấn đề này.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/feature-pyramid-network-model1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mô hình FPN kết hợp thông tin của mô hình theo hướng &lt;em&gt;bottom-up&lt;/em&gt; kết hợp với &lt;em&gt;top-down&lt;/em&gt; để dò tìm đối tượng (trong khi đó, các thuật toán khác chỉ thường sử dụng &lt;em&gt;bottom-up&lt;/em&gt;). Khi chúng ta ở bottom và đi lên (up), độ phân giải sẽ giảm, nhưng giá trị ngữ nghĩa sẽ tăng lên. Xem hình mô phỏng bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/feature-pyramid-network-model2.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SSD đưa ra quyết định dựa vào nhiều feature map. Nhưng layer ở bottom không được sử dụng để nhận dạng đối tượng. Vì những layter này có độ phân giải cao nhưng giá trị ngữ nghĩa của chúng lại không đủ cao (thấp) nên những nhà nghiên cứu bỏ chúng đi để tăng tốc độ xử lý. Các nhà nghiêng cứu biện minh rằng các layer ở bottom chưa đủ mức ý nghĩa cần thiết để nâng cao độ chính xác, thêm các layer đó vào sẽ không nâng độ chính xác cao thêm bao nhiêu và họ bỏ chúng đi để có tốc độ tốt hơn. Cho nên, SSD chỉ sử dụng các layer ở lớp trên , và do đó sẽ không nhận dạng được các đối tượng có kích thước nhỏ.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/ssd-model-bottom-up.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Trong khi đó, FPN xây dựng thêm mô hình top-down, nhằm mục đích xây dựng các layer có độ phân giải cao từ các layer có ngữ nghĩa cao.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-top-down-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Trong quá trình xây dựng lại các layer từ top xuống bottom, chúng ta sẽ gặp một vấn đề khá nghiêm trọng là bị mất mát thông tin của các đối tượng. Ví dụ một đối tượng nhỏ khi lên top sẽ không thấy nó, và từ top đi ngược lại sẽ không thể tái tạo lại đối tượng nhỏ đó. Để giải quyết vấn đề này, chúng ta sẽ tạo các kết nối (skip connection) giữa các reconstruction layter và các feature map để giúp quá trình detector dự đoán các vị trí của đối tượng thực hiện tốt hơn (hạn chế tốt nhất việc mất mát thông tin).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-top-down-model-with-skip-connection.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Thêm các skip connection giữa feature map và reconstruction layer&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Đồ hình bên dưới diễn ta chi tiết đường đi theo bottom-up và top-down. P2, P3, P4, P5 là các pyramid  của các feature map.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-top-down-with-bottom-up.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;so-sánh-feature-pyramid-networks-với-region-proposal-network&#34;&gt;So sánh Feature Pyramid Networks với Region Proposal Network&lt;/h2&gt;

&lt;p&gt;FPN không phải là mô hình phát hiện đối tượng. Nó là mô hình phát hiện đặc trưng và được sử dụng trong phát hiện đối tượng. Các feature map từ P2 đến P5 trong hình bên dưới độc lập với nhau và các đặc trưng được sử dụng để phát hiện đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-detail.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-feature-pyramid-networks-trong-fast-r-cnn-và-faster-r-cnn&#34;&gt;Sử dụng Feature Pyramid Networks trong Fast R-CNN và Faster R-CNN&lt;/h2&gt;

&lt;p&gt;Chúng ta hoàn toàn có thể sử dụng FPN trong Fast và Faster R-CNN. Chúng ta sẽ tạo ra các feature map sử dụng FPN, kết quả là ta thu được các puramid (feature map). Sau đó, chúng ta sẽ rút trích các ROIs trên các feature map đó. Dựa trên kích thước của các ROI, chúng ta sẽ chọn feature map nào tốt nhất để tạo các feature patches (các hình chữ nhật nhỏ). Các bạn có thể xem chi tiết ở hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-in-faster-r-cnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;focal-loss-retinanet&#34;&gt;Focal loss (RetinaNet)&lt;/h2&gt;

&lt;p&gt;Trong thực tế, chúng ta sẽ gặp tình trạng tỷ lệ diện tích của các đối tượng trong ảnh nhỏ hơn nhiều so với phần background còn lại, ví dụ chúng ta cần nhận dạng một quả cam có kích thước 100x100 trong ảnh 1920x1080. Vì phần background quá lớn nên chúng sẽ là thành phần &amp;ldquo;thống trị&amp;rdquo; và làm sai lệch kết quả. SSD sử dụng phương pháp lấy mẫu tỷ lệ của object class và background class trong quá trình train (nên background sẽ không còn thống trị nữa).&lt;/p&gt;

&lt;p&gt;Ngoài ra, chúng ta sẽ còn gặp tình trạng là số lượng tỷ lệ object trong ảnh không đều nhau, ví dụ trong tập huấn luyệt có 1000 quả cam và 10 quả táo.&lt;/p&gt;

&lt;p&gt;Focal loss (FL) được sinh ra để giải quyết tình trạng này. Để đi vào chi tiết hơn, chúng ta nhắc lại hàm lỗi cross entropy.&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
  CE(p,y) =
    \begin{cases}
      -\log(p) &amp;amp; \text{if y=1} \\\&lt;br /&gt;
      -\log(1-p) &amp;amp; \text{otherwise}
    \end{cases}&lt;br /&gt;
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;Trong hàm trên thì y nhận giá trị 1 hoặc -1. Giá trị xác xuất nằm trong khoảng (0,1) là xác suất dự đoán cho lớp có y=1.&lt;/p&gt;

&lt;p&gt;Để rõ ràng hơn, ta có thể viết lại hàm trên như sau:&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
  p_t =
    \begin{cases}
      p &amp;amp; \text{if y=1} \\\&lt;br /&gt;
      1-p &amp;amp; \text{otherwise}
    \end{cases}&lt;br /&gt;
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
    CE(p,y) = CE(p_t) = -\log(p_t)
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;Ta có nhận xét rằng đối với các trường hợp được phân loại tốt (có xác suất lớn hơn 0.6) thì hàm loss nhận gái trị với độ lớn lớn hơn 0. Và trong trường hợp dữ liệu có tỷ lệ lệch cao thì tổng các giá trị này sẽ cho ra kết quả loss với một con số rất lớn so với loss của các trường hợp khó phâm loại. Và nó ảnh hưởng đến quá trình huấn luyện.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/focal-lost.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ý tưởng chính của focal-lost là đối với các trường hợp được phân loại tốt ( xác suất lớn hơn 0.5) thì focal lost sẽ làm giảm giá trị cross-entropy của nó xuống nhỏ hơn so với thông thường. Do đó, ta sẽ thêm trọng số cho hàm cross-entropy để biến thành hàm focal lost.&lt;/p&gt;

&lt;p&gt;$$
FL(p_t) = -(1-p_t)^\gamma\log(p_t)
$$&lt;/p&gt;

&lt;p&gt;Với nhân tử được thêm vào được gọi là modulating factor, gamma lớn hơn hoặc bằng 0 được gọi là tham số focusing.&lt;/p&gt;

&lt;p&gt;Nhìn hình ở trên, ta thấy rằng khi gamma = 0 thì hàm focal lost chính là cross-entropy.&lt;/p&gt;

&lt;p&gt;Đặc điểm của hàm lost trên như sau:&lt;/p&gt;

&lt;p&gt;Khi mẫu bị phân loại sai, pt nhỏ, nhân tố modulating factor gần với 1 và hàm lost ít bị ảnh hưởng. Khi pt tiến gần tới 1 (mẫu phân loại tốt), moduling factor sẽ tiến gần tới 0 và hàm loss trong trường hợp này sẽ bị giảm trọng số xuống.&lt;/p&gt;

&lt;p&gt;Tham số focusing sẽ điều chỉnh tỷ lệ các trường hợp được phân loại tốt được giảm trọng số. Khi gamma càng tăng thì ảnh hưởng của modulating factor cũng tăng. Trong các thí nghiệm cho thấy với gamma = 2 hì kết quả đạt được sẽ tốt nhất.&lt;/p&gt;

&lt;p&gt;Hình bên dưới là đồ hình của RetinaNet được xây dựng dựa trên FPN và ResNet sử dung Focal loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/retina-net-fpn-resnet.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch và tham khảo từ nguồn &lt;a href=&#34;https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&#34;&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu region based object detectors</title>
      <link>/blog/2018-12-05-what-do-we-learn-from-object-detection-p1/</link>
      <pubDate>Wed, 05 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-05-what-do-we-learn-from-object-detection-p1/</guid>
      <description>

&lt;h2 id=&#34;sliding-window-detectors&#34;&gt;Sliding-window detectors&lt;/h2&gt;

&lt;p&gt;Bắt đầu từ năm 2012, sau khi mạng AlexNet giành giải nhất cuộc thi 2012 ILSVRC, mọi nghiên cứu về phân lớp dữ liệu đều sử dụng mạng CNN. Kể từ đó đến đây, CNN được coi như là thuật toán thống trị trên mọi publish paper về các bài toán phân lớp đối tượng. Trong khi đó, để nhận dạng 1 đối tượng trong ảnh, các đơn giản nhất là thiết lập một cửa sổ trượt có kích thước là window size trượt từ trái qua phải, từ trên xuống dưới, quét qua toàn bộ bức ảnh. Để phát hiện các đối tượng khác nhau ở các góc nhìn khác nhau, chúng ta sẽ sử dụng cửa sổ trượt có kích thước thay đổi  và ảnh đầu vào có kích thước thay đổi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sliding-window.gif&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-base-various-windowsize.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-base-various-windowsize1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Dựa vào windowsize, chúng ta có thể cắt tấm hình bự thành các tấm hình nhỏ, sau đó sẽ rescale các phần nhỏ của bức ảnh thành các bức ảnh có kích thước cố định.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fixed-size-image.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Các phần của bức ảnh sau đó sẽ được đem qua bộ phân lớp CNN để rút trích các đặc trưng, sau đó sử dụng một hàm phân lớp (như svm, logictic regression) để xác định lớp của bức hình và sử dụng linear regressor để tìm bao đóng của đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sliding-window-detector.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mã giả của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for window in windows
    patch = get_patch(image, window)
    results = detector(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cách dễ dàng nhất để cải tiến hiệu năng của phương pháp này là giảm số lượng tấm hình nhỏ xuống (ví dụ tăng kích thước window size). Cách này còn được giang hồ gọi là  brute force.&lt;/p&gt;

&lt;h2 id=&#34;selective-search&#34;&gt;Selective Search&lt;/h2&gt;

&lt;p&gt;Thay vì hướng tiếp cận brute force ở trên, chúng ta sử dụng phương pháp region proposal để tạo các region of interest (ROIs) để phát hiện đối tượng. Selective search là một phương pháp nằm trong nhóm region proposal. Trong phương pháp selective search(SS), chúng ta bắt đầu bằng cách xem các pixel là mỗi nhóm, các lần lặp tiếp theo, chúng ta sẽ tính khoảng cách ngữ nghĩa (ví dụ như là màu sắc, cường độ ánh sáng) giữa các nhóm và gom các nhóm có khoảng cách gần nhau về chung 1 nhóm để tìm ra phân vùng có khả năng cao nhất chứa đối tượng (ưu tiên gom những nhóm nhỏ trước).&lt;/p&gt;

&lt;p&gt;Như hình bên dưới, dòng đầu tiên, bức ảnh đâu tiên là ta có một vài nhóm nhỏ ở thời điểm X nào đó, ở hình thứ 2 là thực hiện gom nhớm theo cường độ màu sắc của hình số 1, và ở bước cuối cùng, ta thu được hình số 3. Những hình chữ nhật màu xanh ở dòng thứ 2 là những ROIS mô phỏng quá trình gom nhóm để tìm phân vùng có khả năng chứa đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/selectivesearch.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;selective search&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;mạng-r-cnn&#34;&gt;Mạng R-CNN&lt;/h2&gt;

&lt;p&gt;Mạng R-CNN sử dụng phương pháp region proposal để tạo ra khoảng 2000 ROIs. Các vùng sau đó sẽ được rescale theo một kích thước cố định nào đó và được đưa vào mô hình CNN có lớp cuối cùng kà một full conected layer để phân lớp đối tượng và để lọc ra boundary box (bao đóng) của đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-cnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô phỏng việc sử dụng region proposal&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô phỏng việc sử dụng region proposal của RCNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mã giả của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; ROIs = region_proposal(image)
for ROI in ROIs
    patch = get_patch(image, ROI)
    results = detector(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với việc sử dụng ít tấm ảnh nhỏ hơn, và chất lượng của mỗi tấm ảnh nhỏ tốt hơn, Mạng R-CNN chạy nhanh hơn và có độ chính xác cao hơn so với mô hình sử dụng cửa sổ trượt.&lt;/p&gt;

&lt;h2 id=&#34;mạng-fast-r-cnn&#34;&gt;Mạng Fast R-CNN&lt;/h2&gt;

&lt;p&gt;Trong thực tế, các phân vùng của mạng R-CNN bị chồng lấp một phần / toàn bộ với các phân vùng khác. Do đó, việc huấn luyện và thực thi ( inference ) mạng R-CNN diễn ra khá chậm. Nếu chúng ta có 2000 proposal của mạng R-CNN, chúng ta phải thực hiện 2000 lần việc rút trích đặc trưng, một con số khác lớn.&lt;/p&gt;

&lt;p&gt;Thay vì phải rút trích đặc trưng của mỗi proposal, chúng ta có thể dùng CNN rút trích đặc trưng của toàn bộ bức ảnh trước (được feature map), đồng thời rút trích các proposal, lấy các proposal tương ứng trên feature map, rescale và cuối cùng là phân lớp và tìm vị trí của object. Với việc không phải lặp lại 2000 lần việc rút trích đặc trưng, Fast R-CNN giảm thời gian xử lý một cách đáng kể.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fast-r-cnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô phỏngviệc sử dụng propoxal trên feature map và các bước tiếp theo của Fast R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fast-r-cnn-network-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Đồ hình của Fast R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mã giả của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; feature_maps = process(image)
ROIs = region_proposal(image)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với việc không phải lặp đi lặp lại quá trình tìm ra các proposal, tốc độ của thuật toán tăng lên kha khá. Trong thực nghiệm, mô hình Fast R-CNN chạy nhanh hơn gấp 10 lần so với R-CNN trong quá trình huấn luyện. Và nhanh hơn 150 lần trong inferencing.&lt;/p&gt;

&lt;p&gt;Một khác biệt lớn nhất của Fast R-CNN là toàn bộ network (feature extractior, classifier, boundary box regressor) có thể huấn luyện end-to end (nghĩa là từ đầu đến cuối) với 2 hàm độ lỗi (loss funtion) khác nhau cùng lúc (classification loss và localization loss). Điều này làm tăng độ chính xác của mô hình.&lt;/p&gt;

&lt;h2 id=&#34;roi-pooling&#34;&gt;ROI Pooling&lt;/h2&gt;

&lt;p&gt;Vì Fast R-CNN sử dụng full connected layter ở lớp cuối, nên đòi hỏi input của chúng phải có kích thước cố định, nên ta phải resize lại feature về 1 kích thước cố định (do 2000 proposal có kích thước không cố định). Ở đây, các tác giả sử dụng ROI pooling để resize. Thuật toán ở đây được sử dụng như sau:&lt;/p&gt;

&lt;p&gt;Giả sử đơn giản là chúng ta có một proposal có kích thước 5x7, và chúng ta cần resize về hình dạng 2x2. Chúng ta xem kỹ hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/roi-pooling-example.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Hình ảnh mô phỏng ROI pooling&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hình ở bên trái là feature map của chúng ta.&lt;/p&gt;

&lt;p&gt;Hình số 2, vùng hình chữ nhật xanh là vùng proposal 5x7.&lt;/p&gt;

&lt;p&gt;Vì chúng ta cần resize về vùng có kích thước 2x2 (4 phần), nên ta chia vùng proposal 5x7 thành 4 phần (&lt;sup&gt;5&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; =2 dư 3, vậy có 1 phần là 2, 1 phần là 3. Tương tự &lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; = 3 dư 4, vậy có 1 phần 3, một phần 4. Cuối cùng ta có 4 hình chữ nhật có kích thước tương ứng là 2x3, 2x4, 3x3, 3x4) (Hình số 3).&lt;/p&gt;

&lt;p&gt;Hình số 4, từ 4 phần của vùng số 3, ta sẽ lấy giá trị lớn nhất của mỗi vùng.&lt;/p&gt;

&lt;p&gt;Vậy là ta thu được feature proposal có kích thước 2x2 rồi.&lt;/p&gt;

&lt;h2 id=&#34;faster-r-cnn&#34;&gt;Faster R-CNN&lt;/h2&gt;

&lt;p&gt;Nhìn kỹ lại vào thuật toán F-CNN, chúng ta cần phải rút rích 2000 ROIs, và nó là nguyên nhân lớn gây nên sự chậm trể của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; feature_maps = process(image)
ROIs = region_proposal(image)         # Expensive, slow
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thuật toán Faster R-CNN sử dụng mô hình gần như tương tự Fast R-CNN, ngoài việc sử dụng thuật toán interal deep network thay cho selective search để tìm region proposal. Thuật toán mới chạy hiệu quả hơn khi tìm tất cả các ROIs trên mỗi bức ảnh với tốc độ 10ms/&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fater-r-cnn.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô hình của Faster R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fater-r-cnn-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Đồ hình của Faster R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;region-proposal-network&#34;&gt;Region proposal network&lt;/h2&gt;

&lt;p&gt;Mạng region proposal sử dụng feature map làm input đầu vào (như hình trên đã mô phỏng). Mạng sử dụng 1 bộ lọc 3x3, sau đó là một mô hình CNN như ZF hoặc VGG hoặc ResNet ( mô hình càng phức tạp thì độ chính xác cao, nhưng bù lại thời gian tìm kiếm sẽ lâu hơn) để dự đoán boundary box và object score (để xét xem trong bodary box trên có chứa đối tượng hay không. Trong thực tế, mạng Faster R-CNN trả về 2 lớp, lớp thứ nhất là có chứa object, lớp thứ 2 là không chứa object ( ví dụ lớp màu nền - background, lớp abc gì gì đó)) .&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Ví dụ Region proposal network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposal-network-1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô hình Region proposal network sử dụng ZF network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Giả sử tại 1 điểm nào đó trên feature map, RPN có k dự đoán, vậy là chúng ta có tổng cộng 4xk toạ độ điểm và 2xk điểm cho điểm đó. Nhìn ví dụ ở hình bên dưới.&lt;/p&gt;

&lt;p&gt;Hình 1: ta có feature map với kích thước 8x8, vùng hình vuông được tô là filter đang xét có kích thước 3x3.
 Hình 2: Giả sử xét điểm có chấm xanh. Tại điểm đó, ta có k=3 sau khi chạy RPN, và ta được 3 hình chữ nhật như hình.&lt;/p&gt;

&lt;p&gt;Tuy nhiên, tại mỗi điểm, ta chỉ cần 1 boundary box tốt nhất. Cách đơn giản nhất là chọn ngẫu nhiên 1 cái. Nhưng như vậy thì ngay từ đầu ta chọn k=1 luôn cho khoẻ, mắc công gì phải chọn k=3. Trong thực tế, Faster R-CNN không sử dụng phương pháp random select. Thay vào đó, thuật toán một reference boxs hay còn được gọi với tên là anchors và tìm mức độ liên quan của k boundary box với k reference boxs và chọn ra boundary box có độ liên quan lớn nhất.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/anchors-box.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
 &lt;em&gt;Ví dụ anchors box&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Các anchors này được lựa chọn trước đó và được xem là config của mô hình. Faster R-CNN sử dụng 9 anchor boxs (tương ứng với k =3) với 3 box đầu tiên có tỷ lệ width, height khác nhau (ví dụ 2x3, 3x3, 3x2), tiếp đó sẽ scale các box trên với các tỷ lệ khác khau (ví dụ 1.5,3,7) để đạt được 9 anchor boxs.&lt;/p&gt;

&lt;p&gt;Vì mỗi điểm sử dụng 9 anchors, nên ta có tổng cộng 2x9 score và 4x9 location (toạ độ)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/anchorsbox_feature_map.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Anchor box có thể được goijlaf priors hoặc default boundary boxes trong mỗi bài báo khác nhau.&lt;/p&gt;

&lt;h2 id=&#34;hiệu-năng-của-mô-hình-r-cnn&#34;&gt;Hiệu năng của mô hình R-CNN&lt;/h2&gt;

&lt;p&gt;Hình bên dưới mô tả benchmark của các mô hình dẫn xuất từ R-CNN, ta thấy Faster R-CNN có tốc độ tốt nhất.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-performance.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;region-based-fully-convolutional-networks&#34;&gt;Region-based Fully Convolutional Networks&lt;/h2&gt;

&lt;p&gt;Giả sử chúng ta chỉ có toạ độ của mắt phải trong khuôn mặt, chúng ta có thể nội suy ra được vị trí của khuôn mặt. Vì ta biết rằng mắt phải nằm ở vị trí trái trái trong bức hình, và ta từ đó suy ra vị trí của các phần còn lại (xem hình).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-image1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nếu chúng ta có thêm thông tin khác, ví như toạ độ của mắt trái, mũi, miệng, &amp;hellip; thì chúng ta có thể kết hợp chúng để tăng độ chính xác của phân vùng khuôn mặt.&lt;/p&gt;

&lt;p&gt;Trong Faster R-CNN, chúng ta phải tìm proposal sử dụng một mô hình CNN, với khoảng 2000 ROI, chúng ta sẽ tiêu tốn một khoảng thời gian khá lớn để tìm chúng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    class_scores, box = detector(patch)         # Expensive, slow
    class_probabilities = softmax(class_scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong khi đó, với Fast R-CNN, chúng ta chỉ cần phải tính max hoặc average, nên Fast R-CNN nhanh hơn Faster R-CNN ở đây.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
ROIs = region_proposal(feature_maps)         
score_maps = compute_score_map(feature_maps)
for ROI in ROIs
    V = region_roi_pool(score_maps, ROI)     
    class_scores, box = average(V)                   # Much simpler, faster.
    class_probabilities = softmax(class_scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Xét feature map M có kích thước 5x5, trong đó có chứa một hình vuông màu xanh, hình vuông xanh là đối tượng thực tế ta cần tìm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ta chia hình vuông thành phân vùng có kích thước 3x3 (hình 2). Sau đó, chúng ta tạo một feature mới để từ M để tìm ra góc trái trên của hình vuông (chỉ tìm góc trái trên) (hình 3). Feature map mới giống hình thứ 3, chỉ có ô được tô màu vàng ở vị trí [2,2] được bật.&lt;/p&gt;

&lt;p&gt;Với mỗi 9 phần của hình vuông, chúng ta có 9 feature map cho mỗi phần, nhận dạng 9 vùng tương ứng cho một đối tượng. Những feature map này được gọi là position sensitive score map, bởi vì chúng detect ra điểm (score) và sub region của một đối tượng (Xem hình bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example2.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Xét ảnh bên dưới, giả sử vùng được tô gạch đỏ là proposal (hình 1). Chúng ta cũng chia nó thành những phân vùng con có kích thước 3x3 (hình 2). Và tìm xem mức độ giống nhau của mỗi vùng con của proposal và vùng con của feature map như thế nào. Kết quả sẽ được lưu vào một ma trận 3x3 như hình số 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example3.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Quá trình ánh xạ điểm từ score maps và ROIS vào mảng vote_array được gọi là position sensitive ROI pool.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example4.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Sau khi tính toán hết các giá trị của position-sensitive ROI pool, chúng ta sẽ tính trung bình của vote_array để lấy điểm của lớp (class score).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example5.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Giả sử mô hình chúng ta phải nhận dạng k lớp, do có thêm lớp background nên chúng ta có tổng cộng k+1 lớp. Với mỗi lớp chúng ta có 3x3 score map, suy ra chúng ta có tổng cộng là (k+1)x3x3 score maps, (k+1) điểm, và dùng softmax ta sẽ thu được xác suất của mỗi lớp.&lt;/p&gt;

&lt;p&gt;Luồng dữ liệu của mô hình&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-data-flow.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch và tham khảo từ nguồn &lt;a href=&#34;https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9&#34;&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân tích giỏ hàng của website instacart</title>
      <link>/blog/2018-11-13-instacart-market-basket-analysis/</link>
      <pubDate>Tue, 13 Nov 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-11-13-instacart-market-basket-analysis/</guid>
      <description>

&lt;h1 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h1&gt;

&lt;p&gt;Instacart là một startup cung ứng đồ tạp hóa qua website và ứng dụng di động. Người dùng chỉ cần chọn đồ muốn mua tại các chuỗi bán lẻ và đặt đồ, Instacart sẽ đi mua và giao đến tận tay họ. Đến nay, Instacart hoạt động tại 15.000 cửa hàng tạp hoá tại 4.000 thành phố với khoảng 50.000 “trợ lý mua sắm”. Team data science của instacart đóng vai trò rất quan trọng trong việc cung cấp trải nghiệm người dùng trong việc sử dụng app để mua hàng. Hiện tại, họ đang sử dụng các dữ liệu của khách hàng để tạo nên mô hình dự đoán sản phẩm nào người dùng sẽ mua lại, sẽ mua thử lần đầu tiên, hoặc sẽ thêm vào giỏ hàng. Hiện họ đã publish khoảng 3 triệu đơn hàng của họ để các nhà khoa học dữ liệu khác sử dụng và nghiên cứu.&lt;/p&gt;

&lt;h1 id=&#34;dẫn-nhập&#34;&gt;Dẫn nhập&lt;/h1&gt;

&lt;h2 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h2&gt;

&lt;p&gt;Các bạn có thể download dữ liệu ở &lt;a href=&#34;https://www.instacart.com/datasets/grocery-shopping-2017&#34;&gt;https://www.instacart.com/datasets/grocery-shopping-2017&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Các file bao gồm:&lt;/p&gt;

&lt;p&gt;File aisles.csv (134 dòng) có 2 cột là aisle_id,aisle&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;aisle_id,aisle  
1,prepared soups salads  
2,specialty cheeses  
3,energy granola bars  
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File departments.csv (21 dòng) gồm 2 cột là department_id,department&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;department_id,department  
1,frozen  
2,other  
3,bakery   
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File order_products__(prior|train).csv (trên 30 triệu dòng)&lt;/p&gt;

&lt;p&gt;Tập này chứa danh sách sản phẩm được mua trong mỗi đơn hàng. File order_products__prior.csv chứa sản phẩm của đơn hàng trước đó của khách hàng. &amp;lsquo;reordered&amp;rsquo; nói rằng sản phẩm này trong đơn hàng hiện tại đã được mua ở đơn hàng trước đó. Vì vậy, sẽ có đơn hàng không được gán là &amp;lsquo;reordered&amp;rsquo; (chúng ta có thể gán nhãn là None hoặc cái gì đó cũng được để chỉ các sản phẩm này). &amp;lsquo;add_to_cart_order&amp;rsquo; là thứ tự của sp được thêm vào giỏ hàng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_id,product_id,add_to_cart_order,reordered  
 1,49302,1,1  
 1,11109,2,1  
 1,10246,3,0  
 ... 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File orders.csv (3.4 triệu dòng, 206k users): chứa thông tin của đơn hàng, trong đó, order_dow là ngày trong tuần, eval_set thuộc một trong 3 loại là prior, train, test.  order_number là thứ tự của đơn hàng của user này.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_id,user_id,eval_set,order_number,order_dow,order_hour_of_day,days_since_prior_order  
 2539329,1,prior,1,2,08,  
 2398795,1,prior,2,3,07,15.0  
 473747,1,prior,3,3,12,21.0  
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File products.csv ((50k dòng) chứa thông tin sản phẩm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; product_id,product_name,aisle_id,department_id
 1,Chocolate Sandwich Cookies,61,19  
 2,All-Seasons Salt,104,13  
 3,Robust Golden Unsweetened Oolong Tea,94,7  
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với mỗi order_id trong tập test ở file orders.csv, chúng ta phải dự đoán các sản phẩm nào người dùng sẽ mua lại (&amp;ldquo;reorder&amp;rdquo;) thuộc đơn hàng đó. Nếu bạn dự đoán đó là đơn hàng không có sản phẩm nào được mua lại, thì ta sẽ điền vào giá trị &amp;lsquo;None&amp;rsquo;&lt;/p&gt;

&lt;p&gt;Ví dụ về kết quả dự đoán:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_id,products  
17,1 2  
34,None  
137,1 2 3  
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;thực-hành&#34;&gt;Thực hành&lt;/h1&gt;

&lt;p&gt;Đầu tiên, ta sẽ import một số thư viện cơ bản để sử dụng, và load tất cả các file lên. Lưu ý một chút là ở đây, mình để tất cả các file trong thư mục data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
from collections import OrderedDict

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

from sklearn import metrics, cross_validation
from sklearn.metrics import f1_score
from sklearn.preprocessing import MinMaxScaler

#Import the files
aisles_df = pd.read_csv(&#39;data/aisles.csv&#39;)
products_df = pd.read_csv(&#39;data/products.csv&#39;)
orders_df = pd.read_csv(&#39;data/orders.csv&#39;)
order_products_prior_df = pd.read_csv(&#39;data/order_products__prior.csv&#39;)
departments_df = pd.read_csv(&#39;data/departments.csv&#39;)
order_products_train_df = pd.read_csv(&#39;data/order_products__train.csv&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau đó, mình sẽ merge đơn hàng vào chi tiết đơn hàng của tập train và tập prior&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_products_train_df = order_products_train_df.merge(orders_df.drop(&#39;eval_set&#39;, axis=1), on=&#39;order_id&#39;)
order_products_prior_df = order_products_prior_df.merge(orders_df.drop(&#39;eval_set&#39;, axis=1), on=&#39;order_id&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;show ra 5 dòng đầu tiên của order_products_train_df&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(order_products_train_df.head())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   order_id  product_id  add_to_cart_order  reordered  user_id  order_number  order_dow  order_hour_of_day  days_since_prior_order
0         1       49302                  1          1   112108             4          4                 10                     9.0
1         1       11109                  2          1   112108             4          4                 10                     9.0
2         1       10246                  3          0   112108             4          4                 10                     9.0
3         1       49683                  4          0   112108             4          4                 10                     9.0
4         1       43633                  5          1   112108             4          4                 10                     9.0

[5 rows x 9 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tổng cộng mình có 9 cột, ý nghĩa các cột mình có giải thích ở trên rồi nha.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta tạo tập tập dữ liệu đếm số lượng sản phẩm của từng người mua&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_product_df = (order_products_prior_df.groupby([&#39;product_id&#39;,&#39;user_id&#39;],as_index=False) 
                                          .agg({&#39;order_id&#39;:&#39;count&#39;}) 
                                          .rename(columns={&#39;order_id&#39;:&#39;user_product_total_orders&#39;}))

train_ids = order_products_train_df[&#39;user_id&#39;].unique() 
df_X = user_product_df[user_product_df[&#39;user_id&#39;].isin(train_ids)]
print(df_X.head())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   product_id  user_id  user_product_total_orders
0           1      138                          2
1           1      709                          1
3           1      777                          1
6           1     1052                          2
9           1     1494                          3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây, người 138 mua sản phẩm 1 2 lần, người 709 mua sản phẩm 1 1 lần, &amp;hellip; tương tự như vậy cho các user và product khác.&lt;/p&gt;

&lt;p&gt;Bước tiếp theo, chúng ta sẽ liệt kê các sản phẩm người dùng đã mua:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_carts = (order_products_train_df.groupby(&#39;user_id&#39;,as_index=False)
                                      .agg({&#39;product_id&#39;:(lambda x: set(x))})
                                      .rename(columns={&#39;product_id&#39;:&#39;latest_cart&#39;}))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;print(train_carts.head())&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  user_id                                        latest_cart
0        1  {196, 26405, 27845, 46149, 13032, 39657, 26088...
1        2  {24838, 11913, 45066, 31883, 48523, 38547, 248...
2        5  {40706, 21413, 20843, 48204, 21616, 19057, 201...
3        7  {17638, 29894, 47272, 45066, 13198, 37999, 408...
4        8  {27104, 15937, 5539, 41540, 31717, 48230, 2224...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mối tương quan giữa sản phẩm được add to card và sản phẩm được mua&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_X = df_X.merge(train_carts, on=&#39;user_id&#39;)
df_X[&#39;in_cart&#39;] = (df_X.apply(lambda row: row[&#39;product_id&#39;] in row[&#39;latest_cart&#39;], axis=1).astype(int))

print(df_X.head())

print(df_X[&#39;in_cart&#39;].value_counts())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# df_X.head()
   product_id  user_id  user_product_total_orders latest_cart  in_cart
0           1      138                          2     {42475}        0
1         907      138                          2     {42475}        0
2        1000      138                          1     {42475}        0
3        3265      138                          1     {42475}        0
4        4913      138                          1     {42475}        0

# df_X[&#39;in_cart&#39;].value_counts()
0    7645837
1     828824
Name: in_cart, dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tỷ lệ khoảng 9.7%. Điều này nói lên rằng, người dùng trong 1 phiên mua hàng có thể add rất nhiều sản phẩm vào giỏ, nhưng chỉ khoảng 10% sản phẩm họ mua thật sự, hơn 90% sản phẩm còn lại sẽ bị remove trước khi nọ nhấn nút thanh toán.&lt;/p&gt;

&lt;h1 id=&#34;xây-dựng-tập-đặc-trưng&#34;&gt;Xây dựng tập đặc trưng&lt;/h1&gt;

&lt;h2 id=&#34;đặc-trưng-sản-phẩm&#34;&gt;Đặc trưng sản phẩm&lt;/h2&gt;

&lt;p&gt;Với đặc trưng sản phẩm, chúng ta sẽ rút trích 2 đặc trưng đơn giản là tổng số lượng đơn hàng của một sản phẩm và trung bình số lượng đơn hàng có chứa sản phẩm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prod_features = [&#39;product_total_orders&#39;,&#39;product_avg_add_to_cart_order&#39;]

prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;],as_index=False)
                                           .agg(OrderedDict(
                                                   [(&#39;order_id&#39;,&#39;nunique&#39;),
                                                    (&#39;add_to_cart_order&#39;,&#39;mean&#39;)])))
prod_features_df.columns = [&#39;product_id&#39;] + prod_features
print(prod_features_df.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
   product_id  product_total_orders  product_avg_add_to_cart_order
0           1                  1852                       5.801836
1           2                    90                       9.888889
2           3                   277                       6.415162
3           4                   329                       9.507599
4           5                    15                       6.466667

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add thêm đặc trưng sản phẩm vào trong tập huấn luyện&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
df_X = df_X.merge(prod_features_df, on=&#39;product_id&#39;)

#note that dropping rows with NA product_avg_days_since_prior_order is likely a naive choice 
df_X = df_X.dropna()
print(df_X.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   product_id  user_id              ...               product_total_orders product_avg_add_to_cart_order
0           1      138              ...                               1852                      5.801836
1           1      709              ...                               1852                      5.801836
2           1      777              ...                               1852                      5.801836
3           1     1052              ...                               1852                      5.801836
4           1     1494              ...                               1852                      5.801836

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;đặc-trưng-người-dùng&#34;&gt;Đặc trưng người dùng&lt;/h2&gt;

&lt;p&gt;Với người dùng, chúng sa sử dụng các đặc trưng là: Tổng số lượng đơn hàng, trung bình số sản phẩm trong 1 đơn hàng, tổng số lượng sản phẩm người dùng mua, Trung bình số ngày user sẽ mua đơn hàng tiếp theo&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_features = [&#39;user_total_orders&#39;,&#39;user_avg_cartsize&#39;,&#39;user_total_products&#39;,&#39;user_avg_days_since_prior_order&#39;]

user_features_df = (order_products_prior_df.groupby([&#39;user_id&#39;],as_index=False)
                                           .agg(OrderedDict(
                                                   [(&#39;order_id&#39;,[&#39;nunique&#39;, (lambda x: x.shape[0] / x.nunique())]),
                                                    (&#39;product_id&#39;,&#39;nunique&#39;),
                                                    (&#39;days_since_prior_order&#39;,&#39;mean&#39;)])))

user_features_df.columns = [&#39;user_id&#39;] + user_features
print(user_features_df.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và chúng ta merge tiếp đặc trưng user vào trong tập huấn luyện.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
df_X = df_X.merge(user_features_df, on=&#39;product_id&#39;)

#note that dropping rows with NA product_avg_days_since_prior_order is likely a naive choice 
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;đặc-trưng-mối-tương-quan-giữa-người-dùng-và-sản-phẩm&#34;&gt;Đặc trưng mối tương quan giữa người dùng và sản phẩm&lt;/h2&gt;

&lt;p&gt;Ở đây, chúng ta sử dụng đặc trưng trung bình số sản phẩm của 1 người được thêm vào đơn hàng và tần suất 1 sản phẩm 1 user add vào đơn hàng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_prod_features = [&#39;user_product_avg_add_to_cart_order&#39;]

user_prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;,&#39;user_id&#39;],as_index=False) \
                                                .agg(OrderedDict(
                                                     [(&#39;add_to_cart_order&#39;,&#39;mean&#39;)])))

user_prod_features_df.columns = [&#39;product_id&#39;,&#39;user_id&#39;] + user_prod_features
df_X = df_X.merge(user_prod_features_df,on=[&#39;user_id&#39;,&#39;product_id&#39;])
df_X[&#39;user_product_order_freq&#39;] = df_X[&#39;user_product_total_orders&#39;] / df_X[&#39;user_total_orders&#39;] 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;bổ-sung-thêm-đặc-trưng&#34;&gt;Bổ sung thêm đặc trưng&lt;/h1&gt;

&lt;p&gt;Ngoài các đặc trưng cơ bản ở trên, ta sẽ bổ sung thêm một số đặc trưng khác:&lt;/p&gt;

&lt;p&gt;Đặc trưng sản phẩm: bổ sung thêm 3 đặc trưng trung bình ngày trong tuần được đặt hàng  (cột order_down), trung bình giờ đặt hàng (cột order_hour_of_day), trung bình ngày đặt hàng kể từ lần đặt trước đó (cột days_since_prior_order) theo sản phẩm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prod_features = [&#39;product_avg_order_dow&#39;, &#39;product_avg_order_hour_of_day&#39;, &#39;product_avg_days_since_prior_order&#39;]

prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;], as_index=False)
                                     .agg(OrderedDict(
                                     [(&#39;order_dow&#39;,&#39;mean&#39;),
                                      (&#39;order_hour_of_day&#39;, &#39;mean&#39;),
                                      (&#39;days_since_prior_order&#39;, &#39;mean&#39;)])))

prod_features_df.columns = [&#39;product_id&#39;] + prod_features

df_X = df_X.merge(prod_features_df, on=&#39;product_id&#39;)
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trưng người dùng: bổ sung thêm 2 cột đặc trung trung bình ngày trong tuần được đặt hàng  (cột order_down) và  trung bình giờ đặt hàng (cột order_hour_of_day) theo người dùng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_features = [&#39;user_avg_order_dow&#39;,&#39;user_avg_order_hour_of_day&#39;]

user_features_df = (order_products_prior_df.groupby([&#39;user_id&#39;],as_index=False)
                                           .agg(OrderedDict(
                                                   [(&#39;order_dow&#39;,&#39;mean&#39;),
                                                    (&#39;order_hour_of_day&#39;,&#39;mean&#39;)])))

user_features_df.columns = [&#39;user_id&#39;] + user_features
df_X = df_X.merge(user_features_df, on=&#39;user_id&#39;)
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trung người dùng - sản phẩm: Bổ sung thêm đặc trưng tung bình trên cột order_down, order_hour_of_day, days_since_prior_order theo người dùng và sản phẩm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
user_prod_features = [&#39;user_product_avg_days_since_prior_order&#39;,
                      &#39;user_product_avg_order_dow&#39;,
                      &#39;user_product_avg_order_hour_of_day&#39;]

user_prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;,&#39;user_id&#39;],as_index=False) \
                                                .agg(OrderedDict(
                                                     [(&#39;days_since_prior_order&#39;,&#39;mean&#39;),
                                                     (&#39;order_dow&#39;,&#39;mean&#39;),
                                                     (&#39;order_hour_of_day&#39;,&#39;mean&#39;)])))

user_prod_features_df.columns = [&#39;product_id&#39;,&#39;user_id&#39;] + user_prod_features 

df_X = df_X.merge(user_prod_features_df, on=[&#39;user_id&#39;, &#39;product_id&#39;])
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trưng độ lệch: Tính độ lệch của của một số đặc trưng so với trung bình của chúng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Create delta columns to compare how users perform against averages
df_X[&#39;product_total_orders_delta_per_user&#39;] = df_X[&#39;product_total_orders&#39;] - df_X[&#39;user_product_total_orders&#39;]

df_X[&#39;product_avg_add_to_cart_order_delta_per_user&#39;] = df_X[&#39;product_avg_add_to_cart_order&#39;] - \
                                                            df_X[&#39;user_product_avg_add_to_cart_order&#39;]

df_X[&#39;product_avg_order_dow_per_user&#39;] = df_X[&#39;product_avg_order_dow&#39;] - df_X[&#39;user_product_avg_order_dow&#39;]

df_X[&#39;product_avg_order_hour_of_day_per_user&#39;] = df_X[&#39;product_avg_order_hour_of_day&#39;] - \
                                                            df_X[&#39;user_product_avg_order_hour_of_day&#39;]

df_X[&#39;product_avg_days_since_prior_order_per_user&#39;] = df_X[&#39;product_avg_days_since_prior_order&#39;] - \
                                                            df_X[&#39;user_product_avg_days_since_prior_order&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bổ sung thêm đặc trưng department name&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;f_departments_df = products_df.merge(departments_df, on = &#39;department_id&#39;)
f_departments_df = f_departments_df[[&#39;product_id&#39;, &#39;department&#39;]]

df_X = df_X.merge(f_departments_df, on = &#39;product_id&#39;)
df_X = df_X.dropna()
df_X = pd.concat([df_X, pd.get_dummies(df_X[&#39;department&#39;])], axis=1)
del df_X[&#39;department&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta có tổng cộng 21 department name, vậy chúng ta thêm 21 cột, một cột tương ứng với một department name, ví dụ: alcohol,babies ,bakery, &amp;hellip; Sản phẩm thuộc department name thì sẽ được đánh số 1, không thuộc department name thì đánh số 0.&lt;/p&gt;

&lt;h1 id=&#34;huấn-luyện-mô-hình&#34;&gt;Huấn luyện mô hình&lt;/h1&gt;

&lt;p&gt;Chia tập dữ liệu thành &lt;sup&gt;80&lt;/sup&gt;&amp;frasl;&lt;sub&gt;20&lt;/sub&gt; trong đó 80% là tập train, 20% là tập test. Sử dụng k-fold-cross_validation với k=10&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
np.random.seed(99)
total_users = df_X[&#39;user_id&#39;].unique() 
test_users = np.random.choice(total_users, size=int(total_users.shape[0] * .20), replace=False)



test_user_sets = []
length = len(test_users)
cv = 10


for x in range (0, cv):
    start = int(x/cv*length)
    finish = int((x+1)/cv*length)
    test_user_sets.append(test_users[start:finish])

cv_f1_scores = []
cv_f1_scores_balanced = []
cv_f1_scores_10fit = []

for test_user_set in test_user_sets:
    df_X_tr, df_X_te = df_X[~df_X[&#39;user_id&#39;].isin(test_user_set)], df_X[df_X[&#39;user_id&#39;].isin(test_user_set)] 

    y_tr, y_te = df_X_tr[&#39;in_cart&#39;], df_X_te[&#39;in_cart&#39;]
    X_tr, X_te = df_X_tr.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \
             df_X_te.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \
        
    scaler = MinMaxScaler()
    X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns=X_tr.columns)
    X_te = pd.DataFrame(scaler.fit_transform(X_te), columns=X_te.columns)
    
    lr = LogisticRegression(C=10000000) 
    lr_balanced = LogisticRegression(class_weight=&#39;balanced&#39;, C=10000000)
    lr_10x = LogisticRegression(class_weight={1 : 6, 0 : 1}, C=10000000)
    
    lr.fit(X_tr, y_tr)
    cv_f1_scores.append(f1_score(lr.predict(X_te), y_te))

    lr_balanced.fit(X_tr, y_tr)
    cv_f1_scores_balanced.append(f1_score(lr_balanced.predict(X_te), y_te))

    lr_10x.fit(X_tr, y_tr)
    cv_f1_scores_10fit.append(f1_score(lr_10x.predict(X_te), y_te))   

print(&amp;quot;cv_f1_scores: &amp;quot; +str( np.mean(cv_f1_scores)))
print(&amp;quot;cv_f1_scores_balanced: &amp;quot;+str(np.mean(cv_f1_scores_balanced)))
print(&amp;quot;cv_f1_scores_10fit: &amp;quot;+str(np.mean(cv_f1_scores_10fit)))

df_X_tr, df_X_te = df_X[~df_X[&#39;user_id&#39;].isin(test_users)], df_X[df_X[&#39;user_id&#39;].isin(test_users)]

y_tr, y_te = df_X_tr[&#39;in_cart&#39;], df_X_te[&#39;in_cart&#39;]
X_tr, X_te = df_X_tr.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \
             df_X_te.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \

lr_10x = LogisticRegression(class_weight={1 : 6, 0 : 1}, C=10000000)
lr_10x.fit(X_tr, y_tr)
print(&amp;quot;F1 store all: &amp;quot;+str(f1_score(lr_10x.predict(X_te), y_te)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_f1_scores: 0.2026889989037295
cv_f1_scores_balanced: 0.3816810646496983
cv_f1_scores_10fit: 0.3899595078917494

F1 store all: 0.3808374055616213
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử in ra hệ số của hàm hồi quy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;coefficients = pd.DataFrame(lr_10x.coef_, columns = X_tr.columns)
coefficients = np.exp(coefficients)
print(coefficients.T)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_product_total_orders                     1.160475
product_total_orders                          1.077254
product_avg_add_to_cart_order                 0.915343
user_total_orders                             0.983272
user_avg_cartsize                             1.059655
user_total_products                           0.993839
user_avg_days_since_prior_order               0.993513
user_product_avg_add_to_cart_order            0.950418
user_product_order_freq                       1.051246
product_avg_order_dow                         0.994744
product_avg_order_hour_of_day                 1.010971
product_avg_days_since_prior_order            0.994498
user_avg_order_dow                            0.997298
user_avg_order_hour_of_day                    1.012958
user_product_avg_days_since_prior_order       1.003382
user_product_avg_order_dow                    0.994477
user_product_avg_order_hour_of_day            1.003457
product_total_orders_delta_per_user           0.928288
product_avg_add_to_cart_order_delta_per_user  0.963095
product_avg_order_dow_per_user                1.000268
product_avg_order_hour_of_day_per_user        1.007489
product_avg_days_since_prior_order_per_user   0.991147
alcohol                                       0.998866
babies                                        1.000313
bakery                                        1.003098
beverages                                     1.007733
breakfast                                     1.000117
bulk                                          0.999980
canned goods                                  0.995017
dairy eggs                                    1.018069
deli                                          1.002720
dry goods pasta                               0.997379
frozen                                        1.000752
household                                     0.992164
international                                 0.996822
meat seafood                                  1.000340
missing                                       1.001953
other                                         0.999607
pantry                                        0.972038
personal care                                 0.992072
pets                                          1.000466
produce                                       1.017809
snacks                                        1.004893
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử show confusion matrix của dữ liệu:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&#39;fivethirtyeight&#39;)

def plot_confusion_matrix(cm,title=&#39;Confusion matrix&#39;, cmap=plt.cm.Reds):
    plt.imshow(cm, interpolation=&#39;nearest&#39;,cmap=cmap)
    plt.title(title)
    plt.colorbar()
    plt.tight_layout()
    plt.ylabel(&#39;True label&#39;)
    plt.xlabel(&#39;Predicted label&#39;)

#y_tr=np.ravel(y_tr)

train_acc=lr_10x.score(X_tr, y_tr)
test_acc=lr_10x.score(X_te, y_te)
print(&amp;quot;Training Data Accuracy: %0.2f&amp;quot; %(train_acc))
print(&amp;quot;Test Data Accuracy:     %0.2f&amp;quot; %(test_acc))
    
y_true = y_te
y_pred = lr_10x.predict(X_te)


conf = confusion_matrix(y_true, y_pred)
print(conf)

print (&#39;\n&#39;)
print (&amp;quot;Precision:              %0.2f&amp;quot; %(conf[1, 1] / (conf[1, 1] + conf[0, 1])))
print (&amp;quot;Recall:                 %0.2f&amp;quot;% (conf[1, 1] / (conf[1, 1] + conf[1, 0])))
    
cm=confusion_matrix(y_true, y_pred, labels=[0, 1])
    
plt.figure()
plot_confusion_matrix(cm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Training Data Accuracy: 0.83
Test Data Accuracy:     0.83
[[1236979  190126]
 [  78107   82493]]


Precision:              0.30
Recall:                 0.51
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/instartcart_plot_confusion_matrix.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Show đường cong ROC của dữ liệu&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from sklearn.metrics import roc_curve, auc

y_score = lr_10x.predict_proba(X_te)[:,1]

fpr, tpr,_ = roc_curve(y_te, y_score)
roc_auc = auc(fpr, tpr)

plt.figure()
# Plotting our Baseline..
plt.plot([0,1],[0,1], linestyle=&#39;--&#39;, color = &#39;black&#39;)
plt.plot(fpr, tpr, color = &#39;green&#39;)
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/instartcart_roc.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dự đoán giá chứng khoán SP500 sử dụng LSTM</title>
      <link>/blog/2018-11-10-stock-prediction_v1/</link>
      <pubDate>Sat, 10 Nov 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-11-10-stock-prediction_v1/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Ở bài viết này, mình sẽ xây dựng mô hình hơn giản để áp dụng vào tập dữ liệu giá chứng khoáng. Mục tiêu của bài này là chúng ta sẽ dự đoán chỉ số S&amp;amp;P 500 sử dụng LSTM. Các bạn có nhu cầu tìm hiểu thêm về chỉ số sp 500 có thể đọc thêm ở &lt;a href=&#34;https://vi.wikipedia.org/wiki/S%26P_500&#34;&gt;https://vi.wikipedia.org/wiki/S%26P_500&lt;/a&gt;. Đây là một ứng dụng nhỏ, không có ý nghĩa nhiều ở thực tế do khi phân tích chứng khoán, ta còn xét thêm rất nhiều yếu tố phụ nữa. Mô hình này thực chất chỉ là một trong những mô hình chơi chơi.&lt;/p&gt;

&lt;h2 id=&#34;dẫn-nhập&#34;&gt;Dẫn nhập&lt;/h2&gt;

&lt;h3 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h3&gt;

&lt;p&gt;Các bạn có thể download dữ liệu ở &lt;a href=&#34;https://github.com/AlexBlack2202/alexmodel/blob/master/GSPC.csv&#34;&gt;https://github.com/AlexBlack2202/alexmodel/blob/master/GSPC.csv&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Đầu tiên, như thường lệ, chúng ta sẽ import các thư viện cần thiết để sử dụng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from subprocess import check_output
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.models import Sequential
from sklearn.cross_validation import  train_test_split
import time #helper libraries
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from numpy import newaxis

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đọc dữ liệu lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
file_name =&#39;GSPC.csv&#39;

prices_dataset =  pd.read_csv(file_name, header=0)

``

Xem kích thước của dữ liệu:

```python
print(prices_dataset.shape)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(17114, 7)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả là ta có  17114 ngàn dòng và 7 cột. Thử show 10 row đầu tiên của dữ liệu lên xem như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(prices_dataset.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;         Date       Open       High        Low      Close  Adj Close   Volume
0  1950-11-09  19.790001  19.790001  19.790001  19.790001  19.790001  1760000
1  1950-11-10  19.940001  19.940001  19.940001  19.940001  19.940001  1640000
2  1950-11-13  20.010000  20.010000  20.010000  20.010000  20.010000  1630000
3  1950-11-14  19.860001  19.860001  19.860001  19.860001  19.860001  1780000
4  1950-11-15  19.820000  19.820000  19.820000  19.820000  19.820000  1620000

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cột đầu tiên là ngày, sau đó là giá mở cửa, giá giao dịch cao nhất, giá giao dịch thấp nhât, giá đóng cử, giá đóng cửa đã điều chỉnh, khối lượng giao dịch.&lt;/p&gt;

&lt;p&gt;Plot đồ thị của mã SP500 lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

plt.plot(prices_dataset.Open.values, color=&#39;red&#39;, label=&#39;open&#39;)
plt.plot(prices_dataset.Close.values, color=&#39;green&#39;, label=&#39;close&#39;)
plt.plot(prices_dataset.Low.values, color=&#39;blue&#39;, label=&#39;low&#39;)
plt.plot(prices_dataset.High.values, color=&#39;black&#39;, label=&#39;high&#39;)
plt.title(&#39;stock price&#39;)
plt.xlabel(&#39;time [days]&#39;)
plt.ylabel(&#39;price&#39;)
plt.legend(loc=&#39;best&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sp500indexv1.png&#34; alt=&#34;Hình ảnh đừng đồ thị của chỉ số sp 500&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hình với số lượng hơi nhiều nên khó phân biệt được giá trị của dữ liệu, chúng ta thử show đồ thị của 50 ngày cuối cùng trong dữ liệu.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prices_dataset_tail_50 = prices_dataset.tail(50)

plt.plot(prices_dataset_tail_50.Open.values, color=&#39;red&#39;, label=&#39;open&#39;)
plt.plot(prices_dataset_tail_50.Close.values, color=&#39;green&#39;, label=&#39;close&#39;)
plt.plot(prices_dataset_tail_50.Low.values, color=&#39;blue&#39;, label=&#39;low&#39;)
plt.plot(prices_dataset_tail_50.High.values, color=&#39;black&#39;, label=&#39;high&#39;)
plt.title(&#39;stock price&#39;)
plt.xlabel(&#39;time [days]&#39;)
plt.ylabel(&#39;price&#39;)
plt.legend(loc=&#39;best&#39;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sp500index_tail_50.png&#34; alt=&#34;Hình ảnh đừng đồ thị của chỉ số sp 500&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hình ảnh trông khá rõ ràng và trực quan hơn rất nhiều.&lt;/p&gt;

&lt;p&gt;Chúng ta sẽ bỏ đi cột DATE,Adj Close,Volume đi. Các cột đó không cần thiết cho quá trình dự đoán.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
prices_dataset_dropout = prices_dataset.drop([&#39;Date&#39;,&#39;Adj Close&#39;,&#39;Volume&#39;], 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scale-dữ-liệu&#34;&gt;Scale dữ liệu&lt;/h3&gt;

&lt;p&gt;Khi sử dụng ANN, chúng ta thông thường sẽ scale dữ liệu input về đoạn [-1,1]. Trong python, thư viện sklearn đã hỗ trợ cho chúng ta sẵn các hàm scale dữ liệu cần thiết.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scale data
def normalize_data(df):
    min_max_scaler = MinMaxScaler()
    df[&#39;Open&#39;] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))
    df[&#39;High&#39;] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))
    df[&#39;Low&#39;] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))
    df[&#39;Close&#39;] = min_max_scaler.fit_transform(df.Close.values.reshape(-1,1))
    return df

prices_dataset_norm = normalize_data(prices_dataset_dropout)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;phân-chia-tập-train-và-test&#34;&gt;Phân chia tập train và test.&lt;/h3&gt;

&lt;p&gt;Chúng ta sẽ chia dữ liệu thành 2 phần với 80% là train và 20% còn lại là test. Chọn seq_len=20, các bạn có thể test với các seq len khác, và sau đó chuyển dữ liệu về dạng numpy array để dễ dàng thực hiện các phép chuyển đổi.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def generate_data(stock_ds, seq_len):
    data_raw = stock_ds.as_matrix()
    data = []
    
    # create all possible sequences of length seq_len
    for index in range(len(data_raw) - seq_len): 
        data.append(data_raw[index: index + seq_len])
    return data

#data as numpy array
def generate_train_test(data_ds,split_percent=0.8):
    print(len(data_ds))
    data = np.asarray(data_ds)
   
    data_size = len(data)
    train_end = int(np.floor(split_percent*data_size))
    
    x_train = data[:train_end,:-1,:]
    y_train = data[:train_end,-1,:]
    
 
    
    x_test = data[train_end:,:-1,:]
    y_test = data[train_end:,-1,:]
    
    return [x_train, y_train, x_test, y_test]



seq_len = 20 # choose sequence length

seq_prices_dataset = generate_data(prices_dataset_norm,seq_len)

x_train, y_train, x_test, y_test = generate_train_test(seq_prices_dataset, 0.8)

print(&#39;x_train.shape = &#39;,x_train.shape)
print(&#39;y_train.shape = &#39;, y_train.shape)
print(&#39;x_test.shape = &#39;, x_test.shape)
print(&#39;y_test.shape = &#39;,y_test.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; x_train.shape =  (13675, 19, 4)
y_train.shape =  (13675, 4)
x_test.shape =  (3419, 19, 4)
y_test.shape =  (3419, 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;xây-dựng-mô-hình-sử-dụng-keras&#34;&gt;Xây dựng mô hình sử dụng keras&lt;/h3&gt;

&lt;p&gt;Ở đây mình sử dụng keras xây dựng mô hình ANN. Mô hình của mình xây dựng gồm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Sequential()

model.add(LSTM(
    input_dim=4,
    output_dim=50,
    return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(
    100,
    return_sequences=False))
model.add(Dropout(0.2))

model.add(Dense(
    output_dim=4))
model.add(Activation(&#39;linear&#39;))



model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])
checkpoint = ModelCheckpoint(filepath=&#39;sp500_stockperdict.h5&#39;, verbose=1, save_best_only=True)
hist = model.fit(x_train, y_train, epochs=300, batch_size=128, verbose=1, callbacks=[checkpoint], validation_split=0.2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau một thời gian chạy, mình cũng thu được model. Các bạn quan tâm có thể download model của mình huấn luyện được tại &lt;a href=&#34;https://drive.google.com/open?id=1ImHQM9yWmOjpF5tjmSI9oqAi5BORa9Rs&#34;&gt;https://drive.google.com/open?id=1ImHQM9yWmOjpF5tjmSI9oqAi5BORa9Rs&lt;/a&gt; . Tiến hành plot dữ liệu tập test lên xem kết quả như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model =load_model(&#39;sp500_stockperdict.h5&#39;)


y_hat = model.predict(x_test)

ft = 3 # 0 = open, 1 = highest, 2 =lowest , 3 = close

plt.plot( y_test[:,ft], color=&#39;blue&#39;, label=&#39;target&#39;)

plt.plot( y_hat[:,ft], color=&#39;red&#39;, label=&#39;prediction&#39;)

plt.title(&#39;future stock prices&#39;)
plt.xlabel(&#39;time [days]&#39;)
plt.ylabel(&#39;normalized price&#39;)
plt.legend(loc=&#39;best&#39;)

plt.show()

from sklearn.metrics import mean_squared_error

# 0 = open, 1 = highest, 2 =lowest , 3 = close
print(&amp;quot;open error: &amp;quot;)
print(mean_squared_error(y_test[:,0], y_hat[ :,0]))

print(&amp;quot;highest error: &amp;quot;)
print(mean_squared_error(y_test[:,1], y_hat[ :,1]))

print(&amp;quot;lowest error: &amp;quot;)
print(mean_squared_error(y_test[:,2], y_hat[ :,2]))

print(&amp;quot;close error: &amp;quot;)
print(mean_squared_error(y_test[:,3], y_hat[ :,3]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sp500index_predict.png&#34; alt=&#34;hình chứng khoán&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;open error:
0.0009739211460315127
highest error:
0.0010539412808401607
lowest error:
0.0010066509540756113
close error:
0.0010840500965408758
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hiện đã có bản tensorflow 2 có tích hợp keras, mình update lại code&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
 from re import T
import numpy as np 
# linear algebra 
import pandas as pd 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler 
import tensorflow as tf
import joblib

import matplotlib
matplotlib.use(&#39;TkAgg&#39;)
import matplotlib.pyplot as plt 


file_name =&#39;GSPC.csv&#39;


prices_dataset =  pd.read_csv(file_name, header=0)


# prices_dataset_dropout = prices_dataset.drop([&#39;Date&#39;,&#39;Adj Close&#39;,&#39;Volume&#39;], 1)
prices_dataset_dropout=prices_dataset.reset_index()[&#39;Close&#39;]


scaler=MinMaxScaler(feature_range=(0,1))
prices_dataset_norm=scaler.fit_transform(np.array(prices_dataset_dropout).reshape(-1,1))
joblib.dump(scaler, &#39;scaler.alex&#39;)


print(prices_dataset_norm[:10])


def generate_data(stock_ds, seq_len,predict_next_t):
    dataX, dataY = [], []
    for i in range(len(stock_ds)-seq_len-1):
        dataX.append(stock_ds[i:(i+seq_len)])
        dataY.append(stock_ds[i + seq_len+predict_next_t])
    return np.array(dataX), np.array(dataY)

#data as numpy array
def generate_train_test(data_x,data_y,split_percent=0.8):
    
    train_end = int(np.floor(split_percent*data_x.shape[0]))
    
    x_train,x_test=data_x[:train_end,:],data_x[train_end:,:]
    y_train,y_test = data_y[:train_end],data_y[train_end:]
    return x_train,y_train,x_test,y_test



seq_len = 100 # choose sequence length
predict_next_t = 1 # 0 is next date, 1 is 2 next date

data_x, data_y = generate_data(prices_dataset_norm,seq_len,predict_next_t)

x_train,y_train,x_test,y_test = generate_train_test(data_x,data_y, 0.8)


x_train =x_train.reshape(x_train.shape[0],x_train.shape[1] , 1)
x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)
print(&#39;x_train.shape = &#39;,x_train.shape)
print(&#39;y_train.shape = &#39;, y_train.shape)
print(&#39;x_test.shape = &#39;, x_test.shape)
print(&#39;y_test.shape = &#39;,y_test.shape)



model = Sequential()

    # input_dim=4,
    # output_dim=50,
model.add(LSTM(units=100,input_shape=x_train.shape[1:],
    return_sequences=True))

model.add(LSTM(
    100,
    return_sequences=False))
model.add(Dense(1))


model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])
checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=&#39;my_model_stock.h5&#39;, verbose=1, save_best_only=True)
hist = model.fit(x_train, y_train, epochs=3, batch_size=64, verbose=1, callbacks=[checkpoint], validation_split=0.2)

from tensorflow.keras.models import load_model
print(&#39;load model&#39;)
model =load_model(&#39;my_model_stock.h5&#39;)

print(&#39;predict&#39;)
y_test = y_test.reshape(y_test.shape[0])
# train_predict=model.predict(x_train)
test_predict=model.predict(x_test)
print(&#39;invert&#39;)
print(y_test.shape)
# train_predict=scaler.inverse_transform(train_predict)

# scaler = joblib.load(&#39;scaler.alex&#39;)

y_hat=scaler.inverse_transform(test_predict)
y_test=scaler.inverse_transform(y_test.reshape(-1, 1))
print(y_hat.shape)
# y_hat = model.predict(x_test)
# import matplotlib
# matplotlib.use(&#39;GTKAgg&#39;) 
# print(&#39;plot&#39;)

plt.plot( y_test, color=&#39;blue&#39;, label=&#39;target&#39;)

plt.plot( y_hat, color=&#39;red&#39;, label=&#39;prediction&#39;)
print(&#39;plot complete&#39;)
plt.title(&#39;future stock prices&#39;)
plt.xlabel(&#39;time [days]&#39;)
plt.ylabel(&#39;normalized price&#39;)
plt.legend(loc=&#39;best&#39;)
print(&#39;plot show&#39;)
plt.savefig(&amp;quot;mygraph.png&amp;quot;)
plt.show()


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả của mô hình trông khá tốt, về hình dạng thì khá tương đồng với kết quả. Chúng ta có thể cải tiến model bằng cách nâng số lượng layer/ hidden node.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overview of gradient descent optimization algorithm</title>
      <link>/blog/2018-11-01-overview-of-gradient-descent-optimization-algorithm/</link>
      <pubDate>Mon, 29 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-11-01-overview-of-gradient-descent-optimization-algorithm/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Sau khi thực hiện bài phân loại chó mèo bằng keras, mình phát hiện rằng keras có hỗ trợ rất nhiều thuật toán tối ưu hoá &lt;a href=&#34;https://keras.io/optimizers/&#34;&gt;https://keras.io/optimizers/&lt;/a&gt;. Nhân dịp rãnh rỗi, mình sẽ tổng hợp lại một vài thuật toán mà keras hỗ trợ.&lt;/p&gt;

&lt;h2 id=&#34;dẫn-nhập&#34;&gt;Dẫn nhập&lt;/h2&gt;

&lt;p&gt;Tại thời điểm hiện tại, Gradient descent là một trong những thuật toán phổ biến được sử dụng để tối ưu hoá mạng neural networks. Các thư viện DNN sẽ implement kèm theo một vài biến thể của gradient descent giúp người dùng dễ dàng sử dụng công cụ hơn.&lt;/p&gt;

&lt;p&gt;Bài viết này mình sẽ cập nhật dần đến khi hoàn thiện.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân loại chó mèo sử dụng pretrain model</title>
      <link>/blog/2018-10-29-phan-loai-cho-meo/</link>
      <pubDate>Mon, 29 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-29-phan-loai-cho-meo/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Bài toán phân loại chó mèo là bài toán khá cũ tại thời điểm hiện tại. Tuy nhiên, đối với các bạn mới bước chân vào con đường machine learning thì đây là một trong những bài toán cơ bản để các bạn thực hành sử dụng và tìm hiểu thư viện mà mình đang có. Ở đây, chúng ta sẽ sử dụng pretrain model có sẵn của kares áp dụng trên tập dữ liệu. Các bạn có thể download tập dữ liệu train và test ở địa chỉ &lt;a href=&#34;https://www.kaggle.com/c/dogs-vs-cats/download/train.zip&#34;&gt;https://www.kaggle.com/c/dogs-vs-cats/download/train.zip&lt;/a&gt; và &lt;a href=&#34;https://www.kaggle.com/c/dogs-vs-cats/download/test1.zip&#34;&gt;https://www.kaggle.com/c/dogs-vs-cats/download/test1.zip&lt;/a&gt; để bắt đầu thực hiện.&lt;/p&gt;

&lt;h2 id=&#34;thực-hiện&#34;&gt;Thực hiện&lt;/h2&gt;

&lt;p&gt;Sau khi giải nén dữ liệu, ta thấy rằng thư mục train có cấu trúc đặt trên sẽ là label.số thứ tự.jpg. Trong đó label có thể là dog hoặc cat, số thứ tự tăng dần từ 0 đến &amp;hellip;. 12499. Để đảm bảo đúng với mô hình, ta phải cấu trúc lại dữ liệu thành dạng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir/classname1/*.*
data_dir/classname2/*.*
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vì vậy, ta tạo ra thư mục cat và copy những file bắt đầu bằng cat.* vào thư mục cat. Làm tương tự với thư mục dog.&lt;/p&gt;

&lt;p&gt;Đầu tiên, các bạn download file pretrain model, giải nén ra và để ở đâu đó trong ổ cứng của máy bạn. Đường dẫn file pretrain model các bạn có thể download ở &lt;a href=&#34;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&#34;&gt;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&lt;/a&gt;. Các bạn có thể download các file pretrain khác nếu có hứng thú tìm hiểu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load dataset lên và tranform nó để đưa vào huấn luyện.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import os
from collections import defaultdict
import numpy as np
import scipy.misc


def preprocess_input(x0):
    x = x0 / 255.
    x -= 0.5
    x *= 2.
    return x


def reverse_preprocess_input(x0):
    x = x0 / 2.0
    x += 0.5
    x *= 255.
    return x


def dataset(base_dir, n):
    print(&amp;quot;base dir: &amp;quot;+base_dir)
    print(&amp;quot;n: &amp;quot;+str(n))
    n = int(n)
    d = defaultdict(list)
    for root, subdirs, files in os.walk(base_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            assert file_path.startswith(base_dir)
            
            suffix = file_path[len(base_dir):]
            
            suffix = suffix.lstrip(&amp;quot;/&amp;quot;)
            suffix = suffix.lstrip(&amp;quot;\\&amp;quot;)
            if(suffix.find(&#39;/&#39;)&amp;gt;-1): #linux
                label = suffix.split(&amp;quot;/&amp;quot;)[0]
            else: #window
                label = suffix.split(&amp;quot;\\&amp;quot;)[0]
            d[label].append(file_path)
    print(&amp;quot;walk directory complete&amp;quot;)
    tags = sorted(d.keys())

    processed_image_count = 0
    useful_image_count = 0

    X = []
    y = []

    for class_index, class_name in enumerate(tags):
        filenames = d[class_name]
        for filename in filenames:
            processed_image_count += 1
            if processed_image_count%100 ==0:
                print(class_name+&amp;quot;\tprocess: &amp;quot;+str(processed_image_count)+&amp;quot;\t&amp;quot;+str(len(d[class_name])))
            img = scipy.misc.imread(filename)
            height, width, chan = img.shape
            assert chan == 3
            aspect_ratio = float(max((height, width))) / min((height, width))
            if aspect_ratio &amp;gt; 2:
                continue
            # We pick the largest center square.
            centery = height // 2
            centerx = width // 2
            radius = min((centerx, centery))
            img = img[centery-radius:centery+radius, centerx-radius:centerx+radius]
            img = scipy.misc.imresize(img, size=(n, n), interp=&#39;bilinear&#39;)
            X.append(img)
            y.append(class_index)
            useful_image_count += 1
    print(&amp;quot;processed %d, used %d&amp;quot; % (processed_image_count, useful_image_count))

    X = np.array(X).astype(np.float32)
    #X = X.transpose((0, 3, 1, 2))
    X = preprocess_input(X)
    y = np.array(y)

    perm = np.random.permutation(len(y))
    X = X[perm]
    y = y[perm]

    print(&amp;quot;classes:&amp;quot;,end=&amp;quot; &amp;quot;)
    for class_index, class_name in enumerate(tags):
        print(class_name, sum(y==class_index),end=&amp;quot; &amp;quot;)
    print(&amp;quot;X shape: &amp;quot;,X.shape)

    return X, y, tags
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đoạn code trên khá đơn giản và dễ hiểu. Lưu ý ở đây là với những bức ảnh có tỷ lệ width và height &amp;gt; 2 thì mình sẽ loại chúng ra khỏi tập dữ liệu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ xây dựng mô hình dựa trên mô hình InceptionV3 có sẵn, thêm một lớp softmax ở cuối để phân lớp dữ liệu, chúng ta sẽ huấn luyện lớp softmax này. Các lớp trước lớp softmax này sẽ bị đóng băng (không cập nhật trọng số trong quá trình huấn luyện ).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# create the base pre-trained model
def build_model(nb_classes):
    base_model = InceptionV3(weights=&#39;imagenet&#39;, include_top=False)

    # add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    # let&#39;s add a fully-connected layer
    x = Dense(1024, activation=&#39;relu&#39;)(x)
    # and a logistic layer
    predictions = Dense(nb_classes, activation=&#39;softmax&#39;)(x)

    # this is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)

    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional InceptionV3 layers
    for layer in base_model.layers:
        layer.trainable = False

    # compile the model (should be done *after* setting layers to non-trainable)
    print(&amp;quot;starting model compile&amp;quot;)
    compile(model)
    print(&amp;quot;model compile done&amp;quot;)
    return model
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visualize một chút xíu về kiến trúc inceptionV3 mình đang dùng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, None, None, 3 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, None, None, 3 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, None, None, 6 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, None, None, 6 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, None, None, 8 5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, None, None, 8 240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, None, None, 8 0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, None, None, 1 138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, None, None, 1 576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, None, None, 6 192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, None, None, 6 0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, None, None, 9 55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, None, None, 4 144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, None, None, 4 0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, None, None, 1 0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, None, None, 6 76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, None, None, 9 82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, None, None, 3 6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, None, None, 6 192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, None, None, 9 288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, None, None, 3 96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, None, None, 6 0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, None, None, 9 0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, None, None, 3 0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, None, None, 2 0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, None, None, 6 192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, None, None, 6 0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, None, None, 9 55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, None, None, 4 144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, None, None, 4 0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, None, None, 6 76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, None, None, 9 82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, None, None, 6 192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, None, None, 9 288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, None, None, 6 0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, None, None, 9 0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, None, None, 2 0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, None, None, 6 192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, None, None, 9 55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, None, None, 4 144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, None, None, 4 0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, None, None, 2 0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, None, None, 6 76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, None, None, 9 82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, None, None, 6 192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, None, None, 9 288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, None, None, 6 192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, None, None, 6 0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, None, None, 9 0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, None, None, 6 0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, None, None, 2 0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, None, None, 6 192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, None, None, 6 0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, None, None, 9 55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, None, None, 9 82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, None, None, 3 1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, None, None, 9 288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, None, None, 9 0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, None, None, 7 0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, None, None, 1 114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, None, None, 1 114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, None, None, 1 384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, None, None, 1 384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, None, None, 1 172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, None, None, 1 172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, None, None, 1 576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, None, None, 1 576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, None, None, 7 0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, None, None, 1 179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, None, None, 1 179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, None, None, 1 480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, None, None, 1 480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, None, None, 1 215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, None, None, 1 215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, None, None, 1 576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, None, None, 1 576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, None, None, 7 0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, None, None, 1 179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, None, None, 1 179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, None, None, 1 480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, None, None, 1 480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, None, None, 1 215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, None, None, 1 215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, None, None, 1 576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, None, None, 1 576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, None, None, 7 0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, None, None, 7 0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, None, None, 1 258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, None, None, 1 258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, None, None, 7 0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, None, None, 1 258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, None, None, 1 576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, None, None, 1 0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, None, None, 3 552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, None, None, 1 331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, None, None, 3 960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, None, None, 1 576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, None, None, 3 0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, None, None, 1 0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, None, None, 1 0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, None, None, 4 1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, None, None, 4 0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, None, None, 3 1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, None, None, 1 0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, None, None, 3 1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, None, None, 3 1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, None, None, 3 960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, None, None, 3 0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, None, None, 3 0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, None, None, 1 576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, None, None, 1 0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, None, None, 2 0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, None, None, 4 1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, None, None, 4 0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, None, None, 3 1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, None, None, 2 0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, None, None, 3 1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, None, None, 3 1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, None, None, 1 393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, None, None, 3 960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, None, None, 3 0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, None, None, 3 0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, None, None, 1 576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, None, None, 1 0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, None, None, 2 0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 2)            2050        dense_1[0][0]
==================================================================================================
Total params: 23,903,010
Trainable params: 2,100,226
Non-trainable params: 21,802,784
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phần train lại sẽ có khoảng hơn 2 triệu tham số, phần layter ở trước đó không train là khoảng 21 triệu tham số.&lt;/p&gt;

&lt;p&gt;Đồ hình của model (các bạn có thể download về rồi zoom bự lên để xem rõ hơn).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/inception_v2_model_311.png&#34; alt=&#34;Hình ảnh kết quả nhận dạng chó mèo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Chia tập dữ liệu ra thành 5 phần, 4 phần làm tập train, 1 phần làm tập validation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y, tags = dataset.dataset(data_directory, n)
nb_classes = len(tags)


sample_count = len(y)
train_size = sample_count * 4 // 5
X_train = X[:train_size]
y_train = y[:train_size]
Y_train = np_utils.to_categorical(y_train, nb_classes)
X_test  = X[train_size:]
y_test  = y[train_size:]
Y_test = np_utils.to_categorical(y_test, nb_classes)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để chống overfit, chúng ta sẽ thêm một số yếu tố như thực hiện các phép biến đổi affine trên ảnh gốc.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;datagen = ImageDataGenerator(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        zca_whitening=False,
        rotation_range=45,
        width_shift_range=0.25,
        height_shift_range=0.25,
        horizontal_flip=True,
        vertical_flip=False,
        zoom_range=0.5,
        channel_shift_range=0.5,
        fill_mode=&#39;nearest&#39;)
        
datagen.fit(X_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cuối cùng, chúng ta sẽ xây dựng mô hình và tiến hành huấn luyện, lưu mô hình. Quá trình này tốn hơi nhiều thời gian.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = net.build_model(nb_classes)
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&amp;quot;accuracy&amp;quot;])

# train the model on the new data for a few epochs

print(&amp;quot;training the newly added dense layers&amp;quot;)

samples_per_epoch = X_train.shape[0]//batch_size*batch_size
steps_per_epoch = samples_per_epoch//batch_size
validation_steps = X_test.shape[0]//batch_size*batch_size

model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),
            samples_per_epoch=samples_per_epoch,
            epochs=nb_epoch,
            steps_per_epoch = steps_per_epoch,
            validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size),
            validation_steps=validation_steps,
            )


net.save(model, tags, model_file_prefix)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Độ chính xác trên tập train.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_pred = model.predict(X_test, batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis=1)

accuracy = float(np.sum(y_test==y_pred)) / len(y_test)
print(&amp;quot;accuracy: &amp;quot;, accuracy)

confusion = np.zeros((nb_classes, nb_classes), dtype=np.int32)
for (predicted_index, actual_index, image) in zip(y_pred, y_test, X_test):
    confusion[predicted_index, actual_index] += 1

print(&amp;quot;rows are predicted classes, columns are actual classes&amp;quot;)
for predicted_index, predicted_tag in enumerate(tags):
    print(predicted_tag[:7], end=&#39;&#39;, flush=True)
    for actual_index, actual_tag in enumerate(tags):
        print(&amp;quot;\t%d&amp;quot; % confusion[predicted_index, actual_index], end=&#39;&#39;)
    print(&amp;quot;&amp;quot;, flush=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy:  0.9907213167661771
rows are predicted classes, columns are actual classes
cat     12238   106
dog     124     12320
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả đạt 0.99 trên tập train, khá tốt phải không các bạn.&lt;/p&gt;

&lt;p&gt;Các bạn có thể download mô hình mình đã huấn luyện ở &lt;a href=&#34;https://drive.google.com/open?id=1qQo8gj3KA6c1rPmJMVS_FZkVDcDmRgSf&#34;&gt;https://drive.google.com/open?id=1qQo8gj3KA6c1rPmJMVS_FZkVDcDmRgSf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thử show ra kết quả trên tập test xem như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_pred = model.predict(X_test, batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis=1)

lst_img = []

columns = 5
rows = 5
# fig,= plt.figure(rows)
for idx, val in enumerate(X_test):
    pred =y_pred[idx]
    label = &amp;quot;{}: {:.2f}%&amp;quot;.format(tags[pred], Y_pred[idx][pred] * 100)
    image = dataset.reverse_preprocess_input(val)
    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
    cv2.putText(image,label , (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,0.7, (255, 000, 0), 2)

    plt.subplot(rows,rows,idx+1)
    plt.imshow(image)
    plt.title(label)
    plt.axis(&#39;off&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/nhan_dang_cho_meo1.png&#34; alt=&#34;Hình ảnh kết quả nhận dạng chó mèo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả có một số hình mèo bị nhận nhầm là chó, và một số hình không phải mèo, không phải chó. Nhìn chung kết quả cũng không đến nỗi nào quá tệ.&lt;/p&gt;

&lt;h2 id=&#34;quậy-phá-mô-hình&#34;&gt;Quậy phá mô hình&lt;/h2&gt;

&lt;p&gt;Mô hình InceptionV3 chúng ta đang xài có tổng cộng 311 lớp, chúng ta sẽ tiến hành một số pha quậy phá mô hình xem kết quả như trả ra như thế nào&lt;/p&gt;

&lt;h3 id=&#34;quậy-phá-1-mở-đóng-băng-một-số-lớp-cuối-và-train-trên-chúng&#34;&gt;Quậy phá 1: Mở đóng băng một số lớp cuối và train trên chúng.&lt;/h3&gt;

&lt;p&gt;Nếu các bạn để ý kỹ, trong đoạn mã nguồn của mình có đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional InceptionV3 layers
    for layer in base_model.layers:
        layer.trainable = False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nghĩa là mình đóng băng toàn bộ 311 lớp, không cho nó train mà chỉ lấy kết quả của nó train lớp softmax cuối cùng. Bây giờ mình sẽ thử nghiệm với việc là để 299 lớp ban đầu vẫn đóng băng, và train lại toàn bộ các lớp còn lại (Các bạn đừng thắc mắc vì sao lại là 299 nha, do mình thích thôi).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for layer in model.layers[:299]:
    layer.trainable = False
for layer in model.layers[299:]:
    layer.trainable = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đồ hình của mô đồ khá giống ở trên, mình chỉ post lại kết quả của số param.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;==================================================================================================
Total params: 23,903,010
Trainable params: 2,493,954
Non-trainable params: 21,409,056
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Như vậy là có khoảng 2 triệu 5 tham số được huấn luyện lại&lt;/p&gt;

&lt;p&gt;Model của mình huấn luyện được các bạn có thể download ở &lt;a href=&#34;https://drive.google.com/open?id=1Ts18LICUAh6gcOnXcmuVr7PUG5IxpCdt&#34;&gt;https://drive.google.com/open?id=1Ts18LICUAh6gcOnXcmuVr7PUG5IxpCdt&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Kết quả đạt được:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy:  0.9834610730133119
rows are predicted classes, columns are actual classes
cat     2429    69
dog     13      2447
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/dog_cat_v1.png&#34; alt=&#34;Hình ảnh kết quả nhận dạng chó mèo đóng băng 299 lớp đầu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả 25 hình ngẫu nhiên cũng khá giống kết quả ở trước đó. Một số hình không có con vật bị nhận nhầm như hình còn mèo ở góc phải trên bị nhận nhầm là chó. Tuy nhiên, với chất lượng hình ảnh như thế này thì mình thấy kết quả như vậy là khá tuyệt vời.&lt;/p&gt;

&lt;h3 id=&#34;quậy-phá-2-chỉ-sử-dụng-72-lớp-đầu-tiên-của-inception&#34;&gt;Quậy phá 2: Chỉ sử dụng 72 lớp đầu tiên của inception.&lt;/h3&gt;

&lt;p&gt;Ở lần thí nghiệm này, mình sẽ chỉ sử dụng 72 lớp đầu tiên của inception để huấn luyện. Mình sẽ sửa lại một xíu ở hàm build model như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = base_model.layers[72].output
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một lưu ý nhỏ là do inception không có tính tuần tự giữa các lớp (các bạn có thể nhìn hình ở trên sẽ thấy rõ), nên index sẽ không phải là 72 như thông thường.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ thực hiện việc huấn luyện lại mô hình và kết quả là:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy:  0.5494150867285196
rows are predicted classes, columns are actual classes
cat     339     131
dog     2103    2385
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả khá tệ, lý do là mô hình các layer không theo sequence, mình lấy ngẫu nhiêu 72 lớp làm thông tin feature của các hình bị mất mát nhiều (ví dụ trường hợp layey 80 là tổng hợp thông tin của layter 79 + layter 4 + layer 48, mà mình chỉ lấy 72 layter đầu, nên sẽ mất đi phần đóng góp cực kỳ quan trọng của layter 4 và 48 ở lớp cao hơn).&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mask R-CNN trong bài toán nhận dạng và phân vùng đối tượng</title>
      <link>/blog/2018-10-08-mask-rnn/</link>
      <pubDate>Mon, 08 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-08-mask-rnn/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Phân vùng đối tượng là một bài toán khá phổ biến trong lĩnh vực computer vision. Trong open cv có hỗ trợ cho chúng ta một số hàm để phân vùng đối tượng rất dễ sử dụng. Đặc điểm chung của các hàm này là độ chính xác không được cao cho lắm. Ở bài viết này, chúng ta sẽ tìm hiểu cách sử dụng mô hình pretrain của DNN để phân vùng các đối tượng trong ảnh.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-pretrain-model&#34;&gt;Sử dụng pretrain model&lt;/h2&gt;

&lt;p&gt;Đầu tiên, các bạn download file pretrain model, giải nén ra và để ở đâu đó trong ổ cứng của máy bạn. Đường dẫn file pretrain model các bạn có thể download ở &lt;a href=&#34;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&#34;&gt;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&lt;/a&gt;. Các bạn có thể download các file pretrain khác nếu có hứng thú tìm hiểu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load mô hình lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import os
import sys
import tarfile
import tensorflow as tf

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image
import PIL.ImageDraw as ImageDraw
import PIL.ImageFont as ImageFont
import cv2

import pprint

import PIL.Image as Image
import PIL.ImageColor as ImageColor

# Model preparation


# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_CKPT = &#39;mask_rcnn_inception_v2_coco_2018_01_28&#39; + &#39;/frozen_inference_graph.pb&#39;

# List of the strings that is used to add correct label for each box.
#PATH_TO_LABELS = &#39;mscoco_label_map.pbtxt&#39;

NUM_CLASSES = 1


# categories

category_index = {1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;person&#39;},
# 3: {&#39;id&#39;: 3, &#39;name&#39;: &#39;car&#39;},
 }

detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, &#39;rb&#39;) as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây, mình chỉ demo detect người trong hình, nên mình chỉ để category_index chỉ là &amp;ldquo;person&amp;rdquo;. Thực tế, mô hình COCO hỗ trợ cho chúng ta nhận dạng 90 loại đối tượng khác nhau, các bạn có nhu cầu tìm hiểu thì thay bằng đoạn mã sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;category_index = {1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;person&#39;},
 2: {&#39;id&#39;: 2, &#39;name&#39;: &#39;bicycle&#39;},
 3: {&#39;id&#39;: 3, &#39;name&#39;: &#39;car&#39;},
 4: {&#39;id&#39;: 4, &#39;name&#39;: &#39;motorcycle&#39;},
 5: {&#39;id&#39;: 5, &#39;name&#39;: &#39;airplane&#39;},
 6: {&#39;id&#39;: 6, &#39;name&#39;: &#39;bus&#39;},
 7: {&#39;id&#39;: 7, &#39;name&#39;: &#39;train&#39;},
 8: {&#39;id&#39;: 8, &#39;name&#39;: &#39;truck&#39;},
 9: {&#39;id&#39;: 9, &#39;name&#39;: &#39;boat&#39;},
 10: {&#39;id&#39;: 10, &#39;name&#39;: &#39;traffic light&#39;},
 11: {&#39;id&#39;: 11, &#39;name&#39;: &#39;fire hydrant&#39;},
 13: {&#39;id&#39;: 13, &#39;name&#39;: &#39;stop sign&#39;},
 14: {&#39;id&#39;: 14, &#39;name&#39;: &#39;parking meter&#39;},
 15: {&#39;id&#39;: 15, &#39;name&#39;: &#39;bench&#39;},
 16: {&#39;id&#39;: 16, &#39;name&#39;: &#39;bird&#39;},
 17: {&#39;id&#39;: 17, &#39;name&#39;: &#39;cat&#39;},
 18: {&#39;id&#39;: 18, &#39;name&#39;: &#39;dog&#39;},
 19: {&#39;id&#39;: 19, &#39;name&#39;: &#39;horse&#39;},
 20: {&#39;id&#39;: 20, &#39;name&#39;: &#39;sheep&#39;},
 21: {&#39;id&#39;: 21, &#39;name&#39;: &#39;cow&#39;},
 22: {&#39;id&#39;: 22, &#39;name&#39;: &#39;elephant&#39;},
 23: {&#39;id&#39;: 23, &#39;name&#39;: &#39;bear&#39;},
 24: {&#39;id&#39;: 24, &#39;name&#39;: &#39;zebra&#39;},
 25: {&#39;id&#39;: 25, &#39;name&#39;: &#39;giraffe&#39;},
 27: {&#39;id&#39;: 27, &#39;name&#39;: &#39;backpack&#39;},
 28: {&#39;id&#39;: 28, &#39;name&#39;: &#39;umbrella&#39;},
 31: {&#39;id&#39;: 31, &#39;name&#39;: &#39;handbag&#39;},
 32: {&#39;id&#39;: 32, &#39;name&#39;: &#39;tie&#39;},
 33: {&#39;id&#39;: 33, &#39;name&#39;: &#39;suitcase&#39;},
 34: {&#39;id&#39;: 34, &#39;name&#39;: &#39;frisbee&#39;},
 35: {&#39;id&#39;: 35, &#39;name&#39;: &#39;skis&#39;},
 36: {&#39;id&#39;: 36, &#39;name&#39;: &#39;snowboard&#39;},
 37: {&#39;id&#39;: 37, &#39;name&#39;: &#39;sports ball&#39;},
 38: {&#39;id&#39;: 38, &#39;name&#39;: &#39;kite&#39;},
 39: {&#39;id&#39;: 39, &#39;name&#39;: &#39;baseball bat&#39;},
 40: {&#39;id&#39;: 40, &#39;name&#39;: &#39;baseball glove&#39;},
 41: {&#39;id&#39;: 41, &#39;name&#39;: &#39;skateboard&#39;},
 42: {&#39;id&#39;: 42, &#39;name&#39;: &#39;surfboard&#39;},
 43: {&#39;id&#39;: 43, &#39;name&#39;: &#39;tennis racket&#39;},
 44: {&#39;id&#39;: 44, &#39;name&#39;: &#39;bottle&#39;},
 46: {&#39;id&#39;: 46, &#39;name&#39;: &#39;wine glass&#39;},
 47: {&#39;id&#39;: 47, &#39;name&#39;: &#39;cup&#39;},
 48: {&#39;id&#39;: 48, &#39;name&#39;: &#39;fork&#39;},
 49: {&#39;id&#39;: 49, &#39;name&#39;: &#39;knife&#39;},
 50: {&#39;id&#39;: 50, &#39;name&#39;: &#39;spoon&#39;},
 51: {&#39;id&#39;: 51, &#39;name&#39;: &#39;bowl&#39;},
 52: {&#39;id&#39;: 52, &#39;name&#39;: &#39;banana&#39;},
 53: {&#39;id&#39;: 53, &#39;name&#39;: &#39;apple&#39;},
 54: {&#39;id&#39;: 54, &#39;name&#39;: &#39;sandwich&#39;},
 55: {&#39;id&#39;: 55, &#39;name&#39;: &#39;orange&#39;},
 56: {&#39;id&#39;: 56, &#39;name&#39;: &#39;broccoli&#39;},
 57: {&#39;id&#39;: 57, &#39;name&#39;: &#39;carrot&#39;},
 58: {&#39;id&#39;: 58, &#39;name&#39;: &#39;hot dog&#39;},
 59: {&#39;id&#39;: 59, &#39;name&#39;: &#39;pizza&#39;},
 60: {&#39;id&#39;: 60, &#39;name&#39;: &#39;donut&#39;},
 61: {&#39;id&#39;: 61, &#39;name&#39;: &#39;cake&#39;},
 62: {&#39;id&#39;: 62, &#39;name&#39;: &#39;chair&#39;},
 63: {&#39;id&#39;: 63, &#39;name&#39;: &#39;couch&#39;},
 64: {&#39;id&#39;: 64, &#39;name&#39;: &#39;potted plant&#39;},
 65: {&#39;id&#39;: 65, &#39;name&#39;: &#39;bed&#39;},
 67: {&#39;id&#39;: 67, &#39;name&#39;: &#39;dining table&#39;},
 70: {&#39;id&#39;: 70, &#39;name&#39;: &#39;toilet&#39;},
 72: {&#39;id&#39;: 72, &#39;name&#39;: &#39;tv&#39;},
 73: {&#39;id&#39;: 73, &#39;name&#39;: &#39;laptop&#39;},
 74: {&#39;id&#39;: 74, &#39;name&#39;: &#39;mouse&#39;},
 75: {&#39;id&#39;: 75, &#39;name&#39;: &#39;remote&#39;},
 76: {&#39;id&#39;: 76, &#39;name&#39;: &#39;keyboard&#39;},
 77: {&#39;id&#39;: 77, &#39;name&#39;: &#39;cell phone&#39;},
 78: {&#39;id&#39;: 78, &#39;name&#39;: &#39;microwave&#39;},
 79: {&#39;id&#39;: 79, &#39;name&#39;: &#39;oven&#39;},
 80: {&#39;id&#39;: 80, &#39;name&#39;: &#39;toaster&#39;},
 81: {&#39;id&#39;: 81, &#39;name&#39;: &#39;sink&#39;},
 82: {&#39;id&#39;: 82, &#39;name&#39;: &#39;refrigerator&#39;},
 84: {&#39;id&#39;: 84, &#39;name&#39;: &#39;book&#39;},
 85: {&#39;id&#39;: 85, &#39;name&#39;: &#39;clock&#39;},
 86: {&#39;id&#39;: 86, &#39;name&#39;: &#39;vase&#39;},
 87: {&#39;id&#39;: 87, &#39;name&#39;: &#39;scissors&#39;},
 88: {&#39;id&#39;: 88, &#39;name&#39;: &#39;teddy bear&#39;},
 89: {&#39;id&#39;: 89, &#39;name&#39;: &#39;hair drier&#39;},
 90: {&#39;id&#39;: 90, &#39;name&#39;: &#39;toothbrush&#39;}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load một số hàm giúp hỗ trợ việc hậu xử lý ảnh để vẽ các mask cho chúng ta xem trực quan hơn.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
    draw  =  ImageDraw.Draw(image)
    im_width,  im_height  =  image.size
    if  use_normalized_coordinates:
        (left,  right,  top,  bottom)  =  (xmin  *  im_width,  xmax  *  im_width,
                                                                    ymin  *  im_height,  ymax  *  im_height)
    else:
        (left,  right,  top,  bottom)  =  (xmin,  xmax,  ymin,  ymax)
    draw.line([(left,  top),  (left,  bottom),  (right,  bottom),
                          (right,  top),  (left,  top)],  width=thickness,  fill=color)
    try:
        font  =  ImageFont.truetype(&#39;arial.ttf&#39;,  24)
    except  IOError:
        font  =  ImageFont.load_default()

    #  If  the  total  height  of  the  display  strings  added  to  the  top  of  the  bounding
    #  box  exceeds  the  top  of  the  image,  stack  the  strings  below  the  bounding  box
    #  instead  of  above.
    display_str_heights  =  [font.getsize(ds)[1]  for  ds  in  display_str_list]
    #  Each  display_str  has  a  top  and  bottom  margin  of  0.05x.
    total_display_str_height  =  (1  +  2  *  0.05)  *  sum(display_str_heights)

    if  top  &amp;gt;  total_display_str_height:
        text_bottom  =  top
    else:
        text_bottom  =  bottom  +  total_display_str_height
    #  Reverse  list  and  print  from  bottom  to  top.
    for  display_str  in  display_str_list[::-1]:
        text_width,  text_height  =  font.getsize(display_str)
        margin  =  np.ceil(0.05  *  text_height)
        draw.rectangle(
                [(left,  text_bottom  -  text_height  -  2  *  margin),  (left  +  text_width,
                                                                                                                    text_bottom)],
                fill=color)
        draw.text(
                (left  +  margin,  text_bottom  -  text_height  -  margin),
                display_str,
                fill=&#39;black&#39;,
                font=font)
        text_bottom  -=  text_height  -  2  *  margin



def visualize_boxes_and_labels_on_image_array(
        image,
        boxes,
        classes,
        scores,
        category_index,
        instance_masks=None,
        instance_boundaries=None,
        keypoints=None,
        use_normalized_coordinates=False,
        max_boxes_to_draw=20,
        min_score_thresh=.5,
        agnostic_mode=False,
        line_thickness=4,
        groundtruth_box_visualization_color=&#39;black&#39;,
        skip_scores=False,
        skip_labels=False):

    box_to_display_str_map = collections.defaultdict(list)
    box_to_color_map = collections.defaultdict(str)
    box_to_instance_masks_map = {}
    box_to_instance_boundaries_map = {}
    box_to_keypoints_map = collections.defaultdict(list)
    if not max_boxes_to_draw:
        max_boxes_to_draw = boxes.shape[0]
    #print(boxes)
    for i in range(min(max_boxes_to_draw, boxes.shape[0])):
        if scores is None or scores[i] &amp;gt; min_score_thresh:
            box = tuple(boxes[i].tolist())
        if instance_masks is not None:
            box_to_instance_masks_map[box] = instance_masks[i]
        if instance_boundaries is not None:
            box_to_instance_boundaries_map[box] = instance_boundaries[i]
        if keypoints is not None:
            box_to_keypoints_map[box].extend(keypoints[i])
        if scores is None:
            box_to_color_map[box] = groundtruth_box_visualization_color
        else:
            display_str = &#39;&#39;
            if not skip_labels:
                if not agnostic_mode:
                    if classes[i] in category_index.keys():
                        class_name = category_index[classes[i]][&#39;name&#39;]
                    else:
                        class_name = &#39;N/A&#39;
                    display_str = str(class_name)
            if not skip_scores:
                if not display_str:
                    display_str = &#39;{}%&#39;.format(int(100 * scores[i]))
                else:
                    display_str = &#39;{}: {}%&#39;.format(
                        display_str, int(100 * scores[i]))
            box_to_display_str_map[box].append(display_str)
            if agnostic_mode:
                box_to_color_map[box] = &#39;DarkOrange&#39;
            else:
                box_to_color_map[box] = STANDARD_COLORS[classes[i] %
                                                        len(STANDARD_COLORS)]

    # Draw all boxes onto image.
    for box, color in box_to_color_map.items():
        ymin, xmin, ymax, xmax = box
        if instance_masks is not None:
            draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color)

        draw_bounding_box_on_image_array(
        image,
        ymin,
        xmin,
        ymax,
        xmax,
        color=color,
        thickness=line_thickness,
        display_str_list=box_to_display_str_map[box],
        use_normalized_coordinates=use_normalized_coordinates)

    return image


def reframe_box_masks_to_image_masks(box_masks,  boxes,  image_height,
                                     image_width):
    &amp;quot;&amp;quot;&amp;quot;Transforms  the  box  masks  back  to  full  image  masks.

    Embeds  masks  in  bounding  boxes  of  larger  masks  whose  shapes  correspond  to
    image  shape.

    Args:
        box_masks:  A  tf.float32  tensor  of  size  [num_masks,  mask_height,  mask_width].
        boxes:  A  tf.float32  tensor  of  size  [num_masks,  4]  containing  the  box
                      corners.  Row  i  contains  [ymin,  xmin,  ymax,  xmax]  of  the  box
                      corresponding  to  mask  i.  Note  that  the  box  corners  are  in
                      normalized  coordinates.
        image_height:  Image  height.  The  output  mask  will  have  the  same  height  as
                                    the  image  height.
        image_width:  Image  width.  The  output  mask  will  have  the  same  width  as  the
                                  image  width.

    Returns:
        A  tf.float32  tensor  of  size  [num_masks,  image_height,  image_width].
    &amp;quot;&amp;quot;&amp;quot;
    #  TODO(rathodv):  Make  this  a  public  function.
    def reframe_box_masks_to_image_masks_default():
        &amp;quot;&amp;quot;&amp;quot;The  default  function  when  there  are  more  than  0  box  masks.&amp;quot;&amp;quot;&amp;quot;
        def transform_boxes_relative_to_boxes(boxes,  reference_boxes):
            boxes = tf.reshape(boxes,  [-1,  2,  2])
            min_corner = tf.expand_dims(reference_boxes[:,  0:2],  1)
            max_corner = tf.expand_dims(reference_boxes[:,  2:4],  1)
            transformed_boxes = (boxes - min_corner) / \
                (max_corner - min_corner)
            return tf.reshape(transformed_boxes,  [-1,  4])

        box_masks_expanded = tf.expand_dims(box_masks,  axis=3)
        num_boxes = tf.shape(box_masks_expanded)[0]
        unit_boxes = tf.concat(
            [tf.zeros([num_boxes,  2]),  tf.ones([num_boxes,  2])],  axis=1)
        reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes,  boxes)
        return tf.image.crop_and_resize(
            image=box_masks_expanded,
            boxes=reverse_boxes,
            box_ind=tf.range(num_boxes),
            crop_size=[image_height,  image_width],
            extrapolation_value=0.0)
    image_masks = tf.cond(
        tf.shape(box_masks)[0] &amp;gt; 0,
        reframe_box_masks_to_image_masks_default,
        lambda:  tf.zeros([0,  image_height,  image_width,  1],  dtype=tf.float32))
    return tf.squeeze(image_masks,  axis=3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cho hình ảnh vào và rút ra kết quả.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def detect_frame(image_np, sess, detection_graph):

    with detection_graph.as_default():

        ops = tf.get_default_graph().get_operations()
        all_tensor_names = {output.name for op in ops for output in op.outputs}
        tensor_dict = {}
        for key in [
            &#39;num_detections&#39;, &#39;detection_boxes&#39;, &#39;detection_scores&#39;,
            &#39;detection_classes&#39;, &#39;detection_masks&#39;
        ]:
            tensor_name = key + &#39;:0&#39;
            if tensor_name in all_tensor_names:
                tensor_dict[key] = tf.get_default_graph(
                ).get_tensor_by_name(tensor_name)
        if &#39;detection_masks&#39; in tensor_dict:
            # The following processing is only for single image
            detection_boxes = tf.squeeze(tensor_dict[&#39;detection_boxes&#39;], [0])
            detection_masks = tf.squeeze(tensor_dict[&#39;detection_masks&#39;], [0])
            # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
            real_num_detection = tf.cast(
                tensor_dict[&#39;num_detections&#39;][0], tf.int32)
           
            detection_boxes = tf.slice(detection_boxes, [0, 0], [
                                       real_num_detection, -1])
            detection_masks = tf.slice(detection_masks, [0, 0, 0], [
                                       real_num_detection, -1, -1])
            detection_masks_reframed = reframe_box_masks_to_image_masks(
                detection_masks, detection_boxes, image_np.shape[0], image_np.shape[1])
            detection_masks_reframed = tf.cast(
                tf.greater(detection_masks_reframed, 0.5), tf.uint8)
            # Follow the convention by adding back the batch dimension
            tensor_dict[&#39;detection_masks&#39;] = tf.expand_dims(
                detection_masks_reframed, 0)
        image_tensor = tf.get_default_graph().get_tensor_by_name(&#39;image_tensor:0&#39;)

      # Run inference
        output_dict = sess.run(tensor_dict,
                               feed_dict={image_tensor: np.expand_dims(image_np, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
        output_dict[&#39;num_detections&#39;] = int(output_dict[&#39;num_detections&#39;][0])
        #print(&amp;quot;num detect &amp;quot;+str(output_dict[&#39;num_detections&#39;]))
        output_dict[&#39;detection_classes&#39;] = output_dict[&#39;detection_classes&#39;][0].astype(
            np.uint8)
        output_dict[&#39;detection_boxes&#39;] = output_dict[&#39;detection_boxes&#39;][0]
        output_dict[&#39;detection_scores&#39;] = output_dict[&#39;detection_scores&#39;][0]
        if &#39;detection_masks&#39; in output_dict:
            output_dict[&#39;detection_masks&#39;] = output_dict[&#39;detection_masks&#39;][0]

        visualize_boxes_and_labels_on_image_array(
            image_np,
            output_dict[&#39;detection_boxes&#39;],
            output_dict[&#39;detection_classes&#39;],
            output_dict[&#39;detection_scores&#39;],
            category_index,
            instance_masks=output_dict.get(&#39;detection_masks&#39;),
            use_normalized_coordinates=True,
            line_thickness=1,
            max_boxes_to_draw=min(output_dict[&#39;num_detections&#39;],20)
            )

    return image_np
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image = cv2.imread(&#39;img2.jpg&#39;)
with detection_graph.as_default():
    with tf.Session(graph=detection_graph) as sess:
        image_np = detect_frame(image, sess, detection_graph)

cv2.imwrite(&#39;output.jpg&#39;, image)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả file output.jpg của chúng ta là:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mask_rnn_output_mieule.jpg&#34; alt=&#34;Phân vùng của mark ca sĩ midu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thử với bức ảnh người và xe hơi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mask_rnn_output_nguoidep_xehoi.jpg&#34; alt=&#34;Phân vùng của người và xe hơi&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning based Multiple Human Pose Estimation using OpenCV</title>
      <link>/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/</link>
      <pubDate>Fri, 05 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Lưu ý: Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv &amp;gt; 3.4.1.&lt;/p&gt;

&lt;p&gt;Ở bài viết trước, chúng ta đã tìm hiểu cách thức rút trích khung xương sử dụng DNN và đã áp dụng thành công trên ảnh có chứa 1 đối tượng người. Trong bài viết này, chúng ta sẽ thực hiện áp dụng mô hình cho bài toán có nhiều người trong cùng 1 bức ảnh.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-pretrain-model-trong-bài-toán-multiple-pose-estimation&#34;&gt;Sử dụng pretrain model trong bài toán multiple Pose Estimation&lt;/h2&gt;

&lt;p&gt;Trong bài viết này, chúng ta tiếp tục sử dụng mô hình MPI để dò tìm các điểm đặc trưng của con người và rút ra mô hình khung xương. Kết quả trả về của thuật toán gồm 15 đặc trưng như bên dưới.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
Left Ankle – 13, Chest – 14, Background – 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Áp dụng mô hình với ảnh của nhóm T-ARA.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2

nPoints = 15
POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]

protoFile = &amp;quot;pose/mpi/pose_deploy_linevec.prototxt&amp;quot;
weightsFile = &amp;quot;pose/mpi/pose_iter_160000.caffemodel&amp;quot;

net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

frame = cv2.imread(&amp;quot;tara1.jpg&amp;quot;)

inWidth = 368
inHeight = 368
 
# Prepare the frame to be fed to the network
inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
 
# Set the prepared object as the input blob of the network
net.setInput(inpBlob)

output = net.forward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử show lên vị trí vùng cổ trong hình.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
i = 0
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_head_heatmap.png&#34; alt=&#34;Hình với điểm đặc trưng vùng đầu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thử show lên hình điểm đặc trưng vùng cổ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 1
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_neck_heatmap.png&#34; alt=&#34;Hình với điểm đặc trưng vùng cổ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bằng một số phép biến đổi quen thuộc có sẵn trong opencv, chúng ta hoàn toàn có thể lấy được toạ độ của các điểm keypoint một cách dễ dàng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Find the Keypoints using Non Maximum Suppression on the Confidence Map
def getKeypoints(probMap, threshold=0.1):
    
    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)

    mapMask = np.uint8(mapSmooth&amp;gt;threshold)
    keypoints = []
    
    #find the blobs
    _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    #for each blob find the maxima
    for cnt in contours:
        blobMask = np.zeros(mapMask.shape)
        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)
        maskedProbMap = mapSmooth * blobMask
        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)
        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))

    return keypoints


detected_keypoints = []
keypoints_list = np.zeros((0,3))
keypoint_id = 0
threshold = 0.1
for i in range(nPoints):
    probMap = output[0, i, :, :]
    probMap = cv2.resize(probMap, (frameWidth, frameHeight))

    keypoints = getKeypoints(probMap, threshold)
    keypoints_with_id = []
    for j in range(len(keypoints)):
        keypoints_with_id.append(keypoints[j] + (keypoint_id,))
        keypoints_list = np.vstack([keypoints_list, keypoints[j]])
        keypoint_id += 1

    detected_keypoints.append(keypoints_with_id)



frameClone = cv2.cvtColor(frameCopy,cv2.COLOR_BGR2RGB)
for i in range(nPoints):
    for j in range(len(detected_keypoints[i])):
        cv2.circle(frameClone, detected_keypoints[i][j][0:2], 3, [0,0,255], -1, cv2.LINE_AA)

plt.imshow(frameClone) 
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_keypoint.png&#34; alt=&#34;Hình toàn bộ điểm đặc trưng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cuối cùng, chúng ta sẽ nối các điểm đặc trưng của các nhân vật thông qua thuật toán Part Affinity Heatmaps. Thuật toán này được đề xuất bởi nhóm tác giả Zhe Cao, Tomas Simon,Shih-En Wei, Yaser Sheikh thuộc phòng thí nghiệm The Robotics Institute trường đại học Carnegie Mellon. Các bạn có nhu cầu có thể tìm hiểu ở &lt;a href=&#34;https://arxiv.org/pdf/1611.08050.pdf&#34;&gt;https://arxiv.org/pdf/1611.08050.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
mapIdx = [[16,17], [18,19], [20,21], [22,23], [24,25], [26,27], [28,29], [30,31], [32,33], [34,35], [36,37], [38,39], [40,41], [42,43]]



colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],
         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],
         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]
# Find valid connections between the different joints of a all persons present
def getValidPairs(output):
    valid_pairs = []
    invalid_pairs = []
    n_interp_samples = 10
    paf_score_th = 0.1
    conf_th = 0.5
    # loop for every POSE_PAIR
    for k in range(len(mapIdx)):
        # A-&amp;gt;B constitute a limb
        pafA = output[0, mapIdx[k][0], :, :]
        pafB = output[0, mapIdx[k][1], :, :]
        pafA = cv2.resize(pafA, (frameWidth, frameHeight))
        pafB = cv2.resize(pafB, (frameWidth, frameHeight))


        # Find the keypoints for the first and second limb
        candA = detected_keypoints[POSE_PAIRS[k][0]]
        candB = detected_keypoints[POSE_PAIRS[k][1]]
        nA = len(candA)
        nB = len(candB)

        # fig=plt.figure(figsize=(8, 8))

        # interp_coord = list(zip(np.linspace(candA[0][0], candB[0][0], num=n_interp_samples),
        #                                     np.linspace(candA[0][1], candB[0][1], num=n_interp_samples)))

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 1)
        
        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)


        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafA, alpha=0.5)

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 2)
        

        

        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)
        
        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafB, alpha=0.5)
        # plt.show()



        

        # If keypoints for the joint-pair is detected
        # check every joint in candA with every joint in candB 
        # Calculate the distance vector between the two joints
        # Find the PAF values at a set of interpolated points between the joints
        # Use the above formula to compute a score to mark the connection valid
        
        if( nA != 0 and nB != 0):
            valid_pair = np.zeros((0,3))
            for i in range(nA):
                max_j=-1
                maxScore = -1
                found = 0
                for j in range(nB):
                    # Find d_ij
                    d_ij = np.subtract(candB[j][:2], candA[i][:2])
                    norm = np.linalg.norm(d_ij)
                    if norm:
                        d_ij = d_ij / norm
                    else:
                        continue
                    # Find p(u)
                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),
                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))
                    # Find L(p(u))
                    paf_interp = []
                    for k in range(len(interp_coord)):
                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],
                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) 
                    # Find E
                    paf_scores = np.dot(paf_interp, d_ij)
                    avg_paf_score = sum(paf_scores)/len(paf_scores)
                    
                    # Check if the connection is valid
                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -&amp;gt; Valid Pair  
                    if ( len(np.where(paf_scores &amp;gt; paf_score_th)[0]) / n_interp_samples ) &amp;gt; conf_th :
                        if avg_paf_score &amp;gt; maxScore:
                            max_j = j
                            maxScore = avg_paf_score
                            found = 1
                # Append the connection to the list
                if found:            
                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)

            # Append the detected connections to the global list
            valid_pairs.append(valid_pair)

            pprint(valid_pair)
        else: # If no keypoints are detected
            print(&amp;quot;No Connection : k = {}&amp;quot;.format(k))
            invalid_pairs.append(k)
            valid_pairs.append([])
    pprint(valid_pairs)
    return valid_pairs, invalid_pairs

# This function creates a list of keypoints belonging to each person
# For each detected valid pair, it assigns the joint(s) to a person
# It finds the person and index at which the joint should be added. This can be done since we have an id for each joint
def getPersonwiseKeypoints(valid_pairs, invalid_pairs):
    # the last number in each row is the overall score 
    personwiseKeypoints = -1 * np.ones((0, 19))

    for k in range(len(mapIdx)):
        if k not in invalid_pairs:
            partAs = valid_pairs[k][:,0]
            partBs = valid_pairs[k][:,1]
            indexA, indexB = np.array(POSE_PAIRS[k])

            for i in range(len(valid_pairs[k])): 
                found = 0
                person_idx = -1
                for j in range(len(personwiseKeypoints)):
                    if personwiseKeypoints[j][indexA] == partAs[i]:
                        person_idx = j
                        found = 1
                        break

                if found:
                    personwiseKeypoints[person_idx][indexB] = partBs[i]
                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]

                # if find no partA in the subset, create a new subset
                elif not found and k &amp;lt; 17:
                    row = -1 * np.ones(19)
                    row[indexA] = partAs[i]
                    row[indexB] = partBs[i]
                    # add the keypoint_scores for the two keypoints and the paf_score 
                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]
                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])
    return personwiseKeypoints

valid_pairs, invalid_pairs = getValidPairs(output)

personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)


for i in range(nPoints-1):
    for n in range(len(personwiseKeypoints)):
       
        index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]
        if -1 in index:
            continue
        B = np.int32(keypoints_list[index.astype(int), 0])
        A = np.int32(keypoints_list[index.astype(int), 1])
        cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)
       
        
        
plt.imshow(frameClone)
    # plt.imshow(mapMask, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_t-ara_finalresult.png&#34; alt=&#34;Hình kết quả cuối cùng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hẹn gặp lại các bạn ở những bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết này được viết dựa vào nguồn &lt;a href=&#34;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&#34;&gt;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&lt;/a&gt; của tác giả VIKAS GUPTA. Tôi sử dụng tập model và hình ảnh khác với bài viết nguyên gốc của tác giả.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning based Human Pose Estimation using OpenCV</title>
      <link>/blog/2018-10-04-deep-learning-base-human-pose-estimation/</link>
      <pubDate>Thu, 04 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-04-deep-learning-base-human-pose-estimation/</guid>
      <description>

&lt;h3 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h3&gt;

&lt;p&gt;Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv &amp;gt; 3.4.1.&lt;/p&gt;

&lt;h2 id=&#34;pose-estimation-là-gì&#34;&gt;Pose Estimation là gì?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=ohX-wkLYhdM&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/ohX-wkLYhdM/0.jpg&#34; alt=&#34;POST ESTIMATION EXAMPLE - Make by Phạm Duy Tùng&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Post Estimation ( đôi khi được dùng với thuật ngữ Keypoint Detection) là một vấn đề khá phổ biến trong lĩnh vực xử lý ảnh khi chúng ta cần xác định vị trí và hướng của một đối tượng. Mức ý nghĩa ở đây là chúng ta phải rút ra được những đặc điểm chính, những đặc điểm đó là những đặc trưng của đối tượng ( có thể mô tả được đối tượng).&lt;/p&gt;

&lt;p&gt;Ví dụ, trong bài toán face pose estimation ( có tên khác là facial landmark detection), chúng ta cần xác định được đâu là vị trí của những điểm landmark trên khuôn mặt người.&lt;/p&gt;

&lt;p&gt;Một bài toán có liên quan đến bài toán trên là head pose estimation. Chúng ta cần xác định những điểm landmark để mô hình hoá lại được mô hình 3D của đầu người.&lt;/p&gt;

&lt;p&gt;Ở trong bài viết này, chúng ta đề cập đến bài toán human pose estimation, công việc chính là xác định và chỉ ra được một phần/ toàn bộ các phần chính của cơ thể con người (vd vai, khuỷu tay, cổ tay, đầu gối v.v).&lt;/p&gt;

&lt;p&gt;Trong bài viết này, chúng ta sẽ sử dụng mô hình được huấn luyện sẵn để chỉ ra các phần chính của cơ thể con người. Kết quả cơ bản của phần nhận diện này sẽ gần giống như hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/midu_pose_estimation.png&#34; alt=&#34;Hình ảnh rút trích những thành phần quan trọng trên cơ thể con người&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-pretrain-model-trong-bài-toán-pose-estimation&#34;&gt;Sử dụng pretrain model trong bài toán Pose Estimation&lt;/h2&gt;

&lt;p&gt;Vào nằm 2016, 2017, Phòng thí nghiệm Perceptual Computing của trường đại học Carnegie Mellon University đã công bố một bài báo có liên quan đến chủ đề Multi-Person Pose Estimation. Và đến nay, họ đã công bố mô hình huấn luyện cho chúng ta sử dụng. Các bạn có nhu cầu tìm hiểu sâu hơn có thể đọc kỹ nguồn dữ liệu của họ công bố ở link &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34;&gt;https://github.com/CMU-Perceptual-Computing-Lab/openpose&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trong bài post này, mình sẽ không đề cập kỹ đến phần kiến trúc mạng neural net họ sử dụng bên dưới, thay vào đó, mình sẽ tập trung hơn vào cách thức sử dụng mô hình để thu được kết quả cần thiết.&lt;/p&gt;

&lt;p&gt;Trước khi bắt đầu vào thực hành, mình sẽ mô tả một chút về mô hình pretrain có sẵn. Ở đây, họ cung cấp cho chúng ta 2 mô hình là MPII model và COCO  model. Đó chính là tên của hai bộ database mà họ sử dụng để đào tạo mô hình. Kết quả trả về của mỗ bộ database là khác nhau hoàn toàn.&lt;/p&gt;

&lt;p&gt;Với bộ COCO dataset, kết quả trả về là 18 đặc trưng gồm các thông tin:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Nose – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
LAnkle – 13, Right Eye – 14, Left Eye – 15, Right Ear – 16,
Left Ear – 17, Background – 18
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với bộ MPII, kết quả trả về là 15 đặc trưng gồm các thông tin:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
Left Ankle – 13, Chest – 14, Background – 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong phần này, chúng ta sẽ tập trung vào mô hình MPII, mô hình COCO sử dụng tương tự, chỉ việc thay lại đường dẫn file mô hình là được.&lt;/p&gt;

&lt;h2 id=&#34;bắt-đầu-code&#34;&gt;Bắt đầu code.&lt;/h2&gt;

&lt;p&gt;Bước 1: Download mô hình.&lt;/p&gt;

&lt;p&gt;Nhóm tác giả sử dụng caffe để huấn luyện mô hình, do đó, để sử dụng được, chúng ta cần download file mô hình ở đường dẫn &lt;a href=&#34;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel&#34;&gt;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel&lt;/a&gt; và file cấu hình ở đường dẫn &lt;a href=&#34;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_deploy_linevec.prototxt&#34;&gt;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_deploy_linevec.prototxt&lt;/a&gt;. Các bạn có thể để đâu đó tuỳ thích, ở đây tôi để trong thư mục pose/mpi để dễ dàng nhận biết với các mô hình khác.&lt;/p&gt;

&lt;p&gt;Bước 2: Load mô hình.&lt;/p&gt;

&lt;p&gt;Để load mô hình lên bộ nhớ chính, đơn giản là chúng ta thực hiện câu lệnh sau trong python&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
# Specify the paths for the 2 files
protoFile = &amp;quot;pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt&amp;quot;
weightsFile = &amp;quot;pose/mpi/pose_iter_160000.caffemodel&amp;quot;
 
# Read the network into Memory
net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đơn giản quá phải không các bạn :).&lt;/p&gt;

&lt;p&gt;Bước 3: Đọc ảnh và đưa ảnh vào trong mô hình.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Read image
frame = cv2.imread(&amp;quot;img2.jpg&amp;quot;)

frameCopy = np.copy(frame)
frameWidth = frame.shape[1]
frameHeight = frame.shape[0]
t = time.time()
# Specify the input image dimensions
inWidth = 368
inHeight = 368
 
# Prepare the frame to be fed to the network
inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
 
# Set the prepared object as the input blob of the network
net.setInput(inpBlob)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chắc không cần phải nói gì thêm, phần comment chú thích đã mô tả khá đầy đủ chức năng của từng phần trong này rồi.&lt;/p&gt;

&lt;p&gt;Bước 4: Thu thập kết quả và trích xuất điểm đặc trưng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
frameCopy = frame.copy()

output = net.forward()
print(&amp;quot;time taken by network : {:.3f}&amp;quot;.format(time.time() - t))
H = output.shape[2]
W = output.shape[3]

nPoints = 15
POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]


threshold = 0.01
# Empty list to store the detected keypoints
points = []
for i in range(nPoints):
    # confidence map of corresponding body&#39;s part.
    probMap = output[0, i, :, :]
 
    # Find global maxima of the probMap.
    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)
     
    # Scale the point to fit on the original image
    x = (frameWidth * point[0]) / W
    y = (frameHeight * point[1]) / H

    print(prob)
 
    if prob &amp;gt; threshold : 
        cv2.circle(frame, (int(x), int(y)), 15, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)
        cv2.putText(frame, &amp;quot;{}&amp;quot;.format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 2, lineType=cv2.LINE_AA)
 
        # Add the point to the list if the probability is greater than the threshold
        points.append((int(x), int(y)))
    else :
        points.append(None)
 
# cv2.imshow(&amp;quot;Output-Keypoints&amp;quot;,frame)
# cv2.waitKey(0)
# cv2.destroyAllWindows()

cv2.imwrite(&amp;quot;dot_keypoint.png&amp;quot;,frame)

# Draw Skeleton
for pair in POSE_PAIRS:
    partA = pair[0]
    partB = pair[1]

    if points[partA] and points[partB]:
        cv2.line(frameCopy, points[partA], points[partB], (0, 255, 255), 2)
        cv2.circle(frameCopy, points[partA], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)
        cv2.circle(frameCopy, points[partB], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)


cv2.imwrite(&amp;quot;line_keypoint.png&amp;quot;,frameCopy)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả của giá trị output là một ma trận 4D, với ý nghĩa của mỗi chiều như sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Chiều đầu tiên là image ID (định danh ảnh trong trường hợp bạn truyền nhiều ảnh vào mạng)&lt;/li&gt;
&lt;li&gt;Chiều thứ 2 là chỉ số của các điểm đặc trưng. Tập MPI trả về tập gồm 44 điểm dữ liệu, ta chỉ sử dụng một vài điểm dữ liệu tương ứng với vị trí các điểm đặc trưng mà chúng ta quan tâm.&lt;/li&gt;
&lt;li&gt;Chiều thứ 3 là height của output map.&lt;/li&gt;
&lt;li&gt;Chiều thứ 4 là width của output map.
Một lưu ý ở đây là tôi có sử dụng đặt giá trị chặn dưới threshold để giảm thiểu sự sai sót do nhận diện sai. Và kết quả đạt được là hai hình bên dưới:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/midu_pose_estimation_keypoint.png&#34; alt=&#34;Hình nhữn điểm đặc trưng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/midu_pose_estimation.png&#34; alt=&#34;Hình khung xương&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hẹn gặp lại các bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân biệt Epoch - Batch size và Iterations</title>
      <link>/blog/2018-10-02-understanding-epoch-batchsize-iterations/</link>
      <pubDate>Tue, 02 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-02-understanding-epoch-batchsize-iterations/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Khi mới bắt đầu bước vào thế giới của ML/DL chúng ta sẽ bắt gặp các thuật ngữ Epoch - Batch size và Iterations. Và sẽ cảm thấy bối rối vì chúng khá giống nhau, nhưng thực tế là chúng khác xa nhau.&lt;/p&gt;

&lt;p&gt;Để hiểu rõ sự khác biệt giữa chúng, các bạn cần tìm hiểu một số khái niệm trong machine learning như Gradient Descent.&lt;/p&gt;

&lt;p&gt;Định nghĩa ngắn gọn của Gradient Descent:&lt;/p&gt;

&lt;p&gt;Gradient Descent là thuật toán lặp tối ưu (iteractive optimization algorithm) được sử dụng trong machine learning để tìm kết quả tốt nhất (minima of a curve).&lt;/p&gt;

&lt;p&gt;Trong đó:
..* Gradient có nghĩa là tỷ lệ của độ nghiêm của đường dốc.&lt;/p&gt;

&lt;p&gt;..* Descent là từ viết tắt của decending - nghĩa là giảm.&lt;/p&gt;

&lt;p&gt;Thuật toán sẽ lặp đi lặp lại nhiều lần để tìm ra được kết quả tối ưu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/gradient.gif&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a&#34;&gt;https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a&lt;/a&gt; Nguồn ảnh&lt;/p&gt;

&lt;p&gt;Thuật toán gradient Descent có một tham số là learning rate. Như hình phía trên bên trái, ban đầu bước nhảy khá lớn, nghĩa là giá trị learning rate lớn, và sau một vài lần lặp, điểm chấm đen đi xuống dần, và giá trị learning rate nhỏ dần theo.&lt;/p&gt;

&lt;p&gt;Chúng ta sử dụng thuật ngữ epochs, batch size, iterations khi dữ liệu của chúng ta quá (rất) lớn (vd 10 triệu mẫu). Lúc này các khái niệm trên mới trở nên rõ ràng, còn với trường hợp dữ liệu nhỏ thì chúng khá tương tự nhau.&lt;/p&gt;

&lt;h2 id=&#34;khái-niện-epoch&#34;&gt;Khái niện Epoch&lt;/h2&gt;

&lt;p&gt;Một Epoch được tính là khi chúng ta đưa tất cả dữ liệu vào mạng neural network 1 lần.&lt;/p&gt;

&lt;p&gt;Khi dữ liệu quá lớn, chúng ta không thể đưa hết mỗi lần tất cả tập dữ liệu vào để huấn luyện được. Buộc lòng chúng ta phải chia nhỏ  tập dữ liệu ra thành các batch (size nhỏ hơn).&lt;/p&gt;

&lt;h3 id=&#34;tại-sao-phải-dùng-hơn-1-epoch&#34;&gt;Tại sao phải dùng hơn 1 Epoch.&lt;/h3&gt;

&lt;p&gt;Câu trả lời ở đây là tại vì chúng ta đang dùng thuật toán tối ưu là Gradient Descent. Thuật toán này đòi hỏi chúng ta phải đem toàn bộ dữ liệu qua mạng một vài lần để tìm được kết quả tối ưu. Vì vậy, dùng 1 epoch thật sự không đủ để tìm được kết quả tốt nhất.&lt;/p&gt;

&lt;p&gt;Với việc chỉ sử dụng 1 lần lặp, xác suất rất cao là dữ liệu sẽ bị underfitting(như hình mô tả bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/overfit_underfit.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Khi số lần lặp tăng dần, trạng thái của mô hình sẽ chuyển dần từ underfitting sang optimal và sau đó là overfitting (thông thường là vậy, trừ khi mô hình huấn luyện của bạn đang sử dụng quá đơn giản, quá ít trọng số thì chúng không thể nào overfitting nổi).&lt;/p&gt;

&lt;p&gt;Chúng ta có thể dùng 1 epoch để huấn luyện mô hình, với điều kiện là ta sử dụng thuật toán tối ưu không phải là gradient descent.&lt;/p&gt;

&lt;h3 id=&#34;số-lần-lặp-tối-ưu-là-bao-nhiêu&#34;&gt;Số lần lặp tối ưu là bao nhiêu?&lt;/h3&gt;

&lt;p&gt;Tiếc rằng không có câu trả lời cho câu hỏi này. Phụ thuộc hoàn toàn vào tập dữ liệu của bạn đang có.&lt;/p&gt;

&lt;h2 id=&#34;batch-size&#34;&gt;Batch Size&lt;/h2&gt;

&lt;p&gt;Batch size là số lượng mẫu dữ liệu trong một batch.&lt;/p&gt;

&lt;p&gt;Ở đây, khái niệm batch size và số lượng batch(number of batch) là hoàn toàn khác nhau.&lt;/p&gt;

&lt;p&gt;Như đã nói ở trên, chúng ta không thể đưa hết toàn bộ dữ liệu vào huấn luyện trong 1 epoch, vì vậy chúng ta cần phải chia tập dữ liệu thành các phần (number of batch), mỗi phần có kích thước là batch size.&lt;/p&gt;

&lt;h2 id=&#34;iterations&#34;&gt;Iterations&lt;/h2&gt;

&lt;p&gt;Iterations là số lượng batchs cần để hoàn thành 1 epoch.&lt;/p&gt;

&lt;p&gt;Ví dụ chúng ta có tập dữ liệu có 20,000 mẫu, batch size là 500, vậy chúng ta cần 40 lần lặp (iteration) để hoàn thành 1 epoch.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi bài viết.&lt;/p&gt;

&lt;p&gt;Nguồn: &lt;a href=&#34;https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9&#34;&gt;https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Xây dựng chương trình gợi ý phim dựa vào tập dữ liệu movie len</title>
      <link>/blog/2018-10-01-buiding-a-movie-model/</link>
      <pubDate>Mon, 01 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-01-buiding-a-movie-model/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;MovieLens là một tập dữ liệu được sử dụng rộng rãi cách đây nhiều năm. Hôm nay, mình sẽ sử dụng tập dữ liệu này và mô hình ALS của spark để xây dựng chương trình dự đoán phim cho người dùng.&lt;/p&gt;

&lt;h2 id=&#34;chuẩn-bị-dữ-liệu&#34;&gt;Chuẩn bị dữ liệu&lt;/h2&gt;

&lt;p&gt;Các bạn có thể download tập dữ liệu MovieLens ở link &lt;a href=&#34;https://grouplens.org/datasets/movielens/&#34;&gt;https://grouplens.org/datasets/movielens/&lt;/a&gt;. Các bạn có thể download trực tiếp 2 file nén ở link &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#34;&gt;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&lt;/a&gt; và link  &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest.zip&#34;&gt;http://files.grouplens.org/datasets/movielens/ml-latest.zip&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Ở trên bao gồm 2 tập dữ liệu. chúng ta tạo thư mục datasets và download rồi bỏ chúng vào trong thư mục đấy.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;complete_dataset_url = &#39;http://files.grouplens.org/datasets/movielens/ml-latest.zip&#39;
small_dataset_url = &#39;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#39;

import os

datasets_path = &#39;datasets&#39;
if not os.path.exists(datasets_path):
    os.makedirs(datasets_path))

complete_dataset_path = os.path.join(datasets_path, &#39;ml-latest.zip&#39;)
small_dataset_path = os.path.join(datasets_path, &#39;ml-latest-small.zip&#39;)

import urllib
import zipfile

if not os.path.exists(small_dataset_url):
    small_f = urllib.urlretrieve (small_dataset_url, small_dataset_path)#Download
    with zipfile.ZipFile(small_dataset_path, &amp;quot;r&amp;quot;) as z:#Giải nén
        z.extractall(datasets_path)
if not os.path.exists(small_dataset_url):
    complete_f = urllib.urlretrieve (complete_dataset_url, complete_dataset_path)#Download
    with zipfile.ZipFile(complete_dataset_path, &amp;quot;r&amp;quot;) as z:#Giải nén
        z.extractall(datasets_path)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong thư mục giải nén, chúng ta sẽ có các file ratings.csv, movies.csv, tags.csv, links.csv, README.txt.&lt;/p&gt;

&lt;h2 id=&#34;loading-và-parsing-dữ-liệu&#34;&gt;Loading và parsing dữ liệu.&lt;/h2&gt;

&lt;p&gt;Mỗi dòng trong tập ratings.csv có định dạng &lt;code&gt;&amp;quot;userId,movieId,rating,timestamp&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mỗi dòng trong tập movies.csv có định dạng &lt;code&gt;&amp;quot;movieId,title,genres&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mỗi dòng trong tập tags.csv có định dạng &lt;code&gt;&amp;quot;userId,movieId,tag,timestamp&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mỗi dòng trong tập links.csv có định dạng &lt;code&gt;&amp;quot;movieId,imdbId,tmdbId&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Tóm lại, các trường dữ liệu trong các file csv đều ngăn cách nhau bởi dấu phẩy (,). Trong python, ta có thể dùng hàm split để cắt chúng ra. Sau đó sẽ load toàn bộ dữ liệu lên RDDs.&lt;/p&gt;

&lt;p&gt;Lưu ý nhỏ:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ở tập dữ liệu ratings, chúng ta chỉ giữ lại các trường &lt;code&gt;(UserID, MovieID, Rating)&lt;/code&gt; bỏ đi trường timestamp vì không cần thiết.&lt;/li&gt;
&lt;li&gt;Ở tập dữ liệu movies  chúng ta giữ lại trường &lt;code&gt;(MovieID, Title)&lt;/code&gt; và bỏ đi trường genres vì lý do tương tự.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;small_ratings_file = os.path.join(datasets_path, &#39;ml-latest-small&#39;, &#39;ratings.csv&#39;)
small_ratings_raw_data = sc.textFile(small_ratings_file)
small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]
small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header).map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()
print(small_ratings_data.take(3)) #Hiện thị top 3 ratting đầu tiên

small_movies_file = os.path.join(datasets_path, &#39;ml-latest-small&#39;, &#39;movies.csv&#39;)

small_movies_raw_data = sc.textFile(small_movies_file)
small_movies_raw_data_header = small_movies_raw_data.take(1)[0]

small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\
    .map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (tokens[0],tokens[1])).cache()
    
small_movies_data.take(3) #Hiện thị top 3 movie đầu tiên
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phần tiếp theo, chúng ta sẽ tìm hiểu lọc cộng tác (Collaborative Filtering) và cách sử dụng Spark MLlib để xây dựng mô hình dự báo.&lt;/p&gt;

&lt;h2 id=&#34;collaborative-filtering&#34;&gt;Collaborative Filtering&lt;/h2&gt;

&lt;p&gt;Ở đây, tôi sẽ không đề cập đến lọc cộng tác là gì, các bạn có nhu cầu tìm hiểu có thể xem ở bài post khác hoặc tham khảo trên wiki. Chúng ta sẽ tập trung vào tìm hiểu cách sử dụng ALS trong thư viện MLlib của Spark. Các tham số của thuật toán này bao gồm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;numBlocks: số lượng block được sử dụng trong tính toán song song (-1 với ý nghĩa là auto configure).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rank: số lượng nhân tố ẩn (latent factor) trong mô hình.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;iterations: số lần lặp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lambda: tham số của chuẩn hoá(regularization ) trong ALS.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;chọn-các-tham-số-cho-als&#34;&gt;Chọn các tham số cho ALS&lt;/h2&gt;

&lt;p&gt;Để chọn được các tham số tốt nhất cho mô hình ALS, chúng ta sẽ sử dụng tập small để grid search. Đầu tiên, chúng ta chia tập dữ liệu thành 3 phần là tập train, tập vali và  tập test. Sau đó tiến hành huấn luyện trên tập train và predict trên tập valid để tìm được tham số tốt nhất. Cuối cùng đánh giá kết quả đạt được trên tập test.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)
validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))
test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))

from pyspark.mllib.recommendation import ALS
import math

seed = 5L
iterations = 10
regularization_parameter = 0.1
ranks = [4, 8, 12]
errors = [0, 0, 0]
err = 0
tolerance = 0.02

min_error = float(&#39;inf&#39;)
best_rank = -1
best_iteration = -1
for rank in ranks:
    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,
                      lambda_=regularization_parameter)
    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))
    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)
    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())
    errors[err] = error
    err += 1
    print(&#39;For rank %s the RMSE is %s&#39; % (rank, error))
    if error &amp;lt; min_error:
        min_error = error
        best_rank = rank

print(&#39;The best model was trained with rank %s&#39; % best_rank)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả sau khi thực hiện đoạn code trên là:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;For rank 4 the RMSE is 0.963681878574
For rank 8 the RMSE is 0.96250475933
For rank 12 the RMSE is 0.971647563632
The best model was trained with rank 8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiến hành thực hiện test.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_test = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,
                      lambda_=regularization_parameter)
predictions = model_test.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))
rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)
error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())
    
print(&#39;For testing data the RMSE is %s&#39; % (error))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;For testing data the RMSE is 0.972342381898
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Xem kỹ hơn một chút về dữ liệu mà spark trả về cho chúng ta. Với predictions và rates_and_preds, ta có:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(predictions.take(3))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[((32, 4018), 3.280114696166238),
 ((375, 4018), 2.7365714977314086),
 ((674, 4018), 2.510684514310653)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tập dữ liệu trả về bao gồm cặp &lt;code&gt;(UserID, MovieID)&lt;/code&gt; và &lt;code&gt;Rating&lt;/code&gt; (tương ứng với colum 0, column 1 và column 2 ở trên),được hiểu ở đây là với người dùng UserID và phim MovieID thì mô hình sẽ dự đoán người dùng sẽ rating kết quả Rating.&lt;/p&gt;

&lt;p&gt;Sau đó chúng ta sẽ nối(join) chúng với tập valid tương ứng theo cặp &lt;code&gt;(UserID, MovieID)&lt;/code&gt;, kết quả đạt được là:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rates_and_preds.take(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[((558, 788), (3.0, 3.0419325487471403)),
 ((176, 3550), (4.5, 3.3214065001580986)),
 ((302, 3908), (1.0, 2.4728711204440765))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Việc còn lại là chúng ta sẽ tính trung bình độ lỗi bằng hàm &lt;code&gt;mean()&lt;/code&gt; và &lt;code&gt;sqlt()&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;xây-dựng-mô-hình-với-tập-dữ-liệu-large&#34;&gt;Xây dựng mô hình với tập dữ liệu large&lt;/h2&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ sử dụng tập dự liệu bự hơn để xây dựng mô hình. Cách thực hiện y chang như tập dữ liệu nhỏ đã được trình bày ở trên, nên tôi sẽ bỏ qua một số giải thích không cần thiết để tránh lặp lại.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the complete dataset file
complete_ratings_file = os.path.join(datasets_path, &#39;ml-latest&#39;, &#39;ratings.csv&#39;)
complete_ratings_raw_data = sc.textFile(complete_ratings_file)
complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]

# Parse
complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\
    .map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()
    
print(&amp;quot;There are %s recommendations in the complete dataset&amp;quot; % (complete_ratings_data.count()))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;There are 21063128 recommendations in the complete dataset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiến hành train và test.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)

complete_model = ALS.train(training_RDD, best_rank, seed=seed,iterations=iterations, lambda_=regularization_parameter)

test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))

predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))
rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)
error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())
    
print(&#39;For testing data the RMSE is %s&#39; % (error))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;For testing data the RMSE is 0.82183583368
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;xây-dựng-mô-hình-dự-đoán-phim&#34;&gt;Xây dựng mô hình dự đoán phim&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;complete_movies_file = os.path.join(datasets_path, &#39;ml-latest&#39;, &#39;movies.csv&#39;)
complete_movies_raw_data = sc.textFile(complete_movies_file)
complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]

# Parse
complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\
    .map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()

complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))
    
print(&amp;quot;There are %s movies in the complete dataset&amp;quot; % (complete_movies_titles.count()))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;There are 27303 movies in the complete dataset
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_counts_and_averages(ID_and_ratings_tuple):
    nratings = len(ID_and_ratings_tuple[1])
    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)

movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())
movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)
movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Giả sử chúng ta có 1 người dùng mới, với các ratting như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_user_ID = 0

# The format of each line is (userID, movieID, rating)
new_user_ratings = [
     (0,260,4), # Star Wars (1977)
     (0,1,3), # Toy Story (1995)
     (0,16,3), # Casino (1995)
     (0,25,4), # Leaving Las Vegas (1995)
     (0,32,4), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)
     (0,335,1), # Flintstones, The (1994)
     (0,379,1), # Timecop (1994)
     (0,296,3), # Pulp Fiction (1994)
     (0,858,5) , # Godfather, The (1972)
     (0,50,4) # Usual Suspects, The (1995)
    ]
new_user_ratings_RDD = sc.parallelize(new_user_ratings)
print(&#39;New user ratings: %s&#39; % new_user_ratings_RDD.take(10))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta tiến hành huấn luyện lại mô hình khi có thêm người mới:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)

from time import time

t0 = time()
new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, 
                              iterations=iterations, lambda_=regularization_parameter)
tt = time() - t0

print(&amp;quot;New model trained in %s seconds&amp;quot; % round(tt,3))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;New model trained in 56.61 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiến hành dự đoán ratting của người dùng mới cho toàn bộ các phim người dùng đó chưa xem.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs
# keep just those not on the ID list (thanks Lei Li for spotting the error!)
new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))

# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies
new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và show ra top 3 kết quả :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)
new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))
new_user_recommendations_rating_title_and_count_RDD = \
    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)
new_user_recommendations_rating_title_and_count_RDD.take(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hiển thị top recommend (Ở đây sẽ flat dữ liệu hiển thị thành dàng &lt;code&gt;((Title, Rating, Ratings Count))&lt;/code&gt; ra cho dễ nhìn).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))

top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]&amp;gt;=25).takeOrdered(25, key=lambda x: -x[1])

print (&#39;TOP recommended movies (with more than 25 reviews):\n%s&#39; %
        &#39;\n&#39;.join(map(str, top_movies)))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TOP recommended movies (with more than 25 reviews):
    (u&#39;&amp;quot;Godfather: Part II&#39;, 8.503749129186701, 29198)
    (u&#39;&amp;quot;Civil War&#39;, 8.386497469089297, 257)
    (u&#39;Frozen Planet (2011)&#39;, 8.372705479107108, 31)
    (u&#39;&amp;quot;Shawshank Redemption&#39;, 8.258510064442426, 67741)
    (u&#39;Cosmos (1980)&#39;, 8.252254825768972, 948)
    (u&#39;Band of Brothers (2001)&#39;, 8.225114960311624, 4450)
    (u&#39;Generation Kill (2008)&#39;, 8.206487040524653, 52)
    (u&amp;quot;Schindler&#39;s List (1993)&amp;quot;, 8.172761674773625, 53609)
    (u&#39;Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)&#39;, 8.166229786764168, 23915)
    (u&amp;quot;One Flew Over the Cuckoo&#39;s Nest (1975)&amp;quot;, 8.15617022970577, 32948)
    (u&#39;Casablanca (1942)&#39;, 8.141303207981174, 26114)
    (u&#39;Seven Samurai (Shichinin no samurai) (1954)&#39;, 8.139633165142612, 11796)
    (u&#39;Goodfellas (1990)&#39;, 8.12931139039048, 27123)
    (u&#39;Star Wars: Episode V - The Empire Strikes Back (1980)&#39;, 8.124225700242096, 47710)
    (u&#39;Jazz (2001)&#39;, 8.078538221315313, 25)
    (u&amp;quot;Long Night&#39;s Journey Into Day (2000)&amp;quot;, 8.050176820606127, 34)
    (u&#39;Lawrence of Arabia (1962)&#39;, 8.041331489948814, 13452)
    (u&#39;Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)&#39;, 8.0399424815528, 45908)
    (u&#39;12 Angry Men (1957)&#39;, 8.011389274280754, 13235)
    (u&amp;quot;It&#39;s Such a Beautiful Day (2012)&amp;quot;, 8.007734839026181, 35)
    (u&#39;Apocalypse Now (1979)&#39;, 8.005094327199552, 23905)
    (u&#39;Paths of Glory (1957)&#39;, 7.999379786394267, 3598)
    (u&#39;Rear Window (1954)&#39;, 7.9860865203540214, 17996)
    (u&#39;State of Play (2003)&#39;, 7.981582126801772, 27)
    (u&#39;Chinatown (1974)&#39;, 7.978673289692703, 16195)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dự-đoán-rating-của-1-cá-nhân&#34;&gt;Dự đoán rating của 1 cá nhân&lt;/h2&gt;

&lt;p&gt;Một trường hợp khác là chúng ta cần dự đoán giá trị ratting của 1 người dùng với 1 bộ phim cụ thể nào đó.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)
individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)
individual_movie_rating_RDD.take(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[Rating(user=0, product=122880, rating=4.955831875971526)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;lưu-trữ-mô-hình&#34;&gt;Lưu trữ mô hình&lt;/h2&gt;

&lt;p&gt;Sau khi có được mô hình. Chúng ta cần phải lưu trữ chúng lại để sau này dùng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.mllib.recommendation import MatrixFactorizationModel

model_path = os.path.join(&#39;models&#39;, &#39;movie_lens_als&#39;)

# Save and load model
model.save(sc, model_path)
same_model = MatrixFactorizationModel.load(sc, model_path)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu về mạng neural network AlexNet</title>
      <link>/blog/2018-06-15-understanding-alexnet/</link>
      <pubDate>Fri, 15 Jun 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-06-15-understanding-alexnet/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Tỷ phú Peter Thiel đã từng đưa ra câu hỏi tréo ngoe như thế này: &amp;ldquo;What important truth do very few people agree with you on?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Nếu bạn đem câu này hỏi giáo sư Geoffrey Hinton vào năm 2010, ông ấy sẽ trả lời rằng mạng Convolutional Neural Networks (CNN) sẽ có bước đột phá lớn và giúp chúng ta giải quyết hoàn toàn bài toán phân loại ảnh. Tại thời điểm năm 2010, các nhà nghiên cứu trong lĩnh vực phân loại ảnh đều không nghĩ như giáo sư Geoffrey Hinton. Và Deep Learning tại thời điểm đó chưa thật sự giải quyết được bài toán này.&lt;/p&gt;

&lt;p&gt;Năm 2010 cũng là năm ra đời của cuộc thi ImageNet Large Scale Visual Recognition Challenge. Tập dữ liệu ảnh trong cuộc thi bao gồm khoảng 1.2 triệu ảnh thuộc 1000 lớp khác nhau, người thắng cuộc là người tạo ra mô hình làm cho độ lỗi trên tập dữ liệu trên là nhỏ nhất.&lt;/p&gt;

&lt;p&gt;Hai năm sau, trong bài báo &amp;ldquo;ImageNet Classification with Deep Convolutional Neural Networks&amp;rdquo; của nhóm tác giả Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, Geoffrey và các cộng sự của mình đã chứng minh điều ông ấy nói hai năm trước là hoàn toàn chính xác.
Ở bài báo này, nhóm tác giả đã huấn luyện mạng CNN và và đạt độ lỗi top-5 error rate là 15.3% (nhóm tác giả đã giành hạng nhất), cách biệt khá xa so với kết quả của nhóm đứng thứ hai(độ lỗi 26.2%). Trong các năm tiếp theo, rất nhiều nhóm đã nghiên cứu, cải tiến kiến trúc của mô hình CNN để đạt được kết quả tốt hơn, thậm chí hơn luôn khả năng nhận biết của con người.&lt;/p&gt;

&lt;p&gt;Kiến trúc mạng CNN được sử dụng vào năm 2012 được cộng đồng nghiên cứu gọi với tên gọi thân thương là AlexNet do tác giả chính của nhóm nghiên cứu là Alex Krizhevsky. Ở trong bài viết này, chúng ta sẽ đi sâu vào tìm hiểu kiến trúc AlexNet và đóng góp chính của nó trong CNN.&lt;/p&gt;

&lt;h2 id=&#34;đầu-vào&#34;&gt;Đầu vào&lt;/h2&gt;

&lt;p&gt;Như đã đề cập ở phần trên, mạng AlexNet đã thắng hạng nhất trong cuộc thi ILSVRC năm 2012. Mô hình giải quyết bài toán phân lớp một bức ảnh vào 1 lớp trong 1000 lớp khác nhau (vd gà, chó, mèo &amp;hellip; ). Đầu ra của mô hình là một vector có 1000 phần tử. Phần tử thứ i của vector đại diện cho xác suất bức ảnh thuộc về lớp thứ i. Do đó, tổng của các phần tử trong vector là 1.&lt;/p&gt;

&lt;p&gt;Đầu vào của mạng AlexNet là một bức ảnh RGB có kích thước 256x256 pixel. Toàn bộ các bức ảnh của tập train và tập test đều có cùng kích thước là 256x256. Nếu một bức ảnh nào đó không có kích thước 256x256, bức ảnh đó sẽ được chuyển về kích thước đúng 256x256. Những bức hình có kích thước nhỏ hơn 256 thì sẽ được phóng bự lên đến kích thước 256, những bức hình nào có kích thước lớn hơn 256 thì sẽ được cắt loại phần thừa để nhận được bức hình có kích thước 256x256. Hình ảnh ở dưới là một ví dụ về việc điều chỉnh bức ảnh về kích thước 256x256.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/AlexNet-Resize-Crop-Input.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nếu ảnh đầu vào là ảnh xám (grayscale), bức ảnh trên sẽ được chuyển đổi thành định dạng RGB bằng cách tạo ra 3 layer kênh màu giống nhau từ ảnh xám.&lt;/p&gt;

&lt;p&gt;Sau khi chuẩn hoá hết tất cả các ảnh về dạng 256x256x3, nhóm tác giả chỉ sử dụng một phần của bức ảnh có kích thước 227x227x3 của một bức ảnh làm đầu vào cho mạng neural network. Trong bài báo nhóm tác giả ghi là 224x224, nhưng đây là một lỗi nhỏ của nhóm tác giả, và kích thước thực tế đầu vào của bức ảnh là 227x227.&lt;/p&gt;

&lt;h2 id=&#34;kiến-trúc-alexnet&#34;&gt;Kiến trúc AlexNet&lt;/h2&gt;

&lt;p&gt;Kiến trúc AlexNet lớn hơn nhiều so với các kiến trúc CNNs được sử dụng trong thị giác máy tính trước kia (trước năm 2010), vd kiến trúc LeNet của Yann LeCun năm 1998. Nó có 60 triệu tham số và 650000 neural và tốn khoảng từ năm đến sáu ngày huấn luyện trên hai GPU GTX 580 3GB. Ngày nay, với sự tiến bộ vượt bật của GPU, chúng ta có nhiều kiến trúc CNN có cấu trúc phức tạp hơn, và hoạt động rất hiệu quả trên những tập dữ liệu phức tạp. Nhưng tại thời điểm năm 2012 thì việc huấn luyện mô hình với lượng tham số và neural lớn như vậy là một vấn đề cực kỳ khó khăn. Nhìn kỹ vào hình bên dưới để hiểu rõ hơn về kiến trúc AlexNet.
&lt;img src=&#34;/post_image/AlexNet-1.png&#34; alt=&#34;Kiến trúc AlexNet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AlexNet bao gồm 5 convolution Layer và 3 Fully connected Layers.&lt;/p&gt;

&lt;p&gt;Những convolution layer ( hay còn gọi với tên khác là các filter) rút trích các thông tin hữu ích trong các bức ảnh. Trong một convolution layer bất kỳ thường bao gồm nhiều kernel có cùng kích thước. Ví dụ như convolution layer đầu tiên của AlexNet chứa 96 kernel có kích thước 11x11x3. Thông thường thì width và height của một kernel bằng nhau, và độ sâu (depth) thường bằng số lượng kênh màu.&lt;/p&gt;

&lt;p&gt;Convolutional 1 và convolution 2 kết nối với nhau qua một Overlapping Max Pooling ở giữa. Tương tự như vậy giữa convolution 2 và convolution 3. Convolutional 3, convolution 4, convolution 5 kết nối trực tiếp với nhau, không thông qua trung gian. Convolutional 5 kết nối fully connected layter 1 thông qua một Overlapping Max pooling, tiếp theo mà một fully connected layter nữa. Và cuối cùng là một bộ phân lớp softmax với 1000 lớp nhãn (các bạn có thể xem hình kiến trúc mạng AlexNet ở trên để có cái nhìn tổng quát hơn).&lt;/p&gt;

&lt;p&gt;ReLU nonlinerity được sử dụng sau tất các các convolution và fully connected layer. Trước đây, ReLU nonlinerity của lớp convolution 1 và 2 thường theo sau bởi một bước chuẩn hoá cục bộ (local normalization) rồi mới thực hiện pooling. Tuy nhiên, các nghiên cứu sau đó nhận thấy rằng việc sử dụng normalization không thật sự hữu ích. Do vậy chúng ta sẽ không đi chi tiết về vấn đề đó.&lt;/p&gt;

&lt;h2 id=&#34;overlapping-max-pooling&#34;&gt;Overlapping Max Pooling&lt;/h2&gt;

&lt;p&gt;Max Pooling layer thường được sử dụng để giảm chiều rộng và chiều dài của một tensor nhưng vẫn giữ nguyên chiều sâu. Overlapping Max Pool layter cũng tương tự như Max Pool layter, ngoại trừ việc là một window của bước này sẽ có một phần chồng lên window của bước tiếp theo. Tác giả sử dụng pooling có kích thước 3x3 và bước nhảy là 2 giữa các pooling. Nghĩa là giữa pooling này và pooling khác sẽ overlapping với nhau 1 pixel. Các thí nghiệm thực tế đã chứng minh rằng việc sử dụng overlapping giữa các pooling giúp giảm độ lỗi top-1 error 0.4% và top-5 error là 0.3% khi so với việc sử dụng pooling có kích thước 2x2 và bước nhảy 2 (vector output của cả hai đều có số chiều bằng nhau).&lt;/p&gt;

&lt;h2 id=&#34;relu-nonlinearity&#34;&gt;ReLu Nonlinearity&lt;/h2&gt;

&lt;p&gt;Một cải tiến quan trọng khác của AlexNet là việc sử dụng hàm phi tuyến ReLU. Trước đây, các nhóm nghiên cứu khác thường sử dụng hàm kích hoạt là hàm Tanh hoặc hàm Sigmoid để huấn luyên mô hình neural network. AlexNet chỉ ra rằng, khi sử dụng ReLU, mô hình deep CNN sẽ huấn luyện nhanh hơn so với viêc sử dụng tanh hoặc sigmoid. Hình bên dưới được rút ra từ bài báo chỉ ra rằng với việc sử dụng ReLU (đường nét liền trong hình), AlexNet đạt độ lỗi 25% trên tập huấn luyện và nhanh hơn gấp 6 lần so với mô hình tương tự nhưng sử dụng Tanh (đường nét đứt trong hình). Thí nghiệm trên sử dụng tập dữ liệu CIFAR-10 để huấn luyện.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/ReluNonlinearity-768x635.png&#34; alt=&#34;Tốc độ hội tụ của mạng AlexNet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Để hiểu rõ hơn lý do vì sao ReLU lại nhanh hơn so với các hàm khác, chúng ta hãy đối sánh hình dạng giá trị output của các hàm trên.&lt;/p&gt;

&lt;p&gt;Công thức của ReLU là: f(X) = max(0,x)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/Tanh-300x238.png&#34; alt=&#34;Hàm kích hoạt của ReLU và tanh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn kỹ vào hình trên, ta có nhận xét rằng: hàm tanh đạt giá trị bão hoà khi giá trị z &amp;gt;2.5 và z &amp;lt; -2.5 (số 2.5 là số cảm tính của mình). Và tại vùng |z|&amp;gt;2.5, thì độ dốc của hàm hầu như gần như bằng 0, |z| càng lớn thì độ dốc càng gần 0 hơn. Vì lý do này nên gradient descent sẽ hội tụ chậm. Còn đối với hàm ReLU, với giá trị z dương thì độ dốc của hàm không gần bằng 0 như hàm tanh. Điều này giúp cho việc hội tụ xảy ra nhanh hơn. Với giá trị z âm, độ dốc bằng 0, tuy nhiên, hầu hết các giá trị của các neural trong mạng thường có giá trị dương, nên trường hợp âm ít (hiếm) khi xảy ra. ReLU huấn luyện nhanh hơn so với sigmoid cũng bởi lý do tương tự.&lt;/p&gt;

&lt;h2 id=&#34;reducing-overfitting&#34;&gt;Reducing overfitting&lt;/h2&gt;

&lt;h3 id=&#34;overfitting-là-gì&#34;&gt;Overfitting là gì?&lt;/h3&gt;

&lt;p&gt;Khi bạn dạy một đứa trẻ từ 2-5 tuổi về việc cộng hai số, chúng sẽ học rất nhanh và trả lời đúng hầu hết các câu hỏi mà chúng ta đã dạy chúng. Tuy nhiên, chúng sẽ trả lời sai đối với những câu hỏi hơi lắc léo một chút (câu hỏi tương tự câu chúng ta đã dạy, nhưng thêm một xíu thông tin đòi hỏi trẻ phải suy nghĩ), hoặc các câu hỏi chưa được dạy. Lý do chúng trả lời sai những câu hỏi đó là khi trả lời những câu hỏi được dạy, chúng thường nhớ lại câu trả lời, chứ không thực sự hiểu câu hỏi. Cái này ở Việt Nam ta gọi là học vẹt.&lt;/p&gt;

&lt;p&gt;Tương tự vậy, Neural network chính bản thân nó có khả năng học được những gì được dạy, tuy nhiên, nếu quá trình huấn luyện của bạn không tốt, mô hình có khả năng sẽ giống như những đứa trẻ trên kia, hồi tưởng lại những gì đã dạy cho chúng mà không hiểu bản chất. Và kết quả Neural Network sẽ hoạt động tốt trên tập huấn luyện ( nhưng chúng không rút ra được bản chất chính của vấn đề), và kết quả trên tập test tệ. Người ta gọi trường hợp trên là &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Nhóm nghiên cứu AlexNet sử dụng nhiều phương pháp khác nhau để giảm overfitting.&lt;/p&gt;

&lt;h3 id=&#34;data-augmentation&#34;&gt;Data Augmentation&lt;/h3&gt;

&lt;p&gt;Việc sử dụng nhiều biến thể khác nhau của một bức hình có thể giúp ngăn mô hình không bị overfitting. Với việc sử dụng nhiều biến thể của 1 bức hình, bạn bắt ép mô hình không học vẹt dữ liệu. Có nhiều cách khác nhau để sinh ra dữ liệu mới dựa vào dữ liệu có sẵn. Một vài các mà nhóm AlexNet đã sử dụng là.&lt;/p&gt;

&lt;h4 id=&#34;data-augmentation-by-mirroring&#34;&gt;Data Augmentation by Mirroring&lt;/h4&gt;

&lt;p&gt;Ý tưởng của việc này là lấy ảnh trong gương của một bức hình (ảnh ảo). Nhìn vào ảnh bên dưới, bên trái là hình gốc của con mèo trong tập huấn luyện, bên phải là ảnh của con mèo khi thêm hiệu ứng hình qua gương (đơn giản là xoay qua trục y là được )
&lt;img src=&#34;/post_image/AlexNet-Data-Augmentation-Mirror-Image.jpg&#34; alt=&#34;Tái tạo ảnh sử dụng phản ảnh&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;data-augmentation-by-random-crops&#34;&gt;Data Augmentation by Random Crops&lt;/h4&gt;

&lt;p&gt;Việc lựa chọn vị trí ảnh gốc một cách ngẫu nhiên cũng giúp chúng ta có thêm một ảnh khác so với ảnh gốc ban đầu.&lt;/p&gt;

&lt;p&gt;Nhóm tác giả của AlexNet rút trích ngẫu nhiên bức ảnh có kích thước 227x227 từ bức ảnh 256x256 ban đầu làm input dầu vào cho mô hình. Bằng cách này, chúng ta có thể tăng số lượng dữ liệu lên gấp 2048 lần bằng việc sử dụng cách này.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/AlexNet-Data-Augmentation-Random-Crops.jpg&#34; alt=&#34;radom select&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bốn bức ảnh được crop ngẫu nhiên ở trên thoạt nhìn có vẻ giống nhau, nhưng thực chất không phải như vậy.&lt;/p&gt;

&lt;p&gt;Với việc sử dụng Data Augmentation, chúng ta đang bố gắng dạy cho mô hình rằng với việc nhìn hình con mèo qua gương, nó vẫn là con mèo, hoặc hình hình con mèo ở bất kỳ góc độ nào thì nó vẫn là nó.&lt;/p&gt;

&lt;h3 id=&#34;dropout&#34;&gt;Dropout&lt;/h3&gt;

&lt;p&gt;Với gần 60 triệu tham số trong tập huấn luyện, việc overfitting xảy ra là điều dễ hiểu. Các tác giả của AlexNet đã thực nghiệm nhiều cách nữa để giảm overfitting. Họ sử dụng một kỹ thuật gọi là dropout - kỹ thuật này được giới thiệu ở bài báo khác của G.E. Hintol vào năm 2012. Kỹ thuật này khá đơn giản, một neural sẽ có xác suất bị loại khỏi mô hình là 0.5. Khi một neural bị loại khỏi mô hình, nó sẽ không được tham qia vào quá trình lan truyền tiến hoặc lan truyền ngược. Cho nên, mỗi giá trị input sẽ đi qua một kiến trúc mạng khác nhau. Như mô tả ở hình động ở dưới, kết quả là giá trị của tham số trọng số sẽ tốt hơn và khó bị overfitting hơn. Trong quá trình test, toàn bộ network được sử dụng, không có dropout, tuy nhiên, giá trị output sẽ scaled bởi tham số 0.5 tương ứng với những neural không sử dụng trong quá trình trainning. Với việc sử dụng dropout, chúng ta sẽ tăng gấp đôi lần lặp cần thiết để đạt được độ hội tụ, nhưng khi không sử dụng dropout, mạng AlexNet rất dễ bị overfitting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/dropoutAnimation.gif&#34; alt=&#34;drop out&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ngày nay, chuẩn hoá dropout là một yếu tố không thể thiếu và các mô hình sử dụng nó thường có kết quả tốt hơn so với mô hình tương tự không sử dụng dropout. Chúng ta sẽ bàn sâu hơn về dropout ở một bài khác trong tương lai.&lt;/p&gt;

&lt;p&gt;Tham khảo&lt;/p&gt;

&lt;p&gt;ImageNet Classification with Deep Convolutional Neural Networks  by Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, 2012&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.learnopencv.com/understanding-alexnet/&#34;&gt;https://www.learnopencv.com/understanding-alexnet/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>