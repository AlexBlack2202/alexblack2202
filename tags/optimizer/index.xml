<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>optimizer on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/optimizer/</link>
    <description>Recent content in optimizer on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2016-{year} Phạm Duy Tùng. All Rights Reserved.</copyright>
    <lastBuildDate>Fri, 15 Jan 2021 00:19:00 +0300</lastBuildDate><atom:link href="/tags/optimizer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tìm hiểu thuật toán tối ưu hóa Adabelief Optimizer</title>
      <link>/blog/2021-01-15-adabelief-optimizer/</link>
      <pubDate>Fri, 15 Jan 2021 00:19:00 +0300</pubDate>
      
      <guid>/blog/2021-01-15-adabelief-optimizer/</guid>
      <description>Giới thiệu Hi các bạn, lại là mình đây, hôm nay mình sẽ cùng các bạn tìm hiểu thuật toán tối ưu hóa AdaBelief. Thuật toán này được sử dụng để thay cho thuật toán Adam optimizer mà các bạn hiện đang xài để huấn luyện mô hình Deep learning. Nào, chúng ta cùng bắt đầu tìm hiểu nhé.
Ẩn sâu bên trong các thuật toán sử dụng Neural Network và một vài thuật toán machine learning đều sử dụng các hàm tối ưu hóa.</description>
    </item>
    
  </channel>
</rss>
