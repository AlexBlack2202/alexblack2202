<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>opencv on Phạm Duy Tùng Machine Learning Blog</title>
    <link>/tags/opencv/</link>
    <description>Recent content in opencv on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>alexblack2202@gmail.com (Phạm Duy Tùng)</managingEditor>
    <webMaster>alexblack2202@gmail.com (Phạm Duy Tùng)</webMaster>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Fri, 15 Jan 2021 00:19:00 +0300</lastBuildDate>
    <atom:link href="/tags/opencv/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tìm hiểu thuật toán tối ưu hóa Adabelief Optimizer</title>
      <link>/blog/2021-01-15---adabelief-optimizer/</link>
      <pubDate>Fri, 15 Jan 2021 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2021-01-15---adabelief-optimizer/</guid>
      <description>

&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới thiệu&lt;/h1&gt;

&lt;p&gt;Hi các bạn, lại là mình đây, hôm nay mình sẽ cùng các bạn tìm hiểu thuật toán tối ưu hóa AdaBelief. Thuật toán này được sử dụng để thay cho thuật toán Adam optimizer mà các bạn hiện đang xài để huấn luyện mô hình Deep learning. Nào, chúng ta cùng bắt đầu tìm hiểu nhé.&lt;/p&gt;

&lt;p&gt;Ẩn sâu bên trong các thuật toán sử dụng Neural Network  và một vài thuật toán machine learning đều sử dụng các hàm tối ưu hóa. Chúng ta có thể liệt kê ra một vài cái tên như RMSprop, SGD (Stochastic Gradient Descent), Adam (Adaptive Moment Estimation).&lt;/p&gt;

&lt;p&gt;Một vài các yếu tố hay được sử dụng để đánh giá một thuật toán optimizer:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hội tụ nhanh (trong quá trình train)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sự tổng quát hóa cao (vẫn nhận dạng được những mẫu chưa từng được huấn luyện)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Độ chính xác cao&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Các thuật toán tối ưu thuộc họ Adaptive thường có tốc độ hội tụ nhanh. Trong khi đó, các thuật toán thuộc họ SGD thường có sự tổng quát hóa cao. Gần đây, Juntang Zhuang và các cộng sự thuộc đại học Yale đã nghiên cứu và tạo ra thuật toán AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. Thuật toán này theo lời tác giả, hội tụ cả hai ưu điểm của họ Adaptive và SGD, là vừa có tốc độ hội tụ nhanh, vừa có tính tổng quát hóa cao Mã nguồn được tác giả công bố ở link &lt;a href=&#34;https://github.com/juntang-zhuang/Adabelief-Optimizer&#34;&gt;https://github.com/juntang-zhuang/Adabelief-Optimizer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lời của tác giả:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We propose the AdaBelief optimizer, which adaptively scales the stepsize by the difference betweenpredicted gradient and observed gradient.  To our knowledge, AdaBelief is the first optimizer toachieve three goals simultaneously: fast convergence as in adaptive methods, good generalization asin SGD, and training stability in complex settings such as GANs. Furthermore, Adabelief has the same parameters as Adam, hence is easy to tune. We validate the benefits of AdaBelief with intuitive examples, theoretical convergence analysis in both convex and non-convex cases, and extensiveexperiments on real-world datasets&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Để hiểu về AdaBelief, trước tiên, chúng ta phải có một ít kiến thức cơ bản về SGD và Adam, nên chúng ta sẽ bắt đầu nói về SGD trước&lt;/p&gt;

&lt;h1 id=&#34;sgd-stochastic-gradient-descent&#34;&gt;SGD - Stochastic Gradient Descent&lt;/h1&gt;

&lt;p&gt;Thuật toán SGD là thuật toán tối ưu hóa cơ bản theo họ gradient. Thuật toán này rất triển khai, có nền tảng lý thuyết vững chắc, cực kỳ ổn định trong quá trình huấn luyện, kết quả đạt được có thể so sánh với các thuật toán khác. Ý tưởng của thuật toán khá đơn giản, đó là &amp;ldquo;tính giá trị gradient của mỗi tham số, và đi một bước nhỏ theo chiều của gradient&amp;rdquo;. Nếu chúng ta lặp đi lặp lại quá trình này, và ngẫu nhiên chọn (stochastic) một tập batch trong tập huấn luyện, mô hình chúng ta sẽ được cải tiến dần đến đểm hội tụ.&lt;/p&gt;

&lt;p&gt;Trong quá khứ, phần khó nhất của SGD là việc tính lại giá trị gradient cho toàn bộ các tham số trong mô hình. Nhưng hiện nay, các framwork máy học như Tensorflow, PyTouch, Caffee, Theano, &amp;hellip;. đã giúp chúng ta tính các giá trị gradient một cách tự động. Do đó, công việc của chúng ta hiện thời đơn giản hơn&lt;/p&gt;

&lt;p&gt;$$for \text{ }  i \text{ } in \text{ } range (m): $$
  $$\theta_i = \theta_i - \alpha ( \hat y^{i} - y^i) x^i_j$$&lt;/p&gt;

&lt;p&gt;Một vấn đề chúng ta gặp phải trong quá trình huấn luyện DL với SGD là chậm, siêu chậm. Do thuật toán phải cập nhật toàn bộ các tham số, nên số lượng phép tính và lượng tài nguyên phần cứng được sử dụng rất là nhiều. Rất nhiều các biến thể của SGD đã được đề xuất để giải quyết vấn đề trên.&lt;/p&gt;

&lt;h1 id=&#34;adam-adaptive-moment-estimation&#34;&gt;Adam - Adaptive Moment Estimation&lt;/h1&gt;

&lt;p&gt;Adam optimizer là một thuật toán kết hợp kỹ thuật  của RMS prop và momentum. Thuật toán sử dụng hai internal states momentum (m) và  squared momentum (v) của gradient cho các tham số. Sau mỗi batch huấn luyện, giá trị của m và v được cập nhật lại sử dụng exponential weighted averaging.&lt;/p&gt;

&lt;p&gt;Mã giải của việc cập nhật m và v&lt;/p&gt;

&lt;p&gt;$$m_t = \beta_1m_t-_1 + (1-\beta_1)g_t $$
 $$v_t  = \beta_2v_t-_1 + (1-\beta_2)g^2_t$$&lt;/p&gt;

&lt;p&gt;trong đó, beta được xem như là một siêu tham số. Công thức cập nhật theta như sau:&lt;/p&gt;

&lt;p&gt;$$\theta_t = \theta_t-_1 - \alpha\frac{m_t}{\sqrt{v_t}+ \epsilon }$$&lt;/p&gt;

&lt;p&gt;trong đó, alpha là learning rate, epsion là giá trị được thêm vào để ngăng việc chia cho 0&lt;/p&gt;

&lt;p&gt;Để việc descent  được thực hiện nhanh hơn, thuật toán đã sử dụng hai kỹ thuật:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tính  exponential moving average của giá trị đạo hàm lưu vào biến m và sử dụng nó là tử số của việc  cập nhật hướng. Với ý nghĩa là nếu m có giá trị lớn, thì việc descent đang đi đúng hướng và chúng ta cần bước nhảy lớn hơn để đi nhanh hơn. Tương tự, nếu giá trị m nhỏ, phần descent có thể không đi về hướng tối tiểu và chúng ta nên đi 1 bước nhỏ để thăm dò. Đây là phần momentum của thuật toán.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tính exponential moving average của bình phương gía trị đạo hàm lưu vào biến v và sử dụng nó là phần mẫu số của việc cập nhật hướng. Với ý nghĩa như sau: Giả sử gradient mang các giá trị dương, âm lẫn lộn, thì khi cộng các giá trị lại theo công thức tính m ta sẽ được  giá trị m gần số 0. Do âm dương lẫn lộn nên nó bị triệt tiêu lẫn nhau. Nhưng trong trường hợp này thì v sẽ mang giá trị lớn. Do đó, trong trường hợp này, chúng ta sẽ không hướng tới cực tiểu, chúng ta sẽ không muốn đi theo hướng đạo hàm trong trường hợp này. Chúng ta để v ở phần mẫu vì khi chia cho một giá trị cao, giá trị của  các phần cập nhật sẽ nhỏ, và khi v có giá trị thấp, phần cập nhật sẽ lớn. Đây chính là phần tối ưu RMSProp  của thuật toán.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ở đây, m được xem như là moment thứ nhất, v xem như là moment thứ hai, nên thuật toán có tên là &amp;ldquo;Adaptive moment estimation&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Để lý giải vì sao Adam lại hội tụ nhanh hơn so với SGD, chúng ta có thể giải thích như sau: Exponential weighted averaging cho chúng ta giá trị xấp xỉ gradient mượt hơn qua mỗi lần lặp, dẫn tới tăng tínhs dừng. Sau đó, việc chia cho căng bậc 2 của giá trị v làm số lước của chúng ta giảm mạnh khi phương sai của giá trị gradient tăng lên. Điều này , như giải thích ở trên, có nghĩa là, khi hướng đi của mô hình chỉ ra không rõ ràng, thuật toán Adam thực hiện các bước đi nhỏ coi như là thăm dò thôi. Và sẽ thực hiện các bước đi lớn, nhanh khi hướng đi rõ ràng.&lt;/p&gt;

&lt;p&gt;Thuật toán Adam hoạt động khá hiệu quả, nhưng bản thân nó cũng có những vấn đề. Tác giả của AdaBelief  đã chỉ ra một vài điểm không hiệu quả của thuật toán&lt;/p&gt;

&lt;h1 id=&#34;adabelief-optimizer-adapting-stepsizes-by-the-belief-in-observed-gradients&#34;&gt;AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/adam_error.jpg&#34; alt=&#34;Hình ảnh AdaBelief - Nguồn https://arxiv.org/pdf/2010.07468v5.pdf &#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hãy nhìn vào hình trên, ở mục đánh dấu là số 3, giá trị G lớn vì đường cong ở đoạn đó dốc. Giá trị v cũng lớn. Do đó, nếu sử dụng thuật toán Adam ở đây, bước đi sẽ rất nhỏ. Việc di chuyển một bước đi nhỏ ở đây sẽ làm chậm quá trình hội tụ và không cần thiết. Bởi vì chúng ta tin tưởng rằng chúng ta đang đi đúng hướng, và chúng ta cần một bước đi dài hơn.&lt;/p&gt;

&lt;p&gt;AdaBelief sửa lỗi này bằng một thay đổi nhỏ trong thuật toán của adam. Thay vì tính bình phương của gradient, AdaBelief  sẽ tính phương sai của gradient. Một sự thay đổi nhỏ nhưng mang lại giá trị to lớn.&lt;/p&gt;

&lt;p&gt;$$v_t  = \beta_2v_t-_1 + (1-\beta_2)g^2_t $$
$$s_t  = \beta_2v_t-_1 + (1-\beta_2)(g_t-m_t)^2$$&lt;/p&gt;

&lt;p&gt;Tác giả không dùng biến v nữa, mà thay bằng biến s.&lt;/p&gt;

&lt;p&gt;Với việc dùng biến s. Trong trường hợp trên, g lớn và m lớn, thì s sẽ nhỏ. Và khi s ở phần mẫu nhỏ, chúng ta sẽ có bước đi xa hơn. Ở đây, AdaBelief  đã giải quyết vấn đề&lt;/p&gt;

&lt;p&gt;Qua đây, chúng ta cũng có thể giải thích vì sao có chữ &amp;ldquo;belief&amp;rdquo; trong từ AdaBelief. Giá trị phương sai được tính dựa vào kỳ vọng của giá trị gradient.&lt;/p&gt;

&lt;p&gt;Một chú ý nhỏ ở đây là mục số 1 và mục số 3 được coi là cải tiến của Adam  so với momentum và SGD. Tất nhiên, AdaBelief cũng kế thừa mấy cái này.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ở mục đánh dấu số 1 trên hình, đường cong khá phẳng và giá trị đạo hàm gần như bằng 0. Nếu sử dụng SGD, chúng ta sẽ có một bước đi nhỏ. Trong khi đó, họ Adam sẽ cho chúng ta bước đi lớn hơn vì giá trị căng bậc hai của s hoặc v ở mẫu số sẽ cho ra một kết quả rất nhỏ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ở mục đánh dấu số 2, đường cong ở đây rất dốc và hẹp, g và delta g ở đây rất lớn, cho nên ở đây chúng ta cần một bước di chuyển nhỏ. Nếu sử dụng SGD hoặc momentum thì sẽ đi một bước đi rất lớn do nhân với một lượng moving averages lớn. Trong khi đó, với Adam hoặc AdaBelief, chúng ta sẽ có giá trị căng bậc hai của s hoặc v ở mẫu số lớn nên bước đi sẽ nhỏ hơn.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Về tốc độ hội tụ, tác giả có đề cập rõ và chi tiết trong bài báo, mình không đề cập lại nó nữa ở đây. Các bạn tự xem nhé.&lt;/p&gt;

&lt;h1 id=&#34;kết-luận&#34;&gt;Kết luận&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AdaBelief là thuật toán tối ưu hóa có nguồn gốc từ thuật toán Adam, không có thêm tham số ngoài, chỉ thay đổi 1 dòng code.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thuật toán đã tăng tốc độ hội tụ cũng như mức tổng quát hóa.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thuật toán thực hiện các bước đi dựa vào &amp;ldquo;belief&amp;rdquo; của hướng gradient ở thời điểm hiện tại.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thuật toán giải quyết vấn đề &amp;ldquo;Large gradient, small curvature&amp;rdquo; bằng cách xem xét biên độ và dấu của gradient.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nguồn:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.07468&#34;&gt;https://arxiv.org/abs/2010.07468&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/the-dl/understanding-the-new-adabelief-optimizer-2db70ef6de1e&#34;&gt;https://medium.com/the-dl/understanding-the-new-adabelief-optimizer-2db70ef6de1e&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/adabelief-optimizer-fast-as-adam-generalizes-as-good-as-sgd-71a919597af&#34;&gt;https://towardsdatascience.com/adabelief-optimizer-fast-as-adam-generalizes-as-good-as-sgd-71a919597af&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement Learning và tictactoe</title>
      <link>/blog/2020-12-26---tic-tac-toe/</link>
      <pubDate>Sun, 27 Dec 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-12-26---tic-tac-toe/</guid>
      <description>

&lt;h1 id=&#34;advantages-of-reinforcement-learning&#34;&gt;Advantages of Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;Trong khi trong các phương pháp lý thuyết trò chơi nói chung, ví dụ thuật toán min-max, thuật toán luôn giả định chúng ta có một đối thủ hoàn hảo, công việc phải thực hiện là tối đa hóa phần thưởng của mình và giảm thiểu phần thưởng của đối thủ ( tối đa hóa điểm của mình và tối thiểu hóa điểm của đối thủ), trong học củng cố, chúng ta không cần giả định đối thủ của chúng ta là 1 thiên tài xuất chúng, nhưng chung ta vẫn thu được mô hình với kết quả rất tốt.&lt;/p&gt;

&lt;p&gt;Bằng cách coi đối thủ là một phần của môi trường mà chúng ta có thể tương tác, sau một số lần lặp lại nhất định, đối thủ có thể lập kế hoạch trước mà không cần chúng ta phải làm gì cả. Ưu điểm của phương pháp này là giảm số lượng không gian tìm kiếm và giảm số phép toán suy luận phải thực hiện, nhưng nó có thể đạt được kỹ năng hiện đại chỉ bằng cách thử và học.&lt;/p&gt;

&lt;p&gt;Trong bài viết này, chúng ta sẽ làm các công việc sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Thứ nhất, huấn luyện mô hình cho 2 máy đấu với nhau mà thu được các trọng số cần thiết.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thứ hai, cho người đánh với máy&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Để hình thành bài toán học củng cố Reinforcement Learning , chúng ta cần  phải xác định rõ 3 thành phần chính:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;State&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Action&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reward&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Với:&lt;/p&gt;

&lt;p&gt;State chính là bàn cờ với các nước đi của các người chơi. Chúng ta sẽ tạo một bàn cờ có kích thước 3x3, giá trị của mỗi ô cờ đều là 0. Vị trí người chơi 1 đặt quân sẽ được gán là 1. Vị trí người chơi 2 đặt quân sẽ được gán là -1.&lt;/p&gt;

&lt;p&gt;Action là vị trí người chơi sẽ đi quân khi biết state hiện tại (nghĩa là biết đối thủ đi nước nào, và có những nước nào hiện đang trên bàn cờ).&lt;/p&gt;

&lt;p&gt;Reward: mang giá trị 0 hoặc 1. Khi kết thúc game sẽ trả về giá trị cho reward.&lt;/p&gt;

&lt;p&gt;Ở phần dưới đây, mình sẽ note lại code và sẽ comment trong code để cho rõ ý&lt;/p&gt;

&lt;h1 id=&#34;thiết-lập-bàn-cờ&#34;&gt;Thiết lập bàn cờ&lt;/h1&gt;

&lt;h2 id=&#34;khởi-tạo-bàn-cờ&#34;&gt;Khởi tạo bàn cờ&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def __init__(self, p1, p2):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.p1 = p1
        self.p2 = p2
        self.isEnd = False
        self.boardHash = None
        # init p1 plays first
        self.playerSymbol = 1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta sẽ tạo một bàn cờ có kích thước 3x3, 2 biến người chơi. Người 1 là người chơi đầu tiên.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Trả về danh sách các nước có thể đi
def availablePositions(self):
        positions = []
        for i in range(BOARD_ROWS):
            for j in range(BOARD_COLS):
                if self.board[i, j] == 0:
                    positions.append((i, j))  # need to be tuple
        return positions

# Cập nhật lại lên bàn cờ vị trí của người chơi đặt quân

def updateState(self, position):
    self.board[position] = self.playerSymbol
    # switch to another player
    self.playerSymbol = -1 if self.playerSymbol == 1 else 1
    
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kiểm-tra-reward&#34;&gt;Kiểm tra Reward&lt;/h2&gt;

&lt;p&gt;Sau mỗi nước đi của các kỳ thủ, chúng ta cần 1 hàm để kiểm tra xem kỳ thủ thắng hay thua và trả về kết quả cho reward như đề cập ở trên&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def winner(self):

    # Kiểm tra theo dòng
    
    for i in range(BOARD_ROWS):
        if sum(self.board[i, :]) == 3:
            self.isEnd = True
            return 1
        if sum(self.board[i, :]) == -3:
            self.isEnd = True
            return -1
    # kiểm tra theo cột
    
    for i in range(BOARD_COLS):
        if sum(self.board[:, i]) == 3:
            self.isEnd = True
            return 1
        if sum(self.board[:, i]) == -3:
            self.isEnd = True
            return -1
            
    # kiểm tra theo đường chéo chính và theo đường chéo phụ
    
    diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)]) # đường chéo chính
    
    diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)]) # đường chéo phụ
    
    diag_sum = max(abs(diag_sum1), abs(diag_sum2)) # lấy trị tuyệt đối của các nước đi, nếu bằng 3 nghĩa là có người chơi chiến thắng
    
    if diag_sum == 3:
        self.isEnd = True
        if diag_sum1 == 3 or diag_sum2 == 3:
            return 1
        else:
            return -1

    # Kiểm tra xem còn nước đi hay không
    if len(self.availablePositions()) == 0:
        self.isEnd = True
        return 0
        
    # not end
    self.isEnd = False
    return None

# only when game ends
def giveReward(self):
    result = self.winner()
    # backpropagate reward
    if result == 1:
        self.p1.feedReward(1)
        self.p2.feedReward(0)
    elif result == -1:
        self.p1.feedReward(0)
        self.p2.feedReward(1)
    else:
        self.p1.feedReward(0.1)
        self.p2.feedReward(0.5)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây có một lưu ý. Khi cờ hòa thì chúng ta cũng xem rằng người đi trước thua, nên hệ số lúc cờ hòa sẽ là 0.1-0.5. Các bạn có thể thiết lập một giá trị khác, ví dụ 0.2-0.5 hoặc 0.5-0.5 tùy thích.&lt;/p&gt;

&lt;h1 id=&#34;thiết-lập-người-chơi&#34;&gt;Thiết lập người chơi&lt;/h1&gt;

&lt;p&gt;Người chơi cần có các phương thức sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Chọn nước đi dựa trên trạng thái hiện tại của bàn cờ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lưu lại trạng thái của ván cờ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cập nhật lại giá trị trạng thái sau mỗi ván.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lưu và load các trọng số lên.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;khởi-tạo&#34;&gt;Khởi tạo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def __init__(self, name, exp_rate=0.2):
        self.name = name
        self.states = []  # record all positions taken
        self.lr = 0.2
        self.exp_rate = exp_rate
        self.decay_gamma = 0.9
        self.states_value = {}  # state -&amp;gt; value

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;chọn-nước-đi&#34;&gt;Chọn nước đi&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def chooseAction(self, positions, current_board, symbol):
    randValue = np.random.uniform(0, 1)
    value_max = value = -999
    if  randValue&amp;gt; self.exp_rate:
        
        for p in positions:
            next_board = current_board.copy()
            next_board[p] = symbol
            next_boardHash = self.getHash(next_board)
            value = -999 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)
            # print(&amp;quot;value&amp;quot;, value)
            if value &amp;gt;= value_max:
                value_max = value
                action = p

    if  value_max == -999 :
        # take random action
        idx = np.random.choice(len(positions))
        action = positions[idx]
    
    # print(&amp;quot;{} takes action {}&amp;quot;.format(self.name, action))
    return action

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cập-nhật-trạng-thái&#34;&gt;Cập nhật trạng thái&lt;/h2&gt;

&lt;p&gt;Chúng ta sẽ cập nhật trạng thái với công thức sau&lt;/p&gt;

&lt;p&gt;$$ V(S_t) = V(S&lt;em&gt;t) + \alpha [V(S&lt;/em&gt;{t+1}) - V(S_t)]   $$&lt;/p&gt;

&lt;p&gt;Diễn giải ra tiếng việt, giá trị của trạng thái tại thời điểm t bằng giá trị tại thời điểm hiện tại cộng với độ lệch của trạng thái hiện tại và trạng thái tiếp theo nhân với một hệ số học alpha.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# at the end of game, backpropagate and update states value
def feedReward(self, reward):
        for st in reversed(self.states):
            if self.states_value.get(st) is None:
                self.states_value[st] = 0
            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])
            reward = self.states_value[st]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;huấn-luyện-mô-hình&#34;&gt;Huấn luyện mô hình&lt;/h2&gt;

&lt;p&gt;Phần này nằm trong lớp State. Chúng ta sẽ lần lượt đi qua các quá trình luân phiên nhau giữa người chơi 1 và người chơi 2&lt;/p&gt;

&lt;p&gt;người chơi chọn nước có thể đi -&amp;gt; cập nhật trạng thái -&amp;gt; kiểm tra thắng/thua -&amp;gt; người chơi chọn nước có thể đi &amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def play(self, rounds=100):
    for i in range(rounds):
        if i % 1000 == 0:
            print(&amp;quot;Rounds {}&amp;quot;.format(i))
        while not self.isEnd:
            # Player 1
            positions = self.availablePositions()
            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
            # take action and upate board state
            self.updateState(p1_action)
            board_hash = self.getHash()
            self.p1.addState(board_hash)
            # check board status if it is end

            win = self.winner()
            if win is not None:
                # self.showBoard()
                # ended with p1 either win or draw
                self.giveReward()
                self.p1.reset()
                self.p2.reset()
                self.reset()
                break

            else:
                # Player 2
                positions = self.availablePositions()
                p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)
                self.updateState(p2_action)
                board_hash = self.getHash()
                self.p2.addState(board_hash)

                win = self.winner()
                if win is not None:
                    # self.showBoard()
                    # ended with p2 either win or draw
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi huấn luyện 100 ngàn lần, chúng ta sẽ chơi với máy, chỉ là 1 thay đổi nhỏ trong hàm chooseAction là thay vì lấy nước đi có trọng số lớn nhất, chúng ta sẽ cho người dùng nhập từ bàn phím dòng và cột vào&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

def chooseAction(self, positions):
        while True:
            row = int(input(&amp;quot;Input your action row:&amp;quot;))
            col = int(input(&amp;quot;Input your action col:&amp;quot;))
            action = (row, col)
            if action in positions:
                return action

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và sửa lại hàm play một chút, bỏ loop 100k lần đi, bỏ gọi hàm cập nhật thưởng và bỏ các hàm reset đi&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

# play with human
def play2(self):
    while not self.isEnd:
        # Player 1
        positions = self.availablePositions()
        p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
        # take action and upate board state
        self.updateState(p1_action)
        self.showBoard()
        # check board status if it is end
        win = self.winner()
        if win is not None:
            if win == 1:
                print(self.p1.name, &amp;quot;wins!&amp;quot;)
            else:
                print(&amp;quot;tie!&amp;quot;)
            self.reset()
            break

        else:
            # Player 2
            positions = self.availablePositions()
            p2_action = self.p2.chooseAction(positions)

            self.updateState(p2_action)
            self.showBoard()
            win = self.winner()
            if win is not None:
                if win == -1:
                    print(self.p2.name, &amp;quot;wins!&amp;quot;)
                else:
                    print(&amp;quot;tie!&amp;quot;)
                self.reset()
                break

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mã nguồn hoàn chỉnh của chương trình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import pickle

BOARD_ROWS = 3
BOARD_COLS = 3


class State:
    def __init__(self, p1, p2):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.p1 = p1
        self.p2 = p2
        self.isEnd = False
        self.boardHash = None
        # init p1 plays first
        self.playerSymbol = 1

    # get unique hash of current board state
    def getHash(self):
        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))
        return self.boardHash

    def winner(self):
        # row
        for i in range(BOARD_ROWS):
            if sum(self.board[i, :]) == 3:
                self.isEnd = True
                return 1
            if sum(self.board[i, :]) == -3:
                self.isEnd = True
                return -1
        # col
        for i in range(BOARD_COLS):
            if sum(self.board[:, i]) == 3:
                self.isEnd = True
                return 1
            if sum(self.board[:, i]) == -3:
                self.isEnd = True
                return -1
        # diagonal
        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])
        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])
        diag_sum = max(abs(diag_sum1), abs(diag_sum2))
        if diag_sum == 3:
            self.isEnd = True
            if diag_sum1 == 3 or diag_sum2 == 3:
                return 1
            else:
                return -1

        # tie
        # no available positions
        if len(self.availablePositions()) == 0:
            self.isEnd = True
            return 0
        # not end
        self.isEnd = False
        return None

    def availablePositions(self):
        positions = []
        for i in range(BOARD_ROWS):
            for j in range(BOARD_COLS):
                if self.board[i, j] == 0:
                    positions.append((i, j))  # need to be tuple
        return positions

    def updateState(self, position):
        self.board[position] = self.playerSymbol
        # switch to another player
        self.playerSymbol = -1 if self.playerSymbol == 1 else 1

    # only when game ends
    def giveReward(self):
        result = self.winner()
        # backpropagate reward
        if result == 1:
            self.p1.feedReward(1)
            self.p2.feedReward(0)
        elif result == -1:
            self.p1.feedReward(0)
            self.p2.feedReward(1)
        else:
            self.p1.feedReward(0.1)
            self.p2.feedReward(0.5)

    # board reset
    def reset(self):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.boardHash = None
        self.isEnd = False
        self.playerSymbol = 1

    def play(self, rounds=100):
        for i in range(rounds):
            if i % 1000 == 0:
                print(&amp;quot;Rounds {}&amp;quot;.format(i))
            while not self.isEnd:
                # Player 1
                positions = self.availablePositions()
                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
                # take action and upate board state
                self.updateState(p1_action)
                board_hash = self.getHash()
                self.p1.addState(board_hash)
                # check board status if it is end

                win = self.winner()
                if win is not None:
                    # self.showBoard()
                    # ended with p1 either win or draw
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

                else:
                    # Player 2
                    positions = self.availablePositions()
                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)
                    self.updateState(p2_action)
                    board_hash = self.getHash()
                    self.p2.addState(board_hash)

                    win = self.winner()
                    if win is not None:
                        # self.showBoard()
                        # ended with p2 either win or draw
                        self.giveReward()
                        self.p1.reset()
                        self.p2.reset()
                        self.reset()
                        break
            

    # play with human
    def play2(self):
        while not self.isEnd:
            # Player 1
            positions = self.availablePositions()
            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
            # take action and upate board state
            self.updateState(p1_action)
            self.showBoard()
            # check board status if it is end
            win = self.winner()
            if win is not None:
                if win == 1:
                    print(self.p1.name, &amp;quot;wins!&amp;quot;)
                else:
                    print(&amp;quot;tie!&amp;quot;)
                self.reset()
                break

            else:
                # Player 2
                positions = self.availablePositions()
                p2_action = self.p2.chooseAction(positions)

                self.updateState(p2_action)
                self.showBoard()
                win = self.winner()
                if win is not None:
                    if win == -1:
                        print(self.p2.name, &amp;quot;wins!&amp;quot;)
                    else:
                        print(&amp;quot;tie!&amp;quot;)
                    self.reset()
                    break
        

    def showBoard(self):
        # p1: x  p2: o
        for i in range(0, BOARD_ROWS):
            print(&#39;-------------&#39;)
            out = &#39;| &#39;
            for j in range(0, BOARD_COLS):
                token = &amp;quot;&amp;quot;
                if self.board[i, j] == 1:
                    token = &#39;x&#39;
                if self.board[i, j] == -1:
                    token = &#39;o&#39;
                if self.board[i, j] == 0:
                    token = &#39; &#39;
                out += token + &#39; | &#39;
            print(out)
        print(&#39;-------------&#39;)


class Player:
    def __init__(self, name, exp_rate=0.3):
        self.name = name
        self.states = []  # record all positions taken
        self.lr = 0.3
        self.exp_rate = exp_rate
        self.decay_gamma = 0.9
        self.states_value = {}  # state -&amp;gt; value

    def getHash(self, board):
        boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS))
        return boardHash

    def chooseAction(self, positions, current_board, symbol):
        randValue = np.random.uniform(0, 1)
        value_max = value = -999
        if  randValue&amp;gt; self.exp_rate:
            
            for p in positions:
                next_board = current_board.copy()
                next_board[p] = symbol
                next_boardHash = self.getHash(next_board)
                value = -999 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)
                # print(&amp;quot;value&amp;quot;, value)
                if value &amp;gt;= value_max:
                    value_max = value
                    action = p

        if  value_max == -999 :
            # take random action
            idx = np.random.choice(len(positions))
            action = positions[idx]
        
        # print(&amp;quot;{} takes action {}&amp;quot;.format(self.name, action))
        return action

    # append a hash state
    def addState(self, state):
        self.states.append(state)

    # at the end of game, backpropagate and update states value
    def feedReward(self, reward):
        for st in reversed(self.states):
            if self.states_value.get(st) is None:
                self.states_value[st] = 0
            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])
            reward = self.states_value[st]

    def reset(self):
        self.states = []

    def savePolicy(self):
        fw = open(&#39;policy_&#39; + str(self.name), &#39;wb&#39;)
        pickle.dump(self.states_value, fw)
        fw.close()

    def loadPolicy(self, file):
        fr = open(file, &#39;rb&#39;)
        self.states_value = pickle.load(fr)
        fr.close()


class HumanPlayer:
    def __init__(self, name):
        self.name = name

    def chooseAction(self, positions):
        while True:
            row = int(input(&amp;quot;Input your action row:&amp;quot;))
            col = int(input(&amp;quot;Input your action col:&amp;quot;))
            action = (row, col)
            if action in positions:
                return action

    # append a hash state
    def addState(self, state):
        pass

    # at the end of game, backpropagate and update states value
    def feedReward(self, reward):
        pass

    def reset(self):
        pass


if __name__ == &amp;quot;__main__&amp;quot;:
    # training
    p1 = Player(&amp;quot;p1&amp;quot;)
    p2 = Player(&amp;quot;p2&amp;quot;)

    st = State(p1, p2)
    print(&amp;quot;training...&amp;quot;)
    st.play(100000)

    p1.savePolicy()

    # play with human
    p1 = Player(&amp;quot;computer&amp;quot;, exp_rate=0)
    p1.loadPolicy(&amp;quot;policy_p1&amp;quot;)

    p2 = HumanPlayer(&amp;quot;human&amp;quot;)

    st = State(p1, p2)
    st.play2()



&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nguồn&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reinforcement Learning: An Introduction phiên bản 2 của Richard S. Sutton and Andrew G. Barto&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542&#34;&gt;https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Xây dựng game xếp gạch bằng opencv và python</title>
      <link>/blog/2020-12-25---tetric/</link>
      <pubDate>Sat, 26 Dec 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-12-25---tetric/</guid>
      <description>

&lt;h1 id=&#34;mã-nguồn&#34;&gt;Mã nguồn&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import cv2
import numpy as np
from random import choice

def getColor():
    lstColor = [[255,64,64],[255,165,0],[255,244,79],[102,255,0],[172,229,238],[148,87,235],[148,87,235],[241,156,187]]
    return choice(lstColor)

def getInfo(piece):
    if piece == &amp;quot;&amp;quot;:
        coords = np.array([[0, 0]])
    elif piece == &amp;quot;I&amp;quot;:
        coords = np.array([[0, 3], [0, 4], [0, 5], [0, 6]])
    elif piece == &amp;quot;T&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [1, 5], [0, 4]])
    elif piece == &amp;quot;L&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [1, 5], [0, 5]])
    elif piece == &amp;quot;J&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [1, 5], [0, 3]])
    elif piece == &amp;quot;S&amp;quot;:
        coords = np.array([[1, 5], [1, 4], [0, 3], [0, 4]])
    elif piece == &amp;quot;Z&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [0, 4], [0, 5]])
    else:
        coords = np.array([[0, 4], [0, 5], [1, 4], [1, 5]])
    
    return coords, getColor()

def display(board, coords, color, next_info, held_info, score, SPEED):
    # Generates the display
    
    border = np.uint8(127 - np.zeros([20, 1, 3]))
    border_ = np.uint8(127 - np.zeros([1, 23, 3]))
    
    dummy = board.copy()
    dummy[coords[:,0], coords[:,1]] = color
    
    right = np.uint8(np.zeros([20, 10, 3]))
    right[next_info[0][:,0] + 2, next_info[0][:,1]] = next_info[1]
    
    dummy = np.concatenate(( border, dummy, border, right, border), 1)
    dummy = np.concatenate((border_, dummy, border_), 0)
    dummy = dummy.repeat(20, 0).repeat(20, 1)
    dummy = cv2.putText(dummy, str(score), (325, 150), cv2.FONT_HERSHEY_DUPLEX, 1, [0, 0, 255], 2)
    
    # Instructions for the player
    index_pos = 300
    x_index_pos = 300
    dummy = cv2.putText(dummy, &amp;quot;A - left&amp;quot;, (x_index_pos, index_pos), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    dummy = cv2.putText(dummy, &amp;quot;D - right&amp;quot;, (x_index_pos, index_pos+25), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    dummy = cv2.putText(dummy, &amp;quot;S - drain&amp;quot;, (x_index_pos, index_pos+50), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    dummy = cv2.putText(dummy, &amp;quot;W - rotate&amp;quot;, (x_index_pos, index_pos+75), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    # dummy = cv2.putText(dummy, &amp;quot;J - rotate left&amp;quot;, (45, 300), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255])
    # dummy = cv2.putText(dummy, &amp;quot;L - rotate right&amp;quot;, (45, 325), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255])
    # dummy = cv2.putText(dummy, &amp;quot;I - hold&amp;quot;, (45, 350), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255])
    
    cv2.imshow(&amp;quot;Tetris&amp;quot;, dummy)
    key = cv2.waitKey(int(1000/SPEED))
    
    return key

def getNextPiece():
    next_piece = choice([&amp;quot;O&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;Z&amp;quot;, &amp;quot;L&amp;quot;, &amp;quot;J&amp;quot;, &amp;quot;T&amp;quot;])

    return next_piece

SPEED = 1 # Controls the speed of the tetris pieces

# Make a board

board = np.uint8(np.zeros([20, 10, 3]))

# Initialize some variables

quit = False
place = False
drop = False
switch = False
held_piece = &amp;quot;&amp;quot;
flag = 0
score = 0
next_piece =&amp;quot;&amp;quot;
current_piece = &amp;quot;&amp;quot;
# All the tetris pieces



if __name__ == &amp;quot;__main__&amp;quot;:
    next_piece = getNextPiece()
    while not quit:
        # Check if user wants to swap held and current pieces
        if switch:
           # swap held_piece and current_piece
            held_piece, current_piece = current_piece, held_piece
            switch = False
        else:
            # Generates the next piece and updates the current piece
            current_piece = next_piece
            next_piece = getNextPiece()

        if flag &amp;gt; 0:
            flag -= 1

        # Determines the color and position of the current, next, and held pieces
        
        held_info = getInfo(held_piece)

        next_info = getInfo(next_piece)

        coords, color = getInfo(current_piece)
        if current_piece == &amp;quot;I&amp;quot;:
            top_left = [-2, 3]

        if not np.all(board[coords[:,0], coords[:,1]] == 0):
            break

        while True:
            # Shows the board and gets the key press
            key = display(board, coords, color, next_info, held_info, score, SPEED)
            # Create a copy of the position
            dummy = coords.copy()
            print(&amp;quot;speed &amp;quot;,SPEED, &amp;quot;key &amp;quot;,key,&amp;quot; &amp;quot;, ord(&amp;quot;s&amp;quot;))

            if key == ord(&amp;quot;s&amp;quot;):
                drop = True

            elif key == ord(&amp;quot;a&amp;quot;):
                # Moves the piece left if it isn&#39;t against the left wall
                if np.min(coords[:,1]) &amp;gt; 0:
                    coords[:,1] -= 1
                if current_piece == &amp;quot;I&amp;quot;:
                    top_left[1] -= 1
            elif key == ord(&amp;quot;d&amp;quot;):
                # Moves the piece right if it isn&#39;t against the right wall
                if np.max(coords[:,1]) &amp;lt; 9:
                    coords[:,1] += 1
                    if current_piece == &amp;quot;I&amp;quot;:
                        top_left[1] += 1
            # elif key == ord(&amp;quot;j&amp;quot;) or key == ord(&amp;quot;l&amp;quot;):
            #         # Rotation mechanism
            #     # arr is the array of nearby points which get rotated and pov is the indexes of the blocks within arr
                
            #     if current_piece != &amp;quot;I&amp;quot; and current_piece != &amp;quot;O&amp;quot;:
            #         if coords[1,1] &amp;gt; 0 and coords[1,1] &amp;lt; 9:
            #             arr = coords[1] - 1 + np.array([[[x, y] for y in range(3)] for x in range(3)])
            #             pov = coords - coords[1] + 1
                        
            #     elif current_piece == &amp;quot;I&amp;quot;:
            #         # The straight piece has a 4x4 array, so it needs seperate code
                    
            #         arr = top_left + np.array([[[x, y] for y in range(4)] for x in range(4)])
            #         pov = np.array([np.where(np.logical_and(arr[:,:,0] == pos[0], arr[:,:,1] == pos[1])) for pos in coords])
            #         pov = np.array([k[0] for k in np.swapaxes(pov, 1, 2)])
                
            #     # Rotates the array and repositions the piece to where it is now
                
            #     if current_piece != &amp;quot;O&amp;quot;:
            #         if key == ord(&amp;quot;j&amp;quot;):
            #             arr = np.rot90(arr, -1)
            #         else:
            #             arr = np.rot90(arr)
            #         coords = arr[pov[:,0], pov[:,1]]
            
            elif key == ord(&amp;quot;w&amp;quot;):
                        # Rotation mechanism
                # arr is the array of nearby points which get rotated and pov is the indexes of the blocks within arr
                
                if current_piece != &amp;quot;I&amp;quot; and current_piece != &amp;quot;O&amp;quot;:
                    if coords[1,1] &amp;gt; 0 and coords[1,1] &amp;lt; 9:
                        arr = coords[1] - 1 + np.array([[[x, y] for y in range(3)] for x in range(3)])
                        pov = coords - coords[1] + 1
                        
                elif current_piece == &amp;quot;I&amp;quot;:
                    # The straight piece has a 4x4 array, so it needs seperate code
                    
                    arr = top_left + np.array([[[x, y] for y in range(4)] for x in range(4)])
                    pov = np.array([np.where(np.logical_and(arr[:,:,0] == pos[0], arr[:,:,1] == pos[1])) for pos in coords])
                    pov = np.array([k[0] for k in np.swapaxes(pov, 1, 2)])
                
                # Rotates the array and repositions the piece to where it is now
                
                if current_piece != &amp;quot;O&amp;quot;:
                    if key == ord(&amp;quot;j&amp;quot;):
                        arr = np.rot90(arr, -1)
                    else:
                        arr = np.rot90(arr)
                    coords = arr[pov[:,0], pov[:,1]]
                # Hard drop set to true
                # drop = True
            # elif key == ord(&amp;quot;i&amp;quot;):
            #     # Goes out of the loop and tells the program to switch held and current pieces
            #     if flag == 0:
            #         if held_piece == &amp;quot;&amp;quot;:
            #             held_piece = current_piece
            #         else:
            #             switch = True
            #         flag = 2
            #         break
            elif key == 8 or key == 27:
                quit = True
                break

            # Checks if the piece is overlapping with other pieces or if it&#39;s outside the board, and if so, changes the position to the position before anything happened
            
            if np.max(coords[:,0]) &amp;lt; 20 and np.min(coords[:,0]) &amp;gt;= 0:
                if not (current_piece == &amp;quot;I&amp;quot; and (np.max(coords[:,1]) &amp;gt;= 10 or np.min(coords[:,1]) &amp;lt; 0)):
                    if not np.all(board[coords[:,0], coords[:,1]] == 0):
                        coords = dummy.copy()
                else:
                    coords = dummy.copy()
            else:
                coords = dummy.copy()
            
            if drop:
                    # Every iteration of the loop moves the piece down by 1 and if the piece is resting on the ground or another piece, then it stops and places it
                
                while not place:
                    if np.max(coords[:,0]) != 19:
                        # Checks if the piece is resting on something
                        for pos in coords:
                            if not np.array_equal(board[pos[0] + 1, pos[1]], [0, 0, 0]):
                                place = True
                                break
                    else:
                        # If the position of the piece is at the ground level, then it places
                        place = True
                    
                    if place:
                        break
                    
                    # Keeps going down and checking when the piece needs to be placed
                    
                    coords[:,0] += 1
                    
                    if current_piece == &amp;quot;I&amp;quot;:
                        top_left[0] += 1
                        
                drop = False

            else:
                    # Checks if the piece needs to be placed
                if np.max(coords[:,0]) != 19:
                    for pos in coords:
                        if not np.array_equal(board[pos[0] + 1, pos[1]], [0, 0, 0]):
                            place = True
                            break
                else:
                    place = True
                
            if place:
                # Places the piece where it is on the board
                for pos in coords:
                    board[tuple(pos)] = color
                    
                # Resets place to False
                place = False
                break

            # Moves down by 1

            coords[:,0] += 1
            if current_piece == &amp;quot;I&amp;quot;:
                top_left[0] += 1

        # Clears lines and also counts how many lines have been cleared and updates the score
        
        lines = 0
                
        for line in range(20):
            if np.all([np.any(pos != 0) for pos in board[line]]):
                lines += 1
                board[1:line+1] = board[:line]
                        
        
        score += lines*10

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mã nguồn này được kế thừa từ bài viết &lt;a href=&#34;https://www.learnopencv.com/tetris-with-opencv-python/&#34;&gt;https://www.learnopencv.com/tetris-with-opencv-python/&lt;/a&gt; và mình có modify lại theo sở thích cá nhân của mình. Còn một số bug mà mình chưa fix hết. Bạn đọc nào ghé ngang có đóng góp gì thì để lại comment giúp mình hen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ngưỡng (thresholding) trong opencv</title>
      <link>/blog/2020-12-24-thresholding/</link>
      <pubDate>Fri, 25 Dec 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-12-24-thresholding/</guid>
      <description>

&lt;h1 id=&#34;giá-trị-ngưỡng&#34;&gt;Giá trị ngưỡng:&lt;/h1&gt;

&lt;p&gt;Nói theo kiểu lúa hóa, trong opencv, ngưỡng là một số nằm trong đoạn từ 0 đến 255. Giá trị ngưỡng sẽ chia tách giá trị độ xám của ảnh thành 2 miền riêng biệt. Miền thứ nhất là tập hợp các điểm ảnh có giá trị nhỏ hơn giá trị ngưỡng. Miền thứ hai là tập hợp các các điểm ảnh có giá trị lớn hơn hoặc bằng giá trị ngưỡng.&lt;/p&gt;

&lt;p&gt;Đầu vào của một thuật toán phân ngưỡng trong opencv thường có input là ảnh nguồn (source image) và giá trị ngưỡng. Đầu ra là ảnh đích đã được phân ngưỡng (destination image). Một số thuật toán phân ngưỡng sẽ kèm thêm vài giá trị râu ria khác nữa, chúng ta sẽ không quan tâm đến chúng&lt;/p&gt;

&lt;p&gt;Mã giải của thuật toán phân ngưỡng:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;if src[i] &amp;gt;= T:
    dest[i] = MAXVAL
else:
    dest [i] = 0

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Có rất nhiều thuật toán phân ngưỡng dựa trên cách chúng ta xác định ngưỡng. Chúng ta sẽ tìm hiểu lần lượt các thuật toán trên.&lt;/p&gt;

&lt;h1 id=&#34;thuật-toán-simple-thresholding&#34;&gt;Thuật toán Simple Thresholding&lt;/h1&gt;

&lt;p&gt;Simple Thresholding thực hiện phân ngưỡng bằng cách thay thế giá trị lớn hơn hoặc bằng và giá trị bé hơn giá trị ngưỡng bằng một giá trị mới. Cụ thể chúng ta có thể xem mã nguồn bên dưới&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;
import cv2
import numpy as np
from matplotlib import pyplot as plt

img = cv2.imread(&#39;gradient.png&#39;,0)
ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC)
ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO)
ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV)

titles = [&#39;Original Image&#39;,&#39;BINARY&#39;,&#39;BINARY_INV&#39;,&#39;TRUNC&#39;,&#39;TOZERO&#39;,&#39;TOZERO_INV&#39;]
images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]

for i in xrange(6):
    plt.subplot(2,3,i+1),plt.imshow(images[i],&#39;gray&#39;)
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])

plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/_images/threshold.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình ảnh và thuật toán của mô hình được lấy từ trang opencv-python-tutroals.readthedocs.io&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ở đoạn code trên, chúng ta thiết lập giá trị ngưỡng là 127, với các điểm ảnh có giá trị lớn hơn hoặc bằng 127, chúng ta sẽ gán lại giá trị của nó thành 255. Và các điểm ảnh có giá trị bé hơn 127 sẽ được gán bằng 0 (mặc định).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;

double cv::threshold    (   InputArray  src,
OutputArray     dst,
double  thresh,
double  maxval,
int     type 
)   

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thuật toán sample thresholding của opencv còn có 1 tham số nữa khá quan trọng nữa là loại ngưỡng (type). Hiện tại lúc mình viết bài viết này thì opencv hỗ trợ  8 loại là: THRESH_BINARY,  THRESH_BINARY_INV, THRESH_TRUNC, THRESH_TOZERO, THRESH_TOZERO_INV, THRESH_MASK, THRESH_OTSU, THRESH_TRIANGLE. Ý nghĩa của từng loại như sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;THRESH_BINARY: Có thể dịch là ngưỡng nhị phân. Ý nghĩa y hệt những gì mình đề cập ở trên.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_BINARY_INV: Ngưỡng nhị phân đảo ngược. Có thể hiểu là nó sẽ đảo ngược lại kết quả của THRESH_BINARY.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TRUNC: Những giá trị điểm ảnh  bé hơn ngưỡng sẽ giữ nguyên giá trị, những điểm ảnh lớn hơn hoặc ngưỡng sẽ được gán lại là maxvalue.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TOZERO: Những điểm ảnh bé hơn ngưỡng sẽ bị gán thành 0, những điểm còn lại giữ nguyên.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TOZERO_INV: Những điểm ảnh nhỏ hơn giá trị ngưỡng sẽ được giữ nguyên, những điểm ảnh còn lại sẽ bị gán thành 0.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_MASK: Ở bạn opencv4, hầu như không được xài.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_OTSU: Sử dụng thuật toán Otsu để xác định giá trị ngưỡng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TRIANGLE: Sử dụng thuật toán Triangle  để xác định giá trị ngưỡng.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Giá trị 127 là giá trị trung bình cộng của 0 và 255 làm tròn xuống. Giá trị ngưỡng của thuật toán này đòi hỏi người sử dụng phải có mức độ hiểu biết nhất định về các loại ảnh mình đang xử lý để chọn ngưỡng cho phù hợp.&lt;/p&gt;

&lt;h1 id=&#34;adaptive-thresholding&#34;&gt;Adaptive Thresholding&lt;/h1&gt;

&lt;p&gt;Thuật toán simple thresholding hoạt động khá tốt. Tuy nhiên, nó có 1 nhược điểm là giá trị ngưỡng bị/được gán toàn cục. Thực tế khi chụp, hình ảnh chúng ta nhận được thường bị ảnh hưởng của nhiễu, ví dụ như là bị phơi sáng, bị đèn flask, &amp;hellip;&lt;/p&gt;

&lt;p&gt;Một trong những cách được sử dụng để giải quyết vấn đề trên là chia nhỏ bức ảnh thành những vùng nhỏ (region), và đặt giá trị ngưỡng trên những vùng nhỏ đó -&amp;gt; adaptive thresholding ra đời. Opencv cung cấp cho chúng ta hai cách xác định những vùng nhỏ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
img = cv.imread(&#39;sudoku.png&#39;,0)
img = cv.medianBlur(img,5)
ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\
            cv.THRESH_BINARY,11,2)
th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\
            cv.THRESH_BINARY,11,2)
titles = [&#39;Original Image&#39;, &#39;Global Thresholding (v = 127)&#39;,
            &#39;Adaptive Mean Thresholding&#39;, &#39;Adaptive Gaussian Thresholding&#39;]
images = [img, th1, th2, th3]
for i in xrange(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],&#39;gray&#39;)
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://docs.opencv.org/master/ada_threshold.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình ảnh và thuật toán của mô hình được lấy từ trang docs.opencv.org&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;

void cv::adaptiveThreshold  (   InputArray  src,
OutputArray     dst,
double  maxValue,
int     adaptiveMethod,
int     thresholdType,
int     blockSize,
double  C 
)   

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây:&lt;/p&gt;

&lt;p&gt;blockSize: Kích thước của vùng, bắt buộc phải là một số lẻ lớn hơn 0.&lt;/p&gt;

&lt;p&gt;C: hằng số, giá trị từ -255 đến 255. Có thể gán C bằng 0 để đỡ rối.&lt;/p&gt;

&lt;p&gt;adaptiveMethod nhận vào một trong hai giá trị là cv.ADAPTIVE_THRESH_MEAN_C và cv.ADAPTIVE_THRESH_GAUSSIAN_C, đó là các phương pháp tính ngưỡng.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ADAPTIVE_THRESH_MEAN_C: Tính trung bình các láng giềng xung quanh điểm cần xét trong vùng blockSize * blockSize trừ đi giá trị hằng số C.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ADAPTIVE_THRESH_GAUSSIAN_C: Nhân giá trị xung quanh điểm cần xét với trọng số gauss rồi tính trung bình của nó, sau đó trừ đi giá trị hằng số C.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;thresholdType: Tương tự như Simple Thresholding đã trình bày ở trên.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã quan tâm và theo dõi bài viết, hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Tham khảo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-1-simple-thresholding/&#34;&gt;https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-1-simple-thresholding/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.learnopencv.com/opencv-threshold-python-cpp/&#34;&gt;https://www.learnopencv.com/opencv-threshold-python-cpp/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@anupriyam/basic-image-thresholding-in-opencv-5af9020f2472&#34;&gt;https://medium.com/@anupriyam/basic-image-thresholding-in-opencv-5af9020f2472&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Contour</title>
      <link>/blog/2019-05-26-contours/</link>
      <pubDate>Fri, 24 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-26-contours/</guid>
      <description>

&lt;h1 id=&#34;contour-là-gì&#34;&gt;Contour là gì&lt;/h1&gt;

&lt;p&gt;Các bạn có thể hiểu contour là &amp;ldquo;tập các điểm-liên-tục tạo thành một đường cong (curve) (boundary), và không có khoảng hở trong đường cong đó, đặc điểm chung trong một contour là các các điểm có cùng /gần xấu xỉ một giá trị màu, hoặc cùng mật độ. Contour là một công cụ hữu ích được dùng để phân tích hình dạng đối tượng, phát hiện đối tượng và nhận dạng đối tượng&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Để tìm contour chính xác, chúng ta cần phải &lt;em&gt;nhị phân hóa bức ảnh&lt;/em&gt; (nhớ là ảnh nhị phân nha các bạn, không phải ảnh grayscale đâu). Các kỹ thuật nhị phân hóa ảnh ở xử lý ảnh cơ bản có thể liệt kê đến là đặt ngưỡng, hoặc candy edge detection. Chúng ta sẽ không bàn kỹ về các cách đặt ngưỡng ( mặc dù có khá nhiều cách đặt ngưỡng, và trong opencv cũng có implement một vài phương pháp, nhưng nó không phải là mục tiêu của bài này, nên mình không đề cập ở đây) hoặc edge detection ở bài viết này, mà chúng ta sẽ đi vào các tìm contours bằng các sử dụng opencv luôn.&lt;/p&gt;

&lt;p&gt;Trong opencv, việc tìm một contour là việc &lt;em&gt;tìm một đối tượng có màu trắng trên nền đen&lt;/em&gt;. Cho nên, các bạn hãy nhớ rằng hãy set đối tượng thành màu trắng và để nền là màu đen, đừng làm ngược lại nha.&lt;/p&gt;

&lt;p&gt;Một lưu ý nhỏ là tại thời điểm mình viết bài viết này, mình sử dụng phiên bản opencv3.6. Các bạn có thể sử dụng phiên bản opencv mới hơn, nhưng có thể những sample code mình để bên dưới sẽ không work, do không tương thích.&lt;/p&gt;

&lt;h1 id=&#34;sử-dụng-contour-trong-opencv&#34;&gt;Sử dụng contour trong opencv&lt;/h1&gt;

&lt;p&gt;Opencv hỗ trợ cho chúng ta hàm để tìm contour của một bức ảnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;modifiedImage, contours, hierarchy = cv2.findContours(binaryImage, typeofContour, methodofContour)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong đó:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;contours: Danh sách các contour có trong bức ảnh nhị phân. Mỗi một contour được lưu trữ dưới dạng vector các điểm&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;hierarchy: Danh sách các vector, chứa mối quan hệ giữa các contour.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;modifiedImage: Ảnh sau khi sử dụng contour, thường chúng ta không xài đối số này&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;binaryImage: Ảnh nhị phân gốc. Một chú ý quan trọng ở đây là sau khi sử dụng hàm findContours thì giá trị của binaryImage cũng thay đổi theo, nên khi sử dụng bạn có thể áp dụng binaryImage.copy() để không làm thay đổi giá trị của binaryImage&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;typeofContour: có các dạng sau: RETR_EXTERNAL, RETR_LIST, RETR_CCOMP, RETR_TREE, RETR_FLOODFILL.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;methodofContour: Có các phương thức sau: CHAIN_APPROX_NONE, CHAIN_APPROX_SIMPLE, CHAIN_APPROX_TC89_L1, CHAIN_APPROX_TC89_KCOS.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ví dụ về các sử dụng hàm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import cv2

im = cv2.imread(&#39;test.jpg&#39;) # đọc ảnh màu
imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)  # chuyển ảnh màu sang dạng grayscale
ret,thresh = cv2.threshold(imgray,127,255,0) # nhị phân hóa bức ảnh bằng cách đặt ngưỡng, với giá trị của ngưỡng là 127
im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) # tìm contour

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Opencv hỗ trợ chúng ta hàm để vẽ contor lên bức ảnh, giúp chúng ta nhìn rõ ràng hơn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv2.drawContours(image, contours, contourIndex, colorCode, thickness)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;imgage: ảnh, có thể là ảnh grayscale hoặc ảnh màu.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;contours: danh sách các contour, là vector, nếu bạn muốn vẽ một contour, thì bạn phải cho nó vào trong một list.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;contourIndex Vị trí của contor, thông thường chúng ta để -1&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;colorCode: Giá trị màu của contour chúng ta muốn vẽ, ở dạng BGR, nếu bạn muốn vẽ contour màu xanh lá cây thì set là (0,255,0).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;thickness : độ dày của đường contour cần vẽ, giá trị thickness càng lớn thì đường contor vẽ càng bự&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;ví-dụ-đếm-số-lượng-quả-bóng-bay-trong-hình&#34;&gt;Ví dụ: Đếm số lượng quả bóng bay trong hình&lt;/h1&gt;

&lt;p&gt;Giả sử chúng ta có bức ảnh
&lt;img src=&#34;/post_image/colorfull_ballon.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Bong bóng bay&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Chúng ta thực hiện tìm contour của ảnh trên bằng cách&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import cv2

im = cv2.imread(&#39;colorfull_ballon.jpg&#39;)
imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) # chuyển ảnh xám thành ảnh grayscale
thresh = cv2.Canny(imgray, 127, 255) # nhị phân hóa ảnh
_, contours, _ = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)

cv2.drawContours(im, contours, -1, (0, 255, 0), 2) # vẽ lại ảnh contour vào ảnh gốc

# show ảnh lên
cv2.imshow(&amp;quot;ballons&amp;quot;, im)
cv2.waitKey(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/vietnam_coins_set_contours.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Contour màu xanh là đường curve bao quanh dữ liệu được rút trích được&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>