<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Phạm Duy Tùng Machine Learning Blog</title>
    <link>/</link>
    <description>Recent content on Phạm Duy Tùng Machine Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>alexblack2202@gmail.com (Phạm Duy Tùng)</managingEditor>
    <webMaster>alexblack2202@gmail.com (Phạm Duy Tùng)</webMaster>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Fri, 02 Jul 2021 00:19:00 +0300</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Mô hình phân quyền - Access Control</title>
      <link>/blog/2021-07-02-mo-hinh-phan-quyen/</link>
      <pubDate>Fri, 02 Jul 2021 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2021-07-02-mo-hinh-phan-quyen/</guid>
      <description>

&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới thiệu&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#acl&#34;&gt;Danh sách điều khiển truy cập -  Access Control List &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mac&#34;&gt;Điều khiển truy cập bắt buộc - Mandatory Access Control &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dac&#34;&gt;Điều khiển truy cập tùy quyền -  Discretionary Access Control (DAC) &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbac&#34;&gt;Điều khiển truy cập theo vai - Role Based Access Control (RBAC) &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#abac&#34;&gt;Điều khiển truy cập theo thuộc tính - Attribute Based Access Control (ABAC) &lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;danh-sách-điều-khiển-truy-cập-access-control-list-acl-a-name-acl-a&#34;&gt;Danh sách điều khiển truy cập -  Access Control List (ACL) &lt;a name=&#34;acl&#34;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Là mô hình cấp quyền truy cập dựa vào danh sách các quyền&lt;/p&gt;

&lt;p&gt;Mô hình:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Subject được quyền ( action ) trên object

Tuỳ từng bài toán khác nhau mà subject, action, object là khác nhau

Ví dụ: 

Trong môi trường phân quyền tập tin linux, subject là user, thread, action là READ/WRITE/ EXECUTE object là file, directory, tcp/udp port, thiết bị nhập xuất ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ví dụ:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trong hệ thống phân quyền của linux

User Alice được quyền đọc/ghi/thực thi trên file alice.sh

User Bob được quyền đọc trên file alice.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ứng dụng:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mô hình được ứng dụng trong Filesystem ACLs, POSIX ACL, NFSv4 ACL, Active Directory ACLs, Networking ACLs, SQL implementations. 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tham khảo:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Access-control_list&#34;&gt;https://en.wikipedia.org/wiki/Access-control_list&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;điều-khiển-truy-cập-bắt-buộc-mandatory-access-control-mac-a-name-mac-a&#34;&gt;Điều khiển truy cập bắt buộc - Mandatory Access Control (MAC) &lt;a name=&#34;mac&#34;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Về cơ bản thì mô hình này cũng &amp;ldquo; là mô hình cấp quyền truy cập dựa vào danh sách các quyền&amp;rdquo;. Tuy nhiên, mô hình này sẽ kiểm soát quyền truy cập đến từng object của subject&lt;/p&gt;

&lt;p&gt;Mô hình:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Subject được quyền ( action ) trên object

Object được quyền (action) bởi object

Vì ràng ở mức 2 đầu, nên mô hình này được ràng chặc chẽ hơn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ví dụ:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Ví dụ: Ở một số tổ chức, user có quyền đọc ghi file (subject - action - object), tuy nhiên, có một số file  tuyệt mật được phân quyền đọc/ ghi cho giám đốc  (object - action - subject), nên user bình thường không thể đọc được.

Các bạn có thể đọc thêm 3 ví dụ trong link của cornell mình có để bên dưới
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ứng dụng:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELinux
Windows Vista và Windows Server 2008
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tham khảo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://en.wikipedia.org/wiki/Mandatory_access_control

http://www.cs.cornell.edu/courses/cs5430/2015sp/notes/mac.php
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;điều-khiển-truy-cập-tùy-quyền-discretionary-access-control-dac-a-name-dac-a&#34;&gt;Điều khiển truy cập tùy quyền -  Discretionary Access Control (DAC)  &lt;a name=&#34;dac&#34;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Là mô hình cấp quyền truy cập dựa vào danh sách các quyền. Mô hình này giống với ACL, chỉ có 1 điểm khác là subject có thể chuyển quyền mình đang có cho một subject khác&lt;/p&gt;

&lt;p&gt;Mô hình:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Subject được quyền ( action ) trên object

Subject gán quyền (grant : action - object) cho Subject khác
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ví dụ:
    Alice có quyền đọc, ghi, thực thi file Alice.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Alice gán quyền đọc file Alice.sh cho Bob
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ứng dụng:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Phân quyền file trong hệ điều hành
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;điều-khiển-truy-cập-theo-vai-role-based-access-control-rbac-a-name-rbac-a&#34;&gt;Điều khiển truy cập theo vai - Role Based Access Control (RBAC) &lt;a name=&#34;rbac&#34;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Mô hình còn có tên gọi khác là Role Based Security, là mô hình cấp quyền truy cập dựa vào danh sách các quyền. Tuy nhiên, các subject sẽ được gán vào trong các Role và chúng ta sẽ cấp quyền cho các role.&lt;/p&gt;

&lt;p&gt;Mô hình này có thể kết hợp với mô hình DAC (để tăng khả năng cấp quyền), hoặc MAC (để tăng tính bảo mật) mà không xung đột với 2 mô hình trên.&lt;/p&gt;

&lt;p&gt;Mô hình:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Subject thuộc Roles

Roles  được quyền ( action ) trên object

=&amp;gt; các subject thuộc Roles được quyền (action) trên object
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ví dụ:&lt;/p&gt;

&lt;p&gt;Alice thuộc Role NhanVienTuyenDung, NhanVienIT&lt;/p&gt;

&lt;p&gt;Role NhanVienTuyenDung có quyền read, execute file&lt;/p&gt;

&lt;p&gt;Role NhanVienIT có quyền write file&lt;/p&gt;

&lt;p&gt;=&amp;gt; Alice có quyền read, write, execute file&lt;/p&gt;

&lt;p&gt;Ứng dụng:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Có rất nhiều ứng dụng của mô hình này, ví dụ các forum mã nguồn mở, cấp quyền trong hệ điều hành ....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để tìm hiểu kỹ hơn về mô hình RBAC, các bạn có thể đọc quyển sách tham khảo ở dưới&lt;/p&gt;

&lt;p&gt;Tham khảo :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;David F. Ferraiolo; D. Richard Kuhn; Ramaswamy Chandramouli (2007). Role-based Access Control (2nd ed.). Artech House. ISBN 978-1-59693-113-8.

https://en.wikipedia.org/wiki/Role-based_access_control
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;điều-khiển-truy-cập-theo-thuộc-tính-attribute-based-access-control-abac-a-name-abac-a&#34;&gt;Điều khiển truy cập theo thuộc tính - Attribute Based Access Control (ABAC) &lt;a name=&#34;abac&#34;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Mô hình còn có tên gọi khác là Policy Based Access Control hoặc Claims Based Access Control (CBAC), là mô hình cấp quyền truy cập dựa vào danh sách các quyền kết hợp với các thuộc tính.&lt;/p&gt;

&lt;p&gt;Kiến trúc: Theo NIST đề xuất, kiến trúc của ABAC  nên có các thành phần sau:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-  Policy Enforcement Point PEP: chịu trách nhiệm phân tích các yêu cầu truy xuất và gửi đến PDP để chứng thực.

- Policy Decision Point PDP: nhận thông tin từ PEP và chịu trách nhiệm chứng thực yêu cầu có quyền truy xuất tới các tài nguyên hay không, trả về đồng ý hoặc từ chối. Nếu thiếu tông tin thì 

- Policy Information Point PIP: trả về các attribute mà PDP yêu cầu.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thuộc tính: Bất kể thứ gì trên đời này đều có thể là thuộc tính. Tuy nhiên, chúng sẽ thường được phân làm 4 nhóm chính sau:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- Subject attributes: Các thuộc tính về thông tin người dùng, ví dụ họ tên, ngày tháng năm sinh, quê quán, quốc tịch, địa chỉ, phòng ban, chức vụ, tên công việc, số cmnd, ....

- Action attributes: Các thuộc tính về hành động như chạy , nảy, xoá, thêm, đọc, ghi ...

- Object attributes: Các thuộc tính về thông tin của đối tượng muốn truy xuất, ví dụ như loại file, phần đuôi mở rộng, vị trí, ....

- Contextual (environment) attributes: Các thuộc tính liên quan đến kịch bản diễn ra. Ví dụ hệ điều hành, ram, cpu, thời gian, múi giờ, ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ví dụ:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Toàn bộ nhân viên không được truy xuất database sau 21h đêm

Nhân viên Nguyễn Thị Lụa của GHN được quyền đổ danh sách freelancer ở Hà Nội, Hải Phòng, Hưng Yên
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ứng dụng:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Có thể ứng dụng ABAC vào rất nhiều ứng dụng khác nhau để kiểm soát luồng truy cập tài nguyên của hệ thống. Tuy nhiên, việc xây dựng mô hình ACBA khá tốn kém về tài nguyên và đòi hỏi người quản lý phải có tư duy hệ thống vững chắc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để tìm hiểu kỹ hơn về mô hình ABAC, các bạn có thể đọc quyển sách tham khảo ở dưới&lt;/p&gt;

&lt;p&gt;Tham khảo :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://nvlpubs.nist.gov/nistpubs/specialpublications/NIST.SP.800-162.pdf

https://arxiv.org/pdf/1306.2401.pdf

https://en.wikipedia.org/wiki/Attribute-based_access_control
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cảm ơn các bạn đã chú ý quan tâm theo dõi. Xin chào và hẹn gặp lại ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nâng cấp WSL lên bản WSL 2 trên window 10</title>
      <link>/blog/2021-05-30-upgrade-wls-to-wls2/</link>
      <pubDate>Sun, 30 May 2021 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2021-05-30-upgrade-wls-to-wls2/</guid>
      <description>

&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới thiệu&lt;/h1&gt;

&lt;p&gt;Microsoft đã trình làng phiên bản WLS 2 với nhiều điểm cải tiến nổi trội. Trong bài viết này, mình sẽ hướng dẫn các bạn cài đặt wls 2 và upgrade các distro linux của mình xài WLS 2. Mình có một lưu ý nhỏ là nếu các distro linux của bạn không bị ràng gì thì các bạn nên xóa các linux distro hiện tại và cài mới lại linux. Vì quá trình upgrade chạy rất là lâu.&lt;/p&gt;

&lt;h1 id=&#34;yêu-cầu&#34;&gt;Yêu cầu&lt;/h1&gt;

&lt;p&gt;Để cài đặt WLS 2, Các bạn bắc buộc phải nâng cấp lên các phiên bản &amp;ldquo;Windows 10 May 2020 (2004), Windows 10 May 2019 (1903), or Windows 10 November 2019 (1909)&amp;rdquo; hoặc các bản cập nhật sau đó.&lt;/p&gt;

&lt;p&gt;ĐỂ xác định xem máy bạn đang xài phiên bản bao nhiêu, bạn nãy gõ mở cmd lên và gõ lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;systeminfo | findstr &amp;quot;OS&amp;quot;

------

OS Name:                   Microsoft Windows 10 Home Single Language
OS Version:                10.0.19043 N/A Build 19043
OS Manufacturer:           Microsoft Corporation
OS Configuration:          Standalone Workstation
OS Build Type:             Multiprocessor Free
BIOS Version:              American Megatrends Inc. S551LN.209, 7/8/2014

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nếu thỏa mãn các điều kiện trên, thì các bước chúng ta phải làm là:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart

------

Deployment Image Servicing and Management tool
Version: 10.0.19041.844

Image Version: 10.0.19043.1023

Enabling feature(s)
[==========================100.0%==========================]
The operation completed successfully.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta chạy lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

----------

Deployment Image Servicing and Management tool
Version: 10.0.19041.844

Image Version: 10.0.19043.1023

Enabling feature(s)
[==========================100.0%==========================]
The operation completed successfully.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau đó, bạn phải khởi động lại máy để window tiến hành cập nhật các gói thư viện cần thiết.&lt;/p&gt;

&lt;p&gt;Sau khi khởi động lại máy xong, chúng ta sẽ gọi lệnh set phiên bản mặc định của wsl là bản 2 bằng lệnh:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
wsl --set-default-version 2

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi chạy lệnh này, sẽ có 1 trong 2 trường hợp xảy ra. Trường hợp 1&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;For information on key differences with WSL 2 please visit https://aka.ms/wsl2

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thì chúc mừng bạn, bạn đã enable thành công WSL 2&lt;/p&gt;

&lt;p&gt;Trường hợp thứ 2, bạn sẽ gặp output như thế này:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thì bạn này vào trang &lt;a href=&#34;https://aka.ms/wsl2kernel&#34;&gt;https://aka.ms/wsl2kernel&lt;/a&gt; như hướng dẫn, đọc kỹ file, down về file msi để cài  Linux kernel  vào. Sau đó chạy lại lệnh &amp;ldquo;wsl &amp;ndash;set-default-version 2&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Sau đó, các bạn tiến hành check lại phiên bản linux mình đang sử dụng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
 wsl --list --verbose
 
 -----
 
   NAME            STATE           VERSION
* Ubuntu-18.04    Running         1
  kali-linux      Stopped         1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Như các bạn thấy ở trên, bản ubuntu 18.4 mình đang sử dụng đang ở version 1. Mình sẽ convert qua version 2 bằng lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
wsl --set-version Ubuntu-18.04 2 

-------
Conversion in progress, this may take a few minutes...
For information on key differences with WSL 2 please visit https://aka.ms/wsl2

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi chạy dòng lệnh trên, các bạn chịu khó ngồi chờ một xíu, nó phụ thuộc vào cấu hình máy của các bạn. Kinh nghiệm của mình khi upgrade vài máy là nên tắt chương trình diệt virus như kaspersky, norton, BKAV, bit &amp;hellip;. đi. Tắt những ứng dụng sử dụng nhiều ram thì việc convert sẽ chạy nhanh hơn một chút.&lt;/p&gt;

&lt;p&gt;Kết quả sau khi mình convert.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
  NAME            STATE           VERSION
* Ubuntu-18.04    Stopped         2
  kali-linux      Stopped         1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cảm ơn các bạn đã chú ý theo dõi. Hẹn gặp lại ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Link hướng dẫn gốc từ trang chủ microsoft&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/windows/wsl/install-win10&#34;&gt;https://docs.microsoft.com/en-us/windows/wsl/install-win10&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tinh chỉnh thuật toán XGBoost  với Learning Curves</title>
      <link>/blog/2021-04-11-xgboost_learning_curves/</link>
      <pubDate>Sun, 11 Apr 2021 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2021-04-11-xgboost_learning_curves/</guid>
      <description>

&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới thiệu&lt;/h1&gt;

&lt;p&gt;Trong quá trình giải các bài toán có sử dụng machine learning, vì để làm nhanh nên đôi khi mình sẽ sử dụng các tham số mặc định của mô hình để train. Một phần vì lý do chúng ta không biết cách chỉnh các tham só như thế nào, so với cái gì để có mô hình huấn luyện là tốt nhất. Ở bài viết này, mình sẽ sử dụng Learning Curves để tối ưu hóa các tham số của XGBoost. Các mô hình khác cũng làm tương tự thôi. Mình chọn XGBoost vì mô hình này thường cho kết quả khá tốt trên các cuộc thi ở Kaggle.&lt;/p&gt;

&lt;h1 id=&#34;bắt-đầu&#34;&gt;Bắt đầu&lt;/h1&gt;

&lt;p&gt;Để bắt đầu thí nghiệm, chúng ta sẽ sinh ngẫu nhiên 60 ngàn dữ liệu có 1 ngàn thuộc tính bằng cách sử dụng hàm make_classification, sau đó sẽ chia dữ liệu thành 2 tập train và test với tỷ lệ 10% là tập test&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y = make_classification(n_samples=60000, n_features=1000, n_informative=50, n_redundant=0, random_state=1)
#  split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load mô hình XGBClassifier với các tham số là mặc định. Mô hình này được xem như là baseline và các cải tiến tham số ở sau sẽ so sánh kết quả trên mô hình này.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

model = XGBClassifier()

evalset = [(X_train, y_train), (X_test, y_test)]

model.fit(X_train, y_train, eval_metric=&#39;logloss&#39;, eval_set=evalset)
# evaluate performance
yhat = model.predict(X_test)
score = accuracy_score(y_test, yhat)
print(&#39;Accuracy: %.3f&#39; % score)
# retrieve performance metrics
results = model.evals_result()
# plot learning curves
pyplot.plot(results[&#39;validation_0&#39;][&#39;logloss&#39;], label=&#39;train&#39;)
pyplot.plot(results[&#39;validation_1&#39;][&#39;logloss&#39;], label=&#39;test&#39;)
# show the legend
pyplot.legend()
# show the plot
pyplot.show()


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Độ chính xác: Accuracy: 0.962. Lưu ý ràng độ chính xác khi thực nghiệm của mỗi lần chạy sẽ khác nhau, do data sinh ngẫu nhiên và một phần do sự ngẫu nhiên trong XGBoost.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/xgboost_learning_cruver.jpg&#34; alt=&#34;Hình ảnh Learning Curves&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn vào hình trên, chúng ta thấy rằng đường cong của tập train (đường màu xanh) có độ lỗi tốt hơn so với đường cong của tập test( đường màu đỏ)&lt;/p&gt;

&lt;h1 id=&#34;tiến-hành-turning&#34;&gt;Tiến hành turning&lt;/h1&gt;

&lt;p&gt;Đầu tiên, nhìn vào đồ thị, ta thấy rằng đường cong vẫn còn có độ dốc, nên việc tăng số lần lặp có thể sẽ làm tăng thêm độ chính xác, thử thay đổi số lần lặp lên 500 xem sao.&lt;/p&gt;

&lt;p&gt;Trong XGBoost số lần lặp được tham số hóa bởi tham số n_estimators, chỉnh lại đoạn mã lệnh ở trên với một thay đổi nhỏ rồi chạy lại&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = XGBClassifier(n_estimators=500)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Độ chính xác của mô hình tăng lên 1 chút, đối với thực nghiệm của mình là Accuracy: 0.981&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/xgboost_learning_cruver_n500.jpg&#34; alt=&#34;Hình ảnh Learning Curves với số lần lặp 500&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Quan sát đường cong của hình trên, ta thấy phần đuôi đoạn số lần lặp từ 270 đến 500 có độ dốc nhỏ, hầu như là bằng phẳng, có thể kết luận là việc huấn luyện ở đoạn này hầu như không cải tiến gì nhiều.&lt;/p&gt;

&lt;p&gt;Một nhận xét nữa là đoạn trước 150 có độ dốc khá lớn, có khả năng là hệ số học (learning reate) quá lớn, làm cho mô hình chưa đạt được cực tiểu, thử điều chỉnh hệ số học này nhỏ hơn là 0.01, thay vì 0.3 như giá trị mặc định xem sao.&lt;/p&gt;

&lt;p&gt;Một lưu ý là hệ số học nhỏ thì sẽ lâu hội tụ, nên chúng ta phải tăng số lần lặp lên. Ở đây đồng thời với việc giảm hệ số học xuống 0.01, mình còn tăng số lần lặp lên 1000.&lt;/p&gt;

&lt;p&gt;Trong XGBoost hệ số học được tham số hóa bởi tham số eta&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = XGBClassifier(n_estimators=1000, eta=0.01)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Độ chính xác đạt được: Accuracy: 0.954&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/xgboost_learning_cruver_n1000.jpg&#34; alt=&#34;Hình ảnh Learning Curves với số lần lặp 1000,  eta=0.01&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Tuy mô hình có độ chính xác giảm, nhưng nhìn vào đồ thị thì ta thấy mô hình vẫn còn độ dốc, nghĩa là mô hình sẽ cho kết quả tốt hơn nữa nếu ta tăng số vòng lặp.&lt;/p&gt;

&lt;p&gt;Một cách khách là thay đổi các chuẩn hóa (regularization ) bằng cách giảm các tham số số mẫu ( samples) và số đặc trưng (features) được dùng để xây dựng cây trong tập hợp. Hai tham số này được tham số hóa bởi tham số subsample và colsample_bytree. Giá trị mặc định của chúng là 1. Chúng ta sẽ thay đổi thành 0.35 xem sao nhé&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = XGBClassifier(n_estimators=5000, eta=0.01, subsample=0.35, colsample_bytree=0.35)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả Accuracy: 0.970
&lt;img src=&#34;/post_image/xgboost_learning_cruver_n5k.jpg&#34; alt=&#34;Hình ảnh Learning Curves với số lần lặp 5000,  eta=0.01&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ở hai lần thí nghiệm trên, mình có các hướng xử lý có thể đi tiếp, một là tăng số lần lặp lên, vì độ dốc của mô hình vẫn còn, nên chúng ta hoàn toàn có thể thu được kết quả tốt hơn. Một cách khác là tăng learning rate lên để quá trình hội tụ được xảy ra nhanh hơn, ví dụ để eta = 0.05 hoặc 0.75 chẳn hạn.&lt;/p&gt;

&lt;p&gt;Quá trình này có thể tiếp tục, dựa vào quan sát của các bạn trên đường cong và hơn hết là sự hiệu biết thấu đáo của các bạn trên các tham số mà mô hình của bạn đang sử dụng. Chúc các bạn sẽ có một hướng đi tốt để giảm thiểu thời gian mò mẫm.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã chú ý theo dõi. Hẹn gặp lại ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Nguồn tham khảo&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier&#34;&gt;https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/&#34;&gt;https://machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/&#34;&gt;https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu thuật toán tối ưu hóa Adabelief Optimizer</title>
      <link>/blog/2021-01-15---adabelief-optimizer/</link>
      <pubDate>Fri, 15 Jan 2021 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2021-01-15---adabelief-optimizer/</guid>
      <description>

&lt;h1 id=&#34;giới-thiệu&#34;&gt;Giới thiệu&lt;/h1&gt;

&lt;p&gt;Hi các bạn, lại là mình đây, hôm nay mình sẽ cùng các bạn tìm hiểu thuật toán tối ưu hóa AdaBelief. Thuật toán này được sử dụng để thay cho thuật toán Adam optimizer mà các bạn hiện đang xài để huấn luyện mô hình Deep learning. Nào, chúng ta cùng bắt đầu tìm hiểu nhé.&lt;/p&gt;

&lt;p&gt;Ẩn sâu bên trong các thuật toán sử dụng Neural Network  và một vài thuật toán machine learning đều sử dụng các hàm tối ưu hóa. Chúng ta có thể liệt kê ra một vài cái tên như RMSprop, SGD (Stochastic Gradient Descent), Adam (Adaptive Moment Estimation).&lt;/p&gt;

&lt;p&gt;Một vài các yếu tố hay được sử dụng để đánh giá một thuật toán optimizer:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hội tụ nhanh (trong quá trình train)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sự tổng quát hóa cao (vẫn nhận dạng được những mẫu chưa từng được huấn luyện)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Độ chính xác cao&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Các thuật toán tối ưu thuộc họ Adaptive thường có tốc độ hội tụ nhanh. Trong khi đó, các thuật toán thuộc họ SGD thường có sự tổng quát hóa cao. Gần đây, Juntang Zhuang và các cộng sự thuộc đại học Yale đã nghiên cứu và tạo ra thuật toán AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. Thuật toán này theo lời tác giả, hội tụ cả hai ưu điểm của họ Adaptive và SGD, là vừa có tốc độ hội tụ nhanh, vừa có tính tổng quát hóa cao Mã nguồn được tác giả công bố ở link &lt;a href=&#34;https://github.com/juntang-zhuang/Adabelief-Optimizer&#34;&gt;https://github.com/juntang-zhuang/Adabelief-Optimizer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lời của tác giả:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We propose the AdaBelief optimizer, which adaptively scales the stepsize by the difference betweenpredicted gradient and observed gradient.  To our knowledge, AdaBelief is the first optimizer toachieve three goals simultaneously: fast convergence as in adaptive methods, good generalization asin SGD, and training stability in complex settings such as GANs. Furthermore, Adabelief has the same parameters as Adam, hence is easy to tune. We validate the benefits of AdaBelief with intuitive examples, theoretical convergence analysis in both convex and non-convex cases, and extensiveexperiments on real-world datasets&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Để hiểu về AdaBelief, trước tiên, chúng ta phải có một ít kiến thức cơ bản về SGD và Adam, nên chúng ta sẽ bắt đầu nói về SGD trước&lt;/p&gt;

&lt;h1 id=&#34;sgd-stochastic-gradient-descent&#34;&gt;SGD - Stochastic Gradient Descent&lt;/h1&gt;

&lt;p&gt;Thuật toán SGD là thuật toán tối ưu hóa cơ bản theo họ gradient. Thuật toán này rất triển khai, có nền tảng lý thuyết vững chắc, cực kỳ ổn định trong quá trình huấn luyện, kết quả đạt được có thể so sánh với các thuật toán khác. Ý tưởng của thuật toán khá đơn giản, đó là &amp;ldquo;tính giá trị gradient của mỗi tham số, và đi một bước nhỏ theo chiều của gradient&amp;rdquo;. Nếu chúng ta lặp đi lặp lại quá trình này, và ngẫu nhiên chọn (stochastic) một tập batch trong tập huấn luyện, mô hình chúng ta sẽ được cải tiến dần đến đểm hội tụ.&lt;/p&gt;

&lt;p&gt;Trong quá khứ, phần khó nhất của SGD là việc tính lại giá trị gradient cho toàn bộ các tham số trong mô hình. Nhưng hiện nay, các framwork máy học như Tensorflow, PyTouch, Caffee, Theano, &amp;hellip;. đã giúp chúng ta tính các giá trị gradient một cách tự động. Do đó, công việc của chúng ta hiện thời đơn giản hơn&lt;/p&gt;

&lt;p&gt;$$for \text{ }  i \text{ } in \text{ } range (m): $$
  $$\theta_i = \theta_i - \alpha ( \hat y^{i} - y^i) x^i_j$$&lt;/p&gt;

&lt;p&gt;Một vấn đề chúng ta gặp phải trong quá trình huấn luyện DL với SGD là chậm, siêu chậm. Do thuật toán phải cập nhật toàn bộ các tham số, nên số lượng phép tính và lượng tài nguyên phần cứng được sử dụng rất là nhiều. Rất nhiều các biến thể của SGD đã được đề xuất để giải quyết vấn đề trên.&lt;/p&gt;

&lt;h1 id=&#34;adam-adaptive-moment-estimation&#34;&gt;Adam - Adaptive Moment Estimation&lt;/h1&gt;

&lt;p&gt;Adam optimizer là một thuật toán kết hợp kỹ thuật  của RMS prop và momentum. Thuật toán sử dụng hai internal states momentum (m) và  squared momentum (v) của gradient cho các tham số. Sau mỗi batch huấn luyện, giá trị của m và v được cập nhật lại sử dụng exponential weighted averaging.&lt;/p&gt;

&lt;p&gt;Mã giải của việc cập nhật m và v&lt;/p&gt;

&lt;p&gt;$$m_t = \beta_1m_t-_1 + (1-\beta_1)g_t $$
 $$v_t  = \beta_2v_t-_1 + (1-\beta_2)g^2_t$$&lt;/p&gt;

&lt;p&gt;trong đó, beta được xem như là một siêu tham số. Công thức cập nhật theta như sau:&lt;/p&gt;

&lt;p&gt;$$\theta_t = \theta_t-_1 - \alpha\frac{m_t}{\sqrt{v_t}+ \epsilon }$$&lt;/p&gt;

&lt;p&gt;trong đó, alpha là learning rate, epsion là giá trị được thêm vào để ngăng việc chia cho 0&lt;/p&gt;

&lt;p&gt;Để việc descent  được thực hiện nhanh hơn, thuật toán đã sử dụng hai kỹ thuật:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tính  exponential moving average của giá trị đạo hàm lưu vào biến m và sử dụng nó là tử số của việc  cập nhật hướng. Với ý nghĩa là nếu m có giá trị lớn, thì việc descent đang đi đúng hướng và chúng ta cần bước nhảy lớn hơn để đi nhanh hơn. Tương tự, nếu giá trị m nhỏ, phần descent có thể không đi về hướng tối tiểu và chúng ta nên đi 1 bước nhỏ để thăm dò. Đây là phần momentum của thuật toán.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tính exponential moving average của bình phương gía trị đạo hàm lưu vào biến v và sử dụng nó là phần mẫu số của việc cập nhật hướng. Với ý nghĩa như sau: Giả sử gradient mang các giá trị dương, âm lẫn lộn, thì khi cộng các giá trị lại theo công thức tính m ta sẽ được  giá trị m gần số 0. Do âm dương lẫn lộn nên nó bị triệt tiêu lẫn nhau. Nhưng trong trường hợp này thì v sẽ mang giá trị lớn. Do đó, trong trường hợp này, chúng ta sẽ không hướng tới cực tiểu, chúng ta sẽ không muốn đi theo hướng đạo hàm trong trường hợp này. Chúng ta để v ở phần mẫu vì khi chia cho một giá trị cao, giá trị của  các phần cập nhật sẽ nhỏ, và khi v có giá trị thấp, phần cập nhật sẽ lớn. Đây chính là phần tối ưu RMSProp  của thuật toán.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ở đây, m được xem như là moment thứ nhất, v xem như là moment thứ hai, nên thuật toán có tên là &amp;ldquo;Adaptive moment estimation&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Để lý giải vì sao Adam lại hội tụ nhanh hơn so với SGD, chúng ta có thể giải thích như sau: Exponential weighted averaging cho chúng ta giá trị xấp xỉ gradient mượt hơn qua mỗi lần lặp, dẫn tới tăng tínhs dừng. Sau đó, việc chia cho căng bậc 2 của giá trị v làm số lước của chúng ta giảm mạnh khi phương sai của giá trị gradient tăng lên. Điều này , như giải thích ở trên, có nghĩa là, khi hướng đi của mô hình chỉ ra không rõ ràng, thuật toán Adam thực hiện các bước đi nhỏ coi như là thăm dò thôi. Và sẽ thực hiện các bước đi lớn, nhanh khi hướng đi rõ ràng.&lt;/p&gt;

&lt;p&gt;Thuật toán Adam hoạt động khá hiệu quả, nhưng bản thân nó cũng có những vấn đề. Tác giả của AdaBelief  đã chỉ ra một vài điểm không hiệu quả của thuật toán&lt;/p&gt;

&lt;h1 id=&#34;adabelief-optimizer-adapting-stepsizes-by-the-belief-in-observed-gradients&#34;&gt;AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/adam_error.jpg&#34; alt=&#34;Hình ảnh AdaBelief - Nguồn https://arxiv.org/pdf/2010.07468v5.pdf &#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hãy nhìn vào hình trên, ở mục đánh dấu là số 3, giá trị G lớn vì đường cong ở đoạn đó dốc. Giá trị v cũng lớn. Do đó, nếu sử dụng thuật toán Adam ở đây, bước đi sẽ rất nhỏ. Việc di chuyển một bước đi nhỏ ở đây sẽ làm chậm quá trình hội tụ và không cần thiết. Bởi vì chúng ta tin tưởng rằng chúng ta đang đi đúng hướng, và chúng ta cần một bước đi dài hơn.&lt;/p&gt;

&lt;p&gt;AdaBelief sửa lỗi này bằng một thay đổi nhỏ trong thuật toán của adam. Thay vì tính bình phương của gradient, AdaBelief  sẽ tính phương sai của gradient. Một sự thay đổi nhỏ nhưng mang lại giá trị to lớn.&lt;/p&gt;

&lt;p&gt;$$v_t  = \beta_2v_t-_1 + (1-\beta_2)g^2_t $$
$$s_t  = \beta_2v_t-_1 + (1-\beta_2)(g_t-m_t)^2$$&lt;/p&gt;

&lt;p&gt;Tác giả không dùng biến v nữa, mà thay bằng biến s.&lt;/p&gt;

&lt;p&gt;Với việc dùng biến s. Trong trường hợp trên, g lớn và m lớn, thì s sẽ nhỏ. Và khi s ở phần mẫu nhỏ, chúng ta sẽ có bước đi xa hơn. Ở đây, AdaBelief  đã giải quyết vấn đề&lt;/p&gt;

&lt;p&gt;Qua đây, chúng ta cũng có thể giải thích vì sao có chữ &amp;ldquo;belief&amp;rdquo; trong từ AdaBelief. Giá trị phương sai được tính dựa vào kỳ vọng của giá trị gradient.&lt;/p&gt;

&lt;p&gt;Một chú ý nhỏ ở đây là mục số 1 và mục số 3 được coi là cải tiến của Adam  so với momentum và SGD. Tất nhiên, AdaBelief cũng kế thừa mấy cái này.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ở mục đánh dấu số 1 trên hình, đường cong khá phẳng và giá trị đạo hàm gần như bằng 0. Nếu sử dụng SGD, chúng ta sẽ có một bước đi nhỏ. Trong khi đó, họ Adam sẽ cho chúng ta bước đi lớn hơn vì giá trị căng bậc hai của s hoặc v ở mẫu số sẽ cho ra một kết quả rất nhỏ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ở mục đánh dấu số 2, đường cong ở đây rất dốc và hẹp, g và delta g ở đây rất lớn, cho nên ở đây chúng ta cần một bước di chuyển nhỏ. Nếu sử dụng SGD hoặc momentum thì sẽ đi một bước đi rất lớn do nhân với một lượng moving averages lớn. Trong khi đó, với Adam hoặc AdaBelief, chúng ta sẽ có giá trị căng bậc hai của s hoặc v ở mẫu số lớn nên bước đi sẽ nhỏ hơn.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Về tốc độ hội tụ, tác giả có đề cập rõ và chi tiết trong bài báo, mình không đề cập lại nó nữa ở đây. Các bạn tự xem nhé.&lt;/p&gt;

&lt;h1 id=&#34;kết-luận&#34;&gt;Kết luận&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AdaBelief là thuật toán tối ưu hóa có nguồn gốc từ thuật toán Adam, không có thêm tham số ngoài, chỉ thay đổi 1 dòng code.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thuật toán đã tăng tốc độ hội tụ cũng như mức tổng quát hóa.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thuật toán thực hiện các bước đi dựa vào &amp;ldquo;belief&amp;rdquo; của hướng gradient ở thời điểm hiện tại.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thuật toán giải quyết vấn đề &amp;ldquo;Large gradient, small curvature&amp;rdquo; bằng cách xem xét biên độ và dấu của gradient.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nguồn:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.07468&#34;&gt;https://arxiv.org/abs/2010.07468&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/the-dl/understanding-the-new-adabelief-optimizer-2db70ef6de1e&#34;&gt;https://medium.com/the-dl/understanding-the-new-adabelief-optimizer-2db70ef6de1e&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/adabelief-optimizer-fast-as-adam-generalizes-as-good-as-sgd-71a919597af&#34;&gt;https://towardsdatascience.com/adabelief-optimizer-fast-as-adam-generalizes-as-good-as-sgd-71a919597af&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement Learning và tictactoe</title>
      <link>/blog/2020-12-26---tic-tac-toe/</link>
      <pubDate>Sun, 27 Dec 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-12-26---tic-tac-toe/</guid>
      <description>

&lt;h1 id=&#34;advantages-of-reinforcement-learning&#34;&gt;Advantages of Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;Trong khi trong các phương pháp lý thuyết trò chơi nói chung, ví dụ thuật toán min-max, thuật toán luôn giả định chúng ta có một đối thủ hoàn hảo, công việc phải thực hiện là tối đa hóa phần thưởng của mình và giảm thiểu phần thưởng của đối thủ ( tối đa hóa điểm của mình và tối thiểu hóa điểm của đối thủ), trong học củng cố, chúng ta không cần giả định đối thủ của chúng ta là 1 thiên tài xuất chúng, nhưng chung ta vẫn thu được mô hình với kết quả rất tốt.&lt;/p&gt;

&lt;p&gt;Bằng cách coi đối thủ là một phần của môi trường mà chúng ta có thể tương tác, sau một số lần lặp lại nhất định, đối thủ có thể lập kế hoạch trước mà không cần chúng ta phải làm gì cả. Ưu điểm của phương pháp này là giảm số lượng không gian tìm kiếm và giảm số phép toán suy luận phải thực hiện, nhưng nó có thể đạt được kỹ năng hiện đại chỉ bằng cách thử và học.&lt;/p&gt;

&lt;p&gt;Trong bài viết này, chúng ta sẽ làm các công việc sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Thứ nhất, huấn luyện mô hình cho 2 máy đấu với nhau mà thu được các trọng số cần thiết.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thứ hai, cho người đánh với máy&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Để hình thành bài toán học củng cố Reinforcement Learning , chúng ta cần  phải xác định rõ 3 thành phần chính:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;State&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Action&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reward&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Với:&lt;/p&gt;

&lt;p&gt;State chính là bàn cờ với các nước đi của các người chơi. Chúng ta sẽ tạo một bàn cờ có kích thước 3x3, giá trị của mỗi ô cờ đều là 0. Vị trí người chơi 1 đặt quân sẽ được gán là 1. Vị trí người chơi 2 đặt quân sẽ được gán là -1.&lt;/p&gt;

&lt;p&gt;Action là vị trí người chơi sẽ đi quân khi biết state hiện tại (nghĩa là biết đối thủ đi nước nào, và có những nước nào hiện đang trên bàn cờ).&lt;/p&gt;

&lt;p&gt;Reward: mang giá trị 0 hoặc 1. Khi kết thúc game sẽ trả về giá trị cho reward.&lt;/p&gt;

&lt;p&gt;Ở phần dưới đây, mình sẽ note lại code và sẽ comment trong code để cho rõ ý&lt;/p&gt;

&lt;h1 id=&#34;thiết-lập-bàn-cờ&#34;&gt;Thiết lập bàn cờ&lt;/h1&gt;

&lt;h2 id=&#34;khởi-tạo-bàn-cờ&#34;&gt;Khởi tạo bàn cờ&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def __init__(self, p1, p2):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.p1 = p1
        self.p2 = p2
        self.isEnd = False
        self.boardHash = None
        # init p1 plays first
        self.playerSymbol = 1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta sẽ tạo một bàn cờ có kích thước 3x3, 2 biến người chơi. Người 1 là người chơi đầu tiên.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Trả về danh sách các nước có thể đi
def availablePositions(self):
        positions = []
        for i in range(BOARD_ROWS):
            for j in range(BOARD_COLS):
                if self.board[i, j] == 0:
                    positions.append((i, j))  # need to be tuple
        return positions

# Cập nhật lại lên bàn cờ vị trí của người chơi đặt quân

def updateState(self, position):
    self.board[position] = self.playerSymbol
    # switch to another player
    self.playerSymbol = -1 if self.playerSymbol == 1 else 1
    
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kiểm-tra-reward&#34;&gt;Kiểm tra Reward&lt;/h2&gt;

&lt;p&gt;Sau mỗi nước đi của các kỳ thủ, chúng ta cần 1 hàm để kiểm tra xem kỳ thủ thắng hay thua và trả về kết quả cho reward như đề cập ở trên&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def winner(self):

    # Kiểm tra theo dòng
    
    for i in range(BOARD_ROWS):
        if sum(self.board[i, :]) == 3:
            self.isEnd = True
            return 1
        if sum(self.board[i, :]) == -3:
            self.isEnd = True
            return -1
    # kiểm tra theo cột
    
    for i in range(BOARD_COLS):
        if sum(self.board[:, i]) == 3:
            self.isEnd = True
            return 1
        if sum(self.board[:, i]) == -3:
            self.isEnd = True
            return -1
            
    # kiểm tra theo đường chéo chính và theo đường chéo phụ
    
    diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)]) # đường chéo chính
    
    diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)]) # đường chéo phụ
    
    diag_sum = max(abs(diag_sum1), abs(diag_sum2)) # lấy trị tuyệt đối của các nước đi, nếu bằng 3 nghĩa là có người chơi chiến thắng
    
    if diag_sum == 3:
        self.isEnd = True
        if diag_sum1 == 3 or diag_sum2 == 3:
            return 1
        else:
            return -1

    # Kiểm tra xem còn nước đi hay không
    if len(self.availablePositions()) == 0:
        self.isEnd = True
        return 0
        
    # not end
    self.isEnd = False
    return None

# only when game ends
def giveReward(self):
    result = self.winner()
    # backpropagate reward
    if result == 1:
        self.p1.feedReward(1)
        self.p2.feedReward(0)
    elif result == -1:
        self.p1.feedReward(0)
        self.p2.feedReward(1)
    else:
        self.p1.feedReward(0.1)
        self.p2.feedReward(0.5)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây có một lưu ý. Khi cờ hòa thì chúng ta cũng xem rằng người đi trước thua, nên hệ số lúc cờ hòa sẽ là 0.1-0.5. Các bạn có thể thiết lập một giá trị khác, ví dụ 0.2-0.5 hoặc 0.5-0.5 tùy thích.&lt;/p&gt;

&lt;h1 id=&#34;thiết-lập-người-chơi&#34;&gt;Thiết lập người chơi&lt;/h1&gt;

&lt;p&gt;Người chơi cần có các phương thức sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Chọn nước đi dựa trên trạng thái hiện tại của bàn cờ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lưu lại trạng thái của ván cờ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cập nhật lại giá trị trạng thái sau mỗi ván.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lưu và load các trọng số lên.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;khởi-tạo&#34;&gt;Khởi tạo&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def __init__(self, name, exp_rate=0.2):
        self.name = name
        self.states = []  # record all positions taken
        self.lr = 0.2
        self.exp_rate = exp_rate
        self.decay_gamma = 0.9
        self.states_value = {}  # state -&amp;gt; value

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;chọn-nước-đi&#34;&gt;Chọn nước đi&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def chooseAction(self, positions, current_board, symbol):
    randValue = np.random.uniform(0, 1)
    value_max = value = -999
    if  randValue&amp;gt; self.exp_rate:
        
        for p in positions:
            next_board = current_board.copy()
            next_board[p] = symbol
            next_boardHash = self.getHash(next_board)
            value = -999 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)
            # print(&amp;quot;value&amp;quot;, value)
            if value &amp;gt;= value_max:
                value_max = value
                action = p

    if  value_max == -999 :
        # take random action
        idx = np.random.choice(len(positions))
        action = positions[idx]
    
    # print(&amp;quot;{} takes action {}&amp;quot;.format(self.name, action))
    return action

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cập-nhật-trạng-thái&#34;&gt;Cập nhật trạng thái&lt;/h2&gt;

&lt;p&gt;Chúng ta sẽ cập nhật trạng thái với công thức sau&lt;/p&gt;

&lt;p&gt;$$ V(S_t) = V(S&lt;em&gt;t) + \alpha [V(S&lt;/em&gt;{t+1}) - V(S_t)]   $$&lt;/p&gt;

&lt;p&gt;Diễn giải ra tiếng việt, giá trị của trạng thái tại thời điểm t bằng giá trị tại thời điểm hiện tại cộng với độ lệch của trạng thái hiện tại và trạng thái tiếp theo nhân với một hệ số học alpha.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# at the end of game, backpropagate and update states value
def feedReward(self, reward):
        for st in reversed(self.states):
            if self.states_value.get(st) is None:
                self.states_value[st] = 0
            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])
            reward = self.states_value[st]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;huấn-luyện-mô-hình&#34;&gt;Huấn luyện mô hình&lt;/h2&gt;

&lt;p&gt;Phần này nằm trong lớp State. Chúng ta sẽ lần lượt đi qua các quá trình luân phiên nhau giữa người chơi 1 và người chơi 2&lt;/p&gt;

&lt;p&gt;người chơi chọn nước có thể đi -&amp;gt; cập nhật trạng thái -&amp;gt; kiểm tra thắng/thua -&amp;gt; người chơi chọn nước có thể đi &amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def play(self, rounds=100):
    for i in range(rounds):
        if i % 1000 == 0:
            print(&amp;quot;Rounds {}&amp;quot;.format(i))
        while not self.isEnd:
            # Player 1
            positions = self.availablePositions()
            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
            # take action and upate board state
            self.updateState(p1_action)
            board_hash = self.getHash()
            self.p1.addState(board_hash)
            # check board status if it is end

            win = self.winner()
            if win is not None:
                # self.showBoard()
                # ended with p1 either win or draw
                self.giveReward()
                self.p1.reset()
                self.p2.reset()
                self.reset()
                break

            else:
                # Player 2
                positions = self.availablePositions()
                p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)
                self.updateState(p2_action)
                board_hash = self.getHash()
                self.p2.addState(board_hash)

                win = self.winner()
                if win is not None:
                    # self.showBoard()
                    # ended with p2 either win or draw
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi huấn luyện 100 ngàn lần, chúng ta sẽ chơi với máy, chỉ là 1 thay đổi nhỏ trong hàm chooseAction là thay vì lấy nước đi có trọng số lớn nhất, chúng ta sẽ cho người dùng nhập từ bàn phím dòng và cột vào&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

def chooseAction(self, positions):
        while True:
            row = int(input(&amp;quot;Input your action row:&amp;quot;))
            col = int(input(&amp;quot;Input your action col:&amp;quot;))
            action = (row, col)
            if action in positions:
                return action

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và sửa lại hàm play một chút, bỏ loop 100k lần đi, bỏ gọi hàm cập nhật thưởng và bỏ các hàm reset đi&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

# play with human
def play2(self):
    while not self.isEnd:
        # Player 1
        positions = self.availablePositions()
        p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
        # take action and upate board state
        self.updateState(p1_action)
        self.showBoard()
        # check board status if it is end
        win = self.winner()
        if win is not None:
            if win == 1:
                print(self.p1.name, &amp;quot;wins!&amp;quot;)
            else:
                print(&amp;quot;tie!&amp;quot;)
            self.reset()
            break

        else:
            # Player 2
            positions = self.availablePositions()
            p2_action = self.p2.chooseAction(positions)

            self.updateState(p2_action)
            self.showBoard()
            win = self.winner()
            if win is not None:
                if win == -1:
                    print(self.p2.name, &amp;quot;wins!&amp;quot;)
                else:
                    print(&amp;quot;tie!&amp;quot;)
                self.reset()
                break

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mã nguồn hoàn chỉnh của chương trình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import pickle

BOARD_ROWS = 3
BOARD_COLS = 3


class State:
    def __init__(self, p1, p2):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.p1 = p1
        self.p2 = p2
        self.isEnd = False
        self.boardHash = None
        # init p1 plays first
        self.playerSymbol = 1

    # get unique hash of current board state
    def getHash(self):
        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))
        return self.boardHash

    def winner(self):
        # row
        for i in range(BOARD_ROWS):
            if sum(self.board[i, :]) == 3:
                self.isEnd = True
                return 1
            if sum(self.board[i, :]) == -3:
                self.isEnd = True
                return -1
        # col
        for i in range(BOARD_COLS):
            if sum(self.board[:, i]) == 3:
                self.isEnd = True
                return 1
            if sum(self.board[:, i]) == -3:
                self.isEnd = True
                return -1
        # diagonal
        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])
        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])
        diag_sum = max(abs(diag_sum1), abs(diag_sum2))
        if diag_sum == 3:
            self.isEnd = True
            if diag_sum1 == 3 or diag_sum2 == 3:
                return 1
            else:
                return -1

        # tie
        # no available positions
        if len(self.availablePositions()) == 0:
            self.isEnd = True
            return 0
        # not end
        self.isEnd = False
        return None

    def availablePositions(self):
        positions = []
        for i in range(BOARD_ROWS):
            for j in range(BOARD_COLS):
                if self.board[i, j] == 0:
                    positions.append((i, j))  # need to be tuple
        return positions

    def updateState(self, position):
        self.board[position] = self.playerSymbol
        # switch to another player
        self.playerSymbol = -1 if self.playerSymbol == 1 else 1

    # only when game ends
    def giveReward(self):
        result = self.winner()
        # backpropagate reward
        if result == 1:
            self.p1.feedReward(1)
            self.p2.feedReward(0)
        elif result == -1:
            self.p1.feedReward(0)
            self.p2.feedReward(1)
        else:
            self.p1.feedReward(0.1)
            self.p2.feedReward(0.5)

    # board reset
    def reset(self):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.boardHash = None
        self.isEnd = False
        self.playerSymbol = 1

    def play(self, rounds=100):
        for i in range(rounds):
            if i % 1000 == 0:
                print(&amp;quot;Rounds {}&amp;quot;.format(i))
            while not self.isEnd:
                # Player 1
                positions = self.availablePositions()
                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
                # take action and upate board state
                self.updateState(p1_action)
                board_hash = self.getHash()
                self.p1.addState(board_hash)
                # check board status if it is end

                win = self.winner()
                if win is not None:
                    # self.showBoard()
                    # ended with p1 either win or draw
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

                else:
                    # Player 2
                    positions = self.availablePositions()
                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)
                    self.updateState(p2_action)
                    board_hash = self.getHash()
                    self.p2.addState(board_hash)

                    win = self.winner()
                    if win is not None:
                        # self.showBoard()
                        # ended with p2 either win or draw
                        self.giveReward()
                        self.p1.reset()
                        self.p2.reset()
                        self.reset()
                        break
            

    # play with human
    def play2(self):
        while not self.isEnd:
            # Player 1
            positions = self.availablePositions()
            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
            # take action and upate board state
            self.updateState(p1_action)
            self.showBoard()
            # check board status if it is end
            win = self.winner()
            if win is not None:
                if win == 1:
                    print(self.p1.name, &amp;quot;wins!&amp;quot;)
                else:
                    print(&amp;quot;tie!&amp;quot;)
                self.reset()
                break

            else:
                # Player 2
                positions = self.availablePositions()
                p2_action = self.p2.chooseAction(positions)

                self.updateState(p2_action)
                self.showBoard()
                win = self.winner()
                if win is not None:
                    if win == -1:
                        print(self.p2.name, &amp;quot;wins!&amp;quot;)
                    else:
                        print(&amp;quot;tie!&amp;quot;)
                    self.reset()
                    break
        

    def showBoard(self):
        # p1: x  p2: o
        for i in range(0, BOARD_ROWS):
            print(&#39;-------------&#39;)
            out = &#39;| &#39;
            for j in range(0, BOARD_COLS):
                token = &amp;quot;&amp;quot;
                if self.board[i, j] == 1:
                    token = &#39;x&#39;
                if self.board[i, j] == -1:
                    token = &#39;o&#39;
                if self.board[i, j] == 0:
                    token = &#39; &#39;
                out += token + &#39; | &#39;
            print(out)
        print(&#39;-------------&#39;)


class Player:
    def __init__(self, name, exp_rate=0.3):
        self.name = name
        self.states = []  # record all positions taken
        self.lr = 0.3
        self.exp_rate = exp_rate
        self.decay_gamma = 0.9
        self.states_value = {}  # state -&amp;gt; value

    def getHash(self, board):
        boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS))
        return boardHash

    def chooseAction(self, positions, current_board, symbol):
        randValue = np.random.uniform(0, 1)
        value_max = value = -999
        if  randValue&amp;gt; self.exp_rate:
            
            for p in positions:
                next_board = current_board.copy()
                next_board[p] = symbol
                next_boardHash = self.getHash(next_board)
                value = -999 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)
                # print(&amp;quot;value&amp;quot;, value)
                if value &amp;gt;= value_max:
                    value_max = value
                    action = p

        if  value_max == -999 :
            # take random action
            idx = np.random.choice(len(positions))
            action = positions[idx]
        
        # print(&amp;quot;{} takes action {}&amp;quot;.format(self.name, action))
        return action

    # append a hash state
    def addState(self, state):
        self.states.append(state)

    # at the end of game, backpropagate and update states value
    def feedReward(self, reward):
        for st in reversed(self.states):
            if self.states_value.get(st) is None:
                self.states_value[st] = 0
            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])
            reward = self.states_value[st]

    def reset(self):
        self.states = []

    def savePolicy(self):
        fw = open(&#39;policy_&#39; + str(self.name), &#39;wb&#39;)
        pickle.dump(self.states_value, fw)
        fw.close()

    def loadPolicy(self, file):
        fr = open(file, &#39;rb&#39;)
        self.states_value = pickle.load(fr)
        fr.close()


class HumanPlayer:
    def __init__(self, name):
        self.name = name

    def chooseAction(self, positions):
        while True:
            row = int(input(&amp;quot;Input your action row:&amp;quot;))
            col = int(input(&amp;quot;Input your action col:&amp;quot;))
            action = (row, col)
            if action in positions:
                return action

    # append a hash state
    def addState(self, state):
        pass

    # at the end of game, backpropagate and update states value
    def feedReward(self, reward):
        pass

    def reset(self):
        pass


if __name__ == &amp;quot;__main__&amp;quot;:
    # training
    p1 = Player(&amp;quot;p1&amp;quot;)
    p2 = Player(&amp;quot;p2&amp;quot;)

    st = State(p1, p2)
    print(&amp;quot;training...&amp;quot;)
    st.play(100000)

    p1.savePolicy()

    # play with human
    p1 = Player(&amp;quot;computer&amp;quot;, exp_rate=0)
    p1.loadPolicy(&amp;quot;policy_p1&amp;quot;)

    p2 = HumanPlayer(&amp;quot;human&amp;quot;)

    st = State(p1, p2)
    st.play2()



&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nguồn&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reinforcement Learning: An Introduction phiên bản 2 của Richard S. Sutton and Andrew G. Barto&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542&#34;&gt;https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Xây dựng game xếp gạch bằng opencv và python</title>
      <link>/blog/2020-12-25---tetric/</link>
      <pubDate>Sat, 26 Dec 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-12-25---tetric/</guid>
      <description>

&lt;h1 id=&#34;mã-nguồn&#34;&gt;Mã nguồn&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import cv2
import numpy as np
from random import choice

def getColor():
    lstColor = [[255,64,64],[255,165,0],[255,244,79],[102,255,0],[172,229,238],[148,87,235],[148,87,235],[241,156,187]]
    return choice(lstColor)

def getInfo(piece):
    if piece == &amp;quot;&amp;quot;:
        coords = np.array([[0, 0]])
    elif piece == &amp;quot;I&amp;quot;:
        coords = np.array([[0, 3], [0, 4], [0, 5], [0, 6]])
    elif piece == &amp;quot;T&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [1, 5], [0, 4]])
    elif piece == &amp;quot;L&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [1, 5], [0, 5]])
    elif piece == &amp;quot;J&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [1, 5], [0, 3]])
    elif piece == &amp;quot;S&amp;quot;:
        coords = np.array([[1, 5], [1, 4], [0, 3], [0, 4]])
    elif piece == &amp;quot;Z&amp;quot;:
        coords = np.array([[1, 3], [1, 4], [0, 4], [0, 5]])
    else:
        coords = np.array([[0, 4], [0, 5], [1, 4], [1, 5]])
    
    return coords, getColor()

def display(board, coords, color, next_info, held_info, score, SPEED):
    # Generates the display
    
    border = np.uint8(127 - np.zeros([20, 1, 3]))
    border_ = np.uint8(127 - np.zeros([1, 23, 3]))
    
    dummy = board.copy()
    dummy[coords[:,0], coords[:,1]] = color
    
    right = np.uint8(np.zeros([20, 10, 3]))
    right[next_info[0][:,0] + 2, next_info[0][:,1]] = next_info[1]
    
    dummy = np.concatenate(( border, dummy, border, right, border), 1)
    dummy = np.concatenate((border_, dummy, border_), 0)
    dummy = dummy.repeat(20, 0).repeat(20, 1)
    dummy = cv2.putText(dummy, str(score), (325, 150), cv2.FONT_HERSHEY_DUPLEX, 1, [0, 0, 255], 2)
    
    # Instructions for the player
    index_pos = 300
    x_index_pos = 300
    dummy = cv2.putText(dummy, &amp;quot;A - left&amp;quot;, (x_index_pos, index_pos), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    dummy = cv2.putText(dummy, &amp;quot;D - right&amp;quot;, (x_index_pos, index_pos+25), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    dummy = cv2.putText(dummy, &amp;quot;S - drain&amp;quot;, (x_index_pos, index_pos+50), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    dummy = cv2.putText(dummy, &amp;quot;W - rotate&amp;quot;, (x_index_pos, index_pos+75), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 234])
    # dummy = cv2.putText(dummy, &amp;quot;J - rotate left&amp;quot;, (45, 300), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255])
    # dummy = cv2.putText(dummy, &amp;quot;L - rotate right&amp;quot;, (45, 325), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255])
    # dummy = cv2.putText(dummy, &amp;quot;I - hold&amp;quot;, (45, 350), cv2.FONT_HERSHEY_DUPLEX, 0.6, [0, 0, 255])
    
    cv2.imshow(&amp;quot;Tetris&amp;quot;, dummy)
    key = cv2.waitKey(int(1000/SPEED))
    
    return key

def getNextPiece():
    next_piece = choice([&amp;quot;O&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;Z&amp;quot;, &amp;quot;L&amp;quot;, &amp;quot;J&amp;quot;, &amp;quot;T&amp;quot;])

    return next_piece

SPEED = 1 # Controls the speed of the tetris pieces

# Make a board

board = np.uint8(np.zeros([20, 10, 3]))

# Initialize some variables

quit = False
place = False
drop = False
switch = False
held_piece = &amp;quot;&amp;quot;
flag = 0
score = 0
next_piece =&amp;quot;&amp;quot;
current_piece = &amp;quot;&amp;quot;
# All the tetris pieces



if __name__ == &amp;quot;__main__&amp;quot;:
    next_piece = getNextPiece()
    while not quit:
        # Check if user wants to swap held and current pieces
        if switch:
           # swap held_piece and current_piece
            held_piece, current_piece = current_piece, held_piece
            switch = False
        else:
            # Generates the next piece and updates the current piece
            current_piece = next_piece
            next_piece = getNextPiece()

        if flag &amp;gt; 0:
            flag -= 1

        # Determines the color and position of the current, next, and held pieces
        
        held_info = getInfo(held_piece)

        next_info = getInfo(next_piece)

        coords, color = getInfo(current_piece)
        if current_piece == &amp;quot;I&amp;quot;:
            top_left = [-2, 3]

        if not np.all(board[coords[:,0], coords[:,1]] == 0):
            break

        while True:
            # Shows the board and gets the key press
            key = display(board, coords, color, next_info, held_info, score, SPEED)
            # Create a copy of the position
            dummy = coords.copy()
            print(&amp;quot;speed &amp;quot;,SPEED, &amp;quot;key &amp;quot;,key,&amp;quot; &amp;quot;, ord(&amp;quot;s&amp;quot;))

            if key == ord(&amp;quot;s&amp;quot;):
                drop = True

            elif key == ord(&amp;quot;a&amp;quot;):
                # Moves the piece left if it isn&#39;t against the left wall
                if np.min(coords[:,1]) &amp;gt; 0:
                    coords[:,1] -= 1
                if current_piece == &amp;quot;I&amp;quot;:
                    top_left[1] -= 1
            elif key == ord(&amp;quot;d&amp;quot;):
                # Moves the piece right if it isn&#39;t against the right wall
                if np.max(coords[:,1]) &amp;lt; 9:
                    coords[:,1] += 1
                    if current_piece == &amp;quot;I&amp;quot;:
                        top_left[1] += 1
            # elif key == ord(&amp;quot;j&amp;quot;) or key == ord(&amp;quot;l&amp;quot;):
            #         # Rotation mechanism
            #     # arr is the array of nearby points which get rotated and pov is the indexes of the blocks within arr
                
            #     if current_piece != &amp;quot;I&amp;quot; and current_piece != &amp;quot;O&amp;quot;:
            #         if coords[1,1] &amp;gt; 0 and coords[1,1] &amp;lt; 9:
            #             arr = coords[1] - 1 + np.array([[[x, y] for y in range(3)] for x in range(3)])
            #             pov = coords - coords[1] + 1
                        
            #     elif current_piece == &amp;quot;I&amp;quot;:
            #         # The straight piece has a 4x4 array, so it needs seperate code
                    
            #         arr = top_left + np.array([[[x, y] for y in range(4)] for x in range(4)])
            #         pov = np.array([np.where(np.logical_and(arr[:,:,0] == pos[0], arr[:,:,1] == pos[1])) for pos in coords])
            #         pov = np.array([k[0] for k in np.swapaxes(pov, 1, 2)])
                
            #     # Rotates the array and repositions the piece to where it is now
                
            #     if current_piece != &amp;quot;O&amp;quot;:
            #         if key == ord(&amp;quot;j&amp;quot;):
            #             arr = np.rot90(arr, -1)
            #         else:
            #             arr = np.rot90(arr)
            #         coords = arr[pov[:,0], pov[:,1]]
            
            elif key == ord(&amp;quot;w&amp;quot;):
                        # Rotation mechanism
                # arr is the array of nearby points which get rotated and pov is the indexes of the blocks within arr
                
                if current_piece != &amp;quot;I&amp;quot; and current_piece != &amp;quot;O&amp;quot;:
                    if coords[1,1] &amp;gt; 0 and coords[1,1] &amp;lt; 9:
                        arr = coords[1] - 1 + np.array([[[x, y] for y in range(3)] for x in range(3)])
                        pov = coords - coords[1] + 1
                        
                elif current_piece == &amp;quot;I&amp;quot;:
                    # The straight piece has a 4x4 array, so it needs seperate code
                    
                    arr = top_left + np.array([[[x, y] for y in range(4)] for x in range(4)])
                    pov = np.array([np.where(np.logical_and(arr[:,:,0] == pos[0], arr[:,:,1] == pos[1])) for pos in coords])
                    pov = np.array([k[0] for k in np.swapaxes(pov, 1, 2)])
                
                # Rotates the array and repositions the piece to where it is now
                
                if current_piece != &amp;quot;O&amp;quot;:
                    if key == ord(&amp;quot;j&amp;quot;):
                        arr = np.rot90(arr, -1)
                    else:
                        arr = np.rot90(arr)
                    coords = arr[pov[:,0], pov[:,1]]
                # Hard drop set to true
                # drop = True
            # elif key == ord(&amp;quot;i&amp;quot;):
            #     # Goes out of the loop and tells the program to switch held and current pieces
            #     if flag == 0:
            #         if held_piece == &amp;quot;&amp;quot;:
            #             held_piece = current_piece
            #         else:
            #             switch = True
            #         flag = 2
            #         break
            elif key == 8 or key == 27:
                quit = True
                break

            # Checks if the piece is overlapping with other pieces or if it&#39;s outside the board, and if so, changes the position to the position before anything happened
            
            if np.max(coords[:,0]) &amp;lt; 20 and np.min(coords[:,0]) &amp;gt;= 0:
                if not (current_piece == &amp;quot;I&amp;quot; and (np.max(coords[:,1]) &amp;gt;= 10 or np.min(coords[:,1]) &amp;lt; 0)):
                    if not np.all(board[coords[:,0], coords[:,1]] == 0):
                        coords = dummy.copy()
                else:
                    coords = dummy.copy()
            else:
                coords = dummy.copy()
            
            if drop:
                    # Every iteration of the loop moves the piece down by 1 and if the piece is resting on the ground or another piece, then it stops and places it
                
                while not place:
                    if np.max(coords[:,0]) != 19:
                        # Checks if the piece is resting on something
                        for pos in coords:
                            if not np.array_equal(board[pos[0] + 1, pos[1]], [0, 0, 0]):
                                place = True
                                break
                    else:
                        # If the position of the piece is at the ground level, then it places
                        place = True
                    
                    if place:
                        break
                    
                    # Keeps going down and checking when the piece needs to be placed
                    
                    coords[:,0] += 1
                    
                    if current_piece == &amp;quot;I&amp;quot;:
                        top_left[0] += 1
                        
                drop = False

            else:
                    # Checks if the piece needs to be placed
                if np.max(coords[:,0]) != 19:
                    for pos in coords:
                        if not np.array_equal(board[pos[0] + 1, pos[1]], [0, 0, 0]):
                            place = True
                            break
                else:
                    place = True
                
            if place:
                # Places the piece where it is on the board
                for pos in coords:
                    board[tuple(pos)] = color
                    
                # Resets place to False
                place = False
                break

            # Moves down by 1

            coords[:,0] += 1
            if current_piece == &amp;quot;I&amp;quot;:
                top_left[0] += 1

        # Clears lines and also counts how many lines have been cleared and updates the score
        
        lines = 0
                
        for line in range(20):
            if np.all([np.any(pos != 0) for pos in board[line]]):
                lines += 1
                board[1:line+1] = board[:line]
                        
        
        score += lines*10

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mã nguồn này được kế thừa từ bài viết &lt;a href=&#34;https://www.learnopencv.com/tetris-with-opencv-python/&#34;&gt;https://www.learnopencv.com/tetris-with-opencv-python/&lt;/a&gt; và mình có modify lại theo sở thích cá nhân của mình. Còn một số bug mà mình chưa fix hết. Bạn đọc nào ghé ngang có đóng góp gì thì để lại comment giúp mình hen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ngưỡng (thresholding) trong opencv</title>
      <link>/blog/2020-12-24-thresholding/</link>
      <pubDate>Fri, 25 Dec 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-12-24-thresholding/</guid>
      <description>

&lt;h1 id=&#34;giá-trị-ngưỡng&#34;&gt;Giá trị ngưỡng:&lt;/h1&gt;

&lt;p&gt;Nói theo kiểu lúa hóa, trong opencv, ngưỡng là một số nằm trong đoạn từ 0 đến 255. Giá trị ngưỡng sẽ chia tách giá trị độ xám của ảnh thành 2 miền riêng biệt. Miền thứ nhất là tập hợp các điểm ảnh có giá trị nhỏ hơn giá trị ngưỡng. Miền thứ hai là tập hợp các các điểm ảnh có giá trị lớn hơn hoặc bằng giá trị ngưỡng.&lt;/p&gt;

&lt;p&gt;Đầu vào của một thuật toán phân ngưỡng trong opencv thường có input là ảnh nguồn (source image) và giá trị ngưỡng. Đầu ra là ảnh đích đã được phân ngưỡng (destination image). Một số thuật toán phân ngưỡng sẽ kèm thêm vài giá trị râu ria khác nữa, chúng ta sẽ không quan tâm đến chúng&lt;/p&gt;

&lt;p&gt;Mã giải của thuật toán phân ngưỡng:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;if src[i] &amp;gt;= T:
    dest[i] = MAXVAL
else:
    dest [i] = 0

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Có rất nhiều thuật toán phân ngưỡng dựa trên cách chúng ta xác định ngưỡng. Chúng ta sẽ tìm hiểu lần lượt các thuật toán trên.&lt;/p&gt;

&lt;h1 id=&#34;thuật-toán-simple-thresholding&#34;&gt;Thuật toán Simple Thresholding&lt;/h1&gt;

&lt;p&gt;Simple Thresholding thực hiện phân ngưỡng bằng cách thay thế giá trị lớn hơn hoặc bằng và giá trị bé hơn giá trị ngưỡng bằng một giá trị mới. Cụ thể chúng ta có thể xem mã nguồn bên dưới&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;
import cv2
import numpy as np
from matplotlib import pyplot as plt

img = cv2.imread(&#39;gradient.png&#39;,0)
ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC)
ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO)
ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV)

titles = [&#39;Original Image&#39;,&#39;BINARY&#39;,&#39;BINARY_INV&#39;,&#39;TRUNC&#39;,&#39;TOZERO&#39;,&#39;TOZERO_INV&#39;]
images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]

for i in xrange(6):
    plt.subplot(2,3,i+1),plt.imshow(images[i],&#39;gray&#39;)
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])

plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/_images/threshold.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình ảnh và thuật toán của mô hình được lấy từ trang opencv-python-tutroals.readthedocs.io&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ở đoạn code trên, chúng ta thiết lập giá trị ngưỡng là 127, với các điểm ảnh có giá trị lớn hơn hoặc bằng 127, chúng ta sẽ gán lại giá trị của nó thành 255. Và các điểm ảnh có giá trị bé hơn 127 sẽ được gán bằng 0 (mặc định).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;

double cv::threshold    (   InputArray  src,
OutputArray     dst,
double  thresh,
double  maxval,
int     type 
)   

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thuật toán sample thresholding của opencv còn có 1 tham số nữa khá quan trọng nữa là loại ngưỡng (type). Hiện tại lúc mình viết bài viết này thì opencv hỗ trợ  8 loại là: THRESH_BINARY,  THRESH_BINARY_INV, THRESH_TRUNC, THRESH_TOZERO, THRESH_TOZERO_INV, THRESH_MASK, THRESH_OTSU, THRESH_TRIANGLE. Ý nghĩa của từng loại như sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;THRESH_BINARY: Có thể dịch là ngưỡng nhị phân. Ý nghĩa y hệt những gì mình đề cập ở trên.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_BINARY_INV: Ngưỡng nhị phân đảo ngược. Có thể hiểu là nó sẽ đảo ngược lại kết quả của THRESH_BINARY.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TRUNC: Những giá trị điểm ảnh  bé hơn ngưỡng sẽ giữ nguyên giá trị, những điểm ảnh lớn hơn hoặc ngưỡng sẽ được gán lại là maxvalue.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TOZERO: Những điểm ảnh bé hơn ngưỡng sẽ bị gán thành 0, những điểm còn lại giữ nguyên.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TOZERO_INV: Những điểm ảnh nhỏ hơn giá trị ngưỡng sẽ được giữ nguyên, những điểm ảnh còn lại sẽ bị gán thành 0.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_MASK: Ở bạn opencv4, hầu như không được xài.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_OTSU: Sử dụng thuật toán Otsu để xác định giá trị ngưỡng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;THRESH_TRIANGLE: Sử dụng thuật toán Triangle  để xác định giá trị ngưỡng.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Giá trị 127 là giá trị trung bình cộng của 0 và 255 làm tròn xuống. Giá trị ngưỡng của thuật toán này đòi hỏi người sử dụng phải có mức độ hiểu biết nhất định về các loại ảnh mình đang xử lý để chọn ngưỡng cho phù hợp.&lt;/p&gt;

&lt;h1 id=&#34;adaptive-thresholding&#34;&gt;Adaptive Thresholding&lt;/h1&gt;

&lt;p&gt;Thuật toán simple thresholding hoạt động khá tốt. Tuy nhiên, nó có 1 nhược điểm là giá trị ngưỡng bị/được gán toàn cục. Thực tế khi chụp, hình ảnh chúng ta nhận được thường bị ảnh hưởng của nhiễu, ví dụ như là bị phơi sáng, bị đèn flask, &amp;hellip;&lt;/p&gt;

&lt;p&gt;Một trong những cách được sử dụng để giải quyết vấn đề trên là chia nhỏ bức ảnh thành những vùng nhỏ (region), và đặt giá trị ngưỡng trên những vùng nhỏ đó -&amp;gt; adaptive thresholding ra đời. Opencv cung cấp cho chúng ta hai cách xác định những vùng nhỏ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;import cv2 as cv
import numpy as np
from matplotlib import pyplot as plt
img = cv.imread(&#39;sudoku.png&#39;,0)
img = cv.medianBlur(img,5)
ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)
th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\
            cv.THRESH_BINARY,11,2)
th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\
            cv.THRESH_BINARY,11,2)
titles = [&#39;Original Image&#39;, &#39;Global Thresholding (v = 127)&#39;,
            &#39;Adaptive Mean Thresholding&#39;, &#39;Adaptive Gaussian Thresholding&#39;]
images = [img, th1, th2, th3]
for i in xrange(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],&#39;gray&#39;)
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://docs.opencv.org/master/ada_threshold.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình ảnh và thuật toán của mô hình được lấy từ trang docs.opencv.org&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python-cpp/&#34;&gt;

void cv::adaptiveThreshold  (   InputArray  src,
OutputArray     dst,
double  maxValue,
int     adaptiveMethod,
int     thresholdType,
int     blockSize,
double  C 
)   

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây:&lt;/p&gt;

&lt;p&gt;blockSize: Kích thước của vùng, bắt buộc phải là một số lẻ lớn hơn 0.&lt;/p&gt;

&lt;p&gt;C: hằng số, giá trị từ -255 đến 255. Có thể gán C bằng 0 để đỡ rối.&lt;/p&gt;

&lt;p&gt;adaptiveMethod nhận vào một trong hai giá trị là cv.ADAPTIVE_THRESH_MEAN_C và cv.ADAPTIVE_THRESH_GAUSSIAN_C, đó là các phương pháp tính ngưỡng.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ADAPTIVE_THRESH_MEAN_C: Tính trung bình các láng giềng xung quanh điểm cần xét trong vùng blockSize * blockSize trừ đi giá trị hằng số C.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ADAPTIVE_THRESH_GAUSSIAN_C: Nhân giá trị xung quanh điểm cần xét với trọng số gauss rồi tính trung bình của nó, sau đó trừ đi giá trị hằng số C.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;thresholdType: Tương tự như Simple Thresholding đã trình bày ở trên.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã quan tâm và theo dõi bài viết, hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Tham khảo&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-1-simple-thresholding/&#34;&gt;https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-1-simple-thresholding/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.learnopencv.com/opencv-threshold-python-cpp/&#34;&gt;https://www.learnopencv.com/opencv-threshold-python-cpp/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@anupriyam/basic-image-thresholding-in-opencv-5af9020f2472&#34;&gt;https://medium.com/@anupriyam/basic-image-thresholding-in-opencv-5af9020f2472&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hai khái niệm quan trọng giúp tăng độ chính xác của các mô hình trong machine learning</title>
      <link>/blog/2020-04-16-two-important-machine-learning-concepts-to-improve-every-model/</link>
      <pubDate>Sun, 26 Jan 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-04-16-two-important-machine-learning-concepts-to-improve-every-model/</guid>
      <description>

&lt;p&gt;Việc huấn luyên mô hình máy học có thể sẽ gây ra cho bạn một chút khó khăn nếu bạn không hiểu những thứ bạn dang làm là đúng hay sai. Trong hầu hết các trường hợp, các mô hình học máy là các &amp;ldquo;hộp đen&amp;rdquo;, chúng ta chỉ có thể &amp;ldquo;nhìn thấy&amp;rdquo; dữ liệu đầu vào và độ chính xác mà mô hình trả ra. Chúng ta không biết bên trong nó đang làm cái gì. Việc hiểu lý do tại sao mô hình cho ra kết quả tệ hại là chìa khóa cho cái &amp;ldquo;cách&amp;rdquo; mà bạn cải tiến nó.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Tìm hiểu lý do &amp;ldquo;tại sao&amp;rdquo; mô hình cho ra kết quả tệ hại bằng cách &amp;ldquo;xác định bias và variance&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tìm hiểu &amp;ldquo;cách&amp;rdquo; cải tiến mô hình bằng việc thực hiện &amp;ldquo;giảm bias và variance&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;xác-định-bias-và-variance&#34;&gt;Xác định bias và variance&lt;/h1&gt;

&lt;p&gt;Trước hết, chúng ta hãy bắt đầu nói về lỗi. Lỗi là phần không chính xác của mô hình trên tập test.&lt;/p&gt;

&lt;p&gt;$$ error = 1 - testing accuracy $$&lt;/p&gt;

&lt;p&gt;Nếu mô hình đạt độ chính xác  là 86% trên tập test, điều đó đồng nghĩa với độ lỗi là 14%. Trong 14% đó bao gồm bias và variance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/bias_variance.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Biểu đồ bias - variance. Nguồn towardsdatascience.com&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hai ý chính của hình trên cần làm rõ ở đây:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Bias là lỗi trên tập huấn luyện.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Variance  là gap giữa độ chính xác trên tập train và độ chính xác trên tập test.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Bạn hãy hình thật kỹ vào hình ở trên, nhìn đi nhìn lại 2, 3 lần. Nhắm mắt lại và nghiền ngẫm thật kỹ hai ý chính mình vừa đề cập ở trên.&lt;/p&gt;

&lt;h2 id=&#34;bias&#34;&gt;Bias&lt;/h2&gt;

&lt;p&gt;Bias mô tả khả năng học của mô hình. Giá trị bias lớn đồng nghĩa với việc mô hình cần phải học nhiều hơn nữa từ tập huấn luyện.&lt;/p&gt;

&lt;p&gt;Nếu mô hình có độ chính xác 90% trên tập train, điều đó đồng nghĩa với việc bạn có 10% bias. Bias cũng được chia làm 2 nhóm, nhóm bias có thể tránh được (avoidable bias) và nhóm bias không thể tránh được (unavoidable bias).&lt;/p&gt;

&lt;p&gt;$$ bias = 1 - trainning accuracy $$&lt;/p&gt;

&lt;h3 id=&#34;unavoidable-bias&#34;&gt;Unavoidable bias&lt;/h3&gt;

&lt;p&gt;Unavoidable bias hay còn được sử dụng dưới tên là optimal error rate. Đây là giới hạn trên của mô hình. Trong một số bài toán, ví dụ như là bài toán dự đoán giá chứng khoán, chúng ta - con người -  không thể dự đoán chính xác 100%. Do đó, trong điều kiện lý tưởng nhất, tại một thời điểm nào đó, mô hình của chúng ta vẫn cứ trả ra kết quả sai.&lt;/p&gt;

&lt;p&gt;Nếu bạn quyết định rằng mô hình có độ sai ít nhất là 4%. Nghĩa là chúng ta có 4% unavoidable bias.&lt;/p&gt;

&lt;h3 id=&#34;avoidable-bias&#34;&gt;Avoidable bias&lt;/h3&gt;

&lt;p&gt;Khác với optimal error rate và trainning error. Độ lỗi này xảy ra khi mô hình chúng ta chưa đủ độ tới. Chúng ta hoàn toàn có thể cái tiến mô hình này để giảm độ lỗi này về mức 0, v&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/bias-variance-avoidable_bias.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Biểu đồ bias - variance. Nguồn towardsdatascience.com&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bạn hãy để ý kỹ phần bias ở hình trên. Bias được chia làm 2 phần. Ở trên phần nét đứt là Unavoidable bias. Nó là điểm tới hạn của mô hình. Việc cần làm của chúng ta là huấn luyện, cải tiến mô hình, để cho đường trainning accuracy  màu đỏ tiến sát với đường nét đứt.&lt;/p&gt;

&lt;h2 id=&#34;variance&#34;&gt;Variance&lt;/h2&gt;

&lt;p&gt;Variance ý nghĩa của nó là mô tả mức độ tổng quát hóa của mô hình của bạn đối với dữ liệu mà nó chưa được huấn luyện. Và định nghĩa của nó là phần sai lệch giữa độ chính xác trên tập huấn luyện và độ chính xác tên tập test.&lt;/p&gt;

&lt;p&gt;$$ Variance = trainning accuracy - testing accuracy $$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/variance.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Biểu đồ variance. Nguồn towardsdatascience.com&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;tradeoff-giữa-bias-và-variance&#34;&gt;Tradeoff giữa bias và variance&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/bias_variance_tradeoff.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Sự đánh đổi giữa bias và variace. Nguồn towardsdatascience.com&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mình nghĩ hình trên đủ nói lên tất cả ý mình muốn nói. Khi mô hình cảng trở nên phức tạp, thì bias sẽ giảm, nhưng  mức độ tổng quát hóa cũng giảm theo (đồng nghĩa với việc variace sẽ tăng).&lt;/p&gt;

&lt;h2 id=&#34;cách-giảm-bias-và-variance&#34;&gt;Cách giảm bias và variance&lt;/h2&gt;

&lt;h3 id=&#34;cách-giảm-bias&#34;&gt;Cách giảm bias&lt;/h3&gt;

&lt;p&gt;Như đã nói ở phần trên, bias được chia thành 2 nhóm là Avoidable bias và unavoidable bias. Chúng ta không thể nào giảm Avoidable bias, nhưng chúng ta có thể giảm unavoidable bias bằng một trong các cách sau.&lt;/p&gt;

&lt;h4 id=&#34;tăng-kích-thước-mô-hình&#34;&gt;Tăng kích thước mô hình&lt;/h4&gt;

&lt;p&gt;Việc tăng kích thước mô hình là một trong những cách làm giảm avoidable bias. Mô hình càng lớn thì có càng nhiều tham số phải điều chỉnh. Có nhiều tham sos đồng nghĩa với việc mô hình sẽ học được nhiều mối quan hệ phức tạp hơn. Chúng ta có thể tăng kích thước mô hình bằng cách thêm nhiều layer hơn nữa, hoặc thêm nhiều node hơn nữa cho mỗi layer.&lt;/p&gt;

&lt;h4 id=&#34;giảm-regulation&#34;&gt;Giảm Regulation&lt;/h4&gt;

&lt;p&gt;Việc giảm regulation cũng giúp mô hình tăng độ chính xác trên tập huấn luyên. Tuy nhiên, nếu chúng ta giảm regularization  quá đà, mô hình sẽ không đạt được mức độ tổng quát hóa, và làm tăng variance. Đây là ví dụ dễ thấy nhất nhất về sự đánh đổi giữa bias và variance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/reduce_bias_reducing_regulation.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Giảm Regulation . Nguồn towardsdatascience.com&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;thay-đổi-kiến-trúc-mô-hình&#34;&gt;Thay đổi kiến trúc mô hình&lt;/h4&gt;

&lt;p&gt;Việc thay đổi kiến trúc mô hình cũng có thể giúp chúng ta đạt được độ chính xác cao hơn.&lt;/p&gt;

&lt;p&gt;Một số mục có thể thay đổi:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Thay đổi activation function ( ví dụ tanh, ReLU, sigmoid, LeakyReLU)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thay đổi loại mô hình (ANN, CNN, RNN, KNNKNN, &amp;hellip;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thay đổi các tham số (learning rate, image size, &amp;hellip;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thay đổi thuật toán tối ưu (Adam, SGD, RMSprop, …)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;thêm-đặc-trưng-mới&#34;&gt;Thêm đặc trưng mới&lt;/h4&gt;

&lt;p&gt;Việc thêm đặc trưng mới giúp chúng ta cung cấp cho mô hình nhiều thông tin hơn. Chúng ta có thể thực hiện điều này thông qua kỹ thuật feature engineering.&lt;/p&gt;

&lt;h3 id=&#34;giảm-variance&#34;&gt;Giảm variance&lt;/h3&gt;

&lt;h4 id=&#34;thêm-nhiều-dữ-liệu&#34;&gt;Thêm nhiều dữ liệu&lt;/h4&gt;

&lt;p&gt;Thêm dữ liệu là cách đơn giản nhất, thường gặp nhất để tăng độ chính xác của mô hình trong trường hợp mô hình huấn luyện của chúng ta bị hight variance. Hiệu quả của việc thêm nhiều dữ liệu vào mô hình đã được đề cập ở bài báo có tựa đề là  &lt;em&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/em&gt; của Andrej Karpathy (link: &lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/a&gt;). Việc thêm dữ liệu thường không ảnh hưởng đến độ lỗi bias, giúp làm giảm variance, nên đây là cách thường được sử dụng nhất.&lt;/p&gt;

&lt;h4 id=&#34;tăng-regularization&#34;&gt;Tăng Regularization&lt;/h4&gt;

&lt;p&gt;Việc tăng Regularization giúp mô hình chống overfitting. Qua đó giúp giảm variance, và tăng bias :(. Một só cách Regularization hot ở thời điểm hiện lại là dropout ( với biến thể là Monte Carlo Dropout), BatchNorm&amp;hellip;&lt;/p&gt;

&lt;h4 id=&#34;giảm-kích-thước-mô-hình&#34;&gt;Giảm kích thước mô hình&lt;/h4&gt;

&lt;p&gt;Việc giảm kích thước mô hình giúp cho chúng ta giảm overfitting trên tập train. Mục tiêu của Việc này làm giảm khả năng liên kết những pattern của dữ liệu. Bởi vậy, mục tiêu của nó hoàn toàn tương tự như tăng Regularization. Trong thực tế, chúng ta thường sử dụng tăng thêm Regularization hơn là giảm kích thước mô hình để chống variace.&lt;/p&gt;

&lt;h4 id=&#34;lựa-chọn-đặc-trưng-feature-selection&#34;&gt;Lựa chọn đặc trưng (feature selection)&lt;/h4&gt;

&lt;p&gt;Giảm chiều dữ liệu, bằng cách bỏ đi các đặc trưng thừa, giúp giảm nhiễu, là cách thường được sử dụng để giảm variace. Chúng ta có thể sử dụng PCA (Principal Component Analysis) để lọc ra các đặc trưng tốt hoặc kết hợp chúng với nhau để tạo các đặc trưng tốt hơn.&lt;/p&gt;

&lt;h2 id=&#34;bức-tranh-tổng-quát&#34;&gt;Bức tranh tổng quát&lt;/h2&gt;

&lt;p&gt;Sau tất cả, chúng ta sẽ xây dựng được một bức tranh tổng quan về lỗi chúng ta đang mắc phải là gì và chúng ta nên làm gì để giảm độ lỗi đó.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/bias_variance_overview.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Tổng quan . Nguồn towardsdatascience.com&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;tổng-kết&#34;&gt;Tổng kết&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Reducing Bias&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Increase model size&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reduce regularization&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Change model architecture&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add features&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reducing Variance&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Add More data&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Decrease model size&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add regularization&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Feature selection&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cảm ơn các bạn đã quan tâm và theo dõi bài viết, hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch từ link &lt;a href=&#34;https://towardsdatascience.com/two-important-machine-learning-concepts-to-improve-every-model-62fd058916b&#34;&gt;https://towardsdatascience.com/two-important-machine-learning-concepts-to-improve-every-model-62fd058916b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nguồn tự liệu từ bài viết được sử dụng trong cuốn sách Machine Learning Yearning của Andrew Ng. Các bạn có thể search theo từ khóa trên hoặc đăng ký trên site &lt;a href=&#34;http://deeplearning.net/&#34;&gt;http://deeplearning.net/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simhash</title>
      <link>/blog/2020-01-26-simhash/</link>
      <pubDate>Sun, 26 Jan 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-01-26-simhash/</guid>
      <description>

&lt;h1 id=&#34;đặt-vấn-đề&#34;&gt;Đặt vấn đề&lt;/h1&gt;

&lt;p&gt;Giả sử bạn và tôi đều thích nghe nhạc trên trang mp3.zing.vn. Mỗi người đều nghe khoảng 100 bài nhạc khác nhau. Để đo sự giống nhau giữa danh sách bài hát bạn nghe và danh sách bài hát tôi nghe, thông thường chúng ta sẽ dùng độ đo Jaccard Similarity, được đo bằng cách lấy phần giao (intersection ) chia cho phần hợp (union). Nghĩa là đếm số lượng bài hát cả hai cùng nghe (phần giao) chia cho tổng số bài hát không lặp của cả hai.&lt;/p&gt;

&lt;p&gt;Trong trường hợp bạn và tôi đều nghe 100 bài, trong đó có 30 bài giống nhau, vậy phần giao là 30, phần hợp là 170, giá trị Jaccard Similarity sẽ là &lt;sup&gt;30&lt;/sup&gt;&amp;frasl;&lt;sub&gt;170&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;Độ đo Jaccard Similarity được sử dụng ở phương pháp apriori , FP Growth, &amp;hellip; mà các bạn đã có dịp học trong môn khai phá dữ liệu ở Đại học.&lt;/p&gt;

&lt;h1 id=&#34;bài-toán-tìm-kiếm-văn-bản-tương-đồng&#34;&gt;Bài toán tìm kiếm văn bản tương đồng&lt;/h1&gt;

&lt;p&gt;Giả sử bạn quản lý một số lượng lớn văn bản (N= 1 tỷ), và xếp của bạn có nhu cầu nhóm những bài viết giống nhau thành từng cụm. Để:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Loại bỏ bớt những kết quả trùng trong khung search.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nhóm những bài viết vào từng nhóm sự kiện theo dòng thời gian, ví dụ sự kiện &amp;lsquo;cô gái giao gà&amp;rsquo;, sự kiện &amp;lsquo;dịch cúm corona&amp;rsquo;, &amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Vì một bất kể lý do nào đó mà trong lúc viết bài này tác giả chưa nghĩ ra.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Khi đó, các vấn đều sau có thể sẽ phát sinh:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nhiều phần nhỏ của văn bản này xuất hiện ở một vị trí lộn xộn nào ở  một hoặc nhiều văn bản khác.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Văn bản quá dài nên không thể lưu trữ hết lên bộ nhớ chính (RAM).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Có quá nhiều cặp văn bản cần phải so sánh.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Để giải quyết bài toán trên, chúng ta sẽ tiếp cận theo hướng sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Shingling: Chuyển văn bản thành tập ký tự, tập từ &amp;hellip;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Min-Hashing: Chuyển tập ký tự thành 1 chuỗi số hash định danh.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Locality-Sensitive Hashing: Tìm các văn bản tương đồng dựa vào chuỗi số định danh.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ở bài viết này, mình chỉ đề cập bước thứ 2 là Min-Hashing. Bước 1 và bước 3 bạn có thể tham khảo thêm trong khóa học, mình có để link bên dưới.&lt;/p&gt;

&lt;h2 id=&#34;vì-sao-phải-dùng-min-hashing&#34;&gt;Vì sao phải dùng Min-Hashing&lt;/h2&gt;

&lt;p&gt;Như bài toán đặt ra ở trên, chúng ta có 1 tỷ văn bản, chúng ta cần N(N-1)/2 = 5*10^17 phép tính Jaccard Similarity. Chúng ta có một server có thể thực hiện 5x10^6 phép so sánh, thì chúng ta phải mất 10^11 giây tương đương 31,710 năm để thực hiện xong.&lt;/p&gt;

&lt;p&gt;Thuật toán MinHash sẽ giúp chúng ta một giá trị xấp xỉ giá trị của Jaccard Similarity của hai tập dữ liệu. Ưu điểm của MinHash:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Có chiều dài đầu ra cố định&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Không phụ thuộc vào chiều dài đầu vào.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Để tính giá trị xấp xỉ Jaccard Similarity (MinHash signatures), đầu tiên ta sẽ tính MinHash của hai tập data, được 2 giá trị hash, sau đó đếm giá trị trùng nhau của 2 chuỗi hash và chia chiều dài gía trị hash, chúng ta sẽ được một giá trị xấp xỉ giá trị Jaccard Similarity.&lt;/p&gt;

&lt;p&gt;Ví dụ ta có hai tập tập dữ liệu {a,x,c,d} và {a,x,d,e} hai giá trị hash ta có tương ứng là 1234 và 1235, số ký tự trùng nhau là 3 (1,2,3), chiều dài là 4, vậy ta có giá trị Jaccard Similarity là &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;Phép tính này sẽ hơn việc tính  Jaccard Similarity truyền thống, lý do là chúng ta không cần phải tính phần giao và phần hợp của hai tập dữ liệu ( trong trường hợp hai tập có nhiều giá trị thì việc tính càng lâu), và giá trị hash thường có chiều dài ngắn hơn so với số lượng phần trử trong tập dữ liệu, ngoài ra phép so sánh cũng đơn giản hơn nhiều.&lt;/p&gt;

&lt;h2 id=&#34;thuật-toán-minhash&#34;&gt;Thuật toán MinHash&lt;/h2&gt;

&lt;p&gt;Ý tưởng của thuật toán khá đơn giản:&lt;/p&gt;

&lt;p&gt;ta có hàm hash:&lt;/p&gt;

&lt;p&gt;$$ h(x) = (ax+b)%c $$&lt;/p&gt;

&lt;p&gt;Trong đó:
- x là số nguyên đầu vào,  a và b là hai số được chọn ngẫu nhiên với điều kiện a và b &amp;lt; x&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;c là số nguyên tố được chọn ngẫu nhiên, với điều kiện c lớn hơn x.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cách thuật toán thực hiện như sau:&lt;/p&gt;

&lt;p&gt;Với 1 văn bản, chạy thuật toán hash 10 lần, do ta có số a và b là ngẫu nhiên nên 10 lần chạy sẽ cho ra các kết quả khác nhau, lấy giá trị hash nhỏ nhất (do đó thuật toán có tên là min hash) làm thành phần đầu tiên của MinHash signature. Lặp lại quá trình trên 10 lần, chúng ta có MinHash signature  với 10 giá trị.&lt;/p&gt;

&lt;p&gt;Xong thuật toán, quá dễ.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã quan tâm và theo dõi bài viết, hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Tham khảo
- Khóa học Mining of Massive Datasets chương 3 &lt;a href=&#34;http://www.mmds.org/&#34;&gt;http://www.mmds.org/&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/&#34;&gt;https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Các hàm hash có sẵn trong python</title>
      <link>/blog/2020-01-13-hash-in-python/</link>
      <pubDate>Sat, 25 Jan 2020 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2020-01-13-hash-in-python/</guid>
      <description>

&lt;h1 id=&#34;built-in-hashing&#34;&gt;Built-In Hashing&lt;/h1&gt;

&lt;p&gt;Python có xây dựng sẵn cho chúng ta một hàm hash, chúng ta cứ việc gọi ra và sử dụng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;hash(&amp;quot;pham duy tung&amp;quot;)
-7141560399917772220
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một lưu ý nhỏ là giá trị của hàm hash  sẽ khác nhau giữa các phiên bản python. Ví dụ ở trên mình xài python 3.8, với bản 3.6 sẽ là&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;hash(&amp;quot;pham duy tung&amp;quot;)
1568935795476364190
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;checksums&#34;&gt;Checksums&lt;/h1&gt;

&lt;p&gt;Chúng ta có thể sử dụng checksums để hash dữ liệu. Checksum được sử dụng trong thuật toán nén file ZIP để đảm bảo toàn vẹn dữ liệu sau khi nén. Thư viện zlib của python hỗ trợ 2 hàm tính checksum là adler32 và crc32. Để đảm bảo tốc độ chương trình và chỉ cần lấy hash đơn giản, chúng ta có thể sử dụng hàm Adler32. Tuy nhiên, nếu bạn muốn chương trình có độ tin cậy cao hoặc đơn giản là checksums, hãy sử dụng crc32. Các bạn có thể đọc bài viết ở đây &lt;a href=&#34;https://www.leviathansecurity.com/blog/analysis-of-adler32&#34;&gt;https://www.leviathansecurity.com/blog/analysis-of-adler32&lt;/a&gt; để hiểu hơn.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import zlib
&amp;gt;&amp;gt;&amp;gt; zlib.adler32(b&amp;quot;Pham Duy Tung&amp;quot;)
524616855
&amp;gt;&amp;gt;&amp;gt; zlib.crc32(b&amp;quot;Pham Duy Tung&amp;quot;)
3750031252
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;secure-hashing&#34;&gt;Secure Hashing&lt;/h1&gt;

&lt;p&gt;Mã hóa an toàn (Secure Hashing) và bảo mật dữ liệu đã được nghiên cứu và ứng dụng từ nhiều năm về trước. Tiền thân là thuật toán MD5 đến SHA1, SHA256, SHA512&amp;hellip;. Mỗi thuật toán ra đời sau sẽ cải tiến độ bảo mật và giảm đụng độ của các thuật toán trước đó.&lt;/p&gt;

&lt;p&gt;Một số hàm hash phổ biến:&lt;/p&gt;

&lt;h2 id=&#34;md5-16-bytes-128-bit&#34;&gt;MD5– 16 bytes/128 bit&lt;/h2&gt;

&lt;p&gt;Chuỗi đầu ra của  MD5 có kích thước 16 bytes hay 16*8 = 128 bits. Ở thời điểm hiện tại MD5 không còn là thuật toán phổ biến và không được khuyến khích dùng bởi các tổ chức bảo mật.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import hashlib
&amp;gt;&amp;gt;&amp;gt; hashlib.md5(b&amp;quot;Pham Duy Tung&amp;quot;).hexdigest()
&#39;58067430b9caa44f5ac1220b171f45c8&#39;
&amp;gt;&amp;gt;&amp;gt; len(hashlib.md5(b&amp;quot;Pham Duy Tung&amp;quot;).digest()) # Chiều dài của đầu ra là 16 bytes
16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chú ý:
Hàm hexdigest biểu diễn một byte thành một ký tự hex (2 ký tự  đầu 58 của ví dụ trên là giá trị hex của số 88 trong hệ thập phân)&lt;/p&gt;

&lt;h2 id=&#34;sha1-20-bytes-160-bits&#34;&gt;SHA1–20 bytes/160 bits&lt;/h2&gt;

&lt;p&gt;Đầu ra của SHA1 có chiều dài là 20 bytes tương ứng với 160 bit. Cũng giống như MD5, SHA1 cũng không được khuyến khích sử dụng ở trong các ứng dụng bảo mật.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import hashlib
&amp;gt;&amp;gt;&amp;gt; hashlib.sha1(b&amp;quot;Pham Duy Tung&amp;quot;).hexdigest()
&#39;b95b8716f15d89b6db67e2e788dea42d3fba5ee8&#39;
&amp;gt;&amp;gt;&amp;gt; len(hashlib.sha1(b&amp;quot;Pham Duy Tung&amp;quot;).digest())
20


&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sha256-32-bytes-256-bit-và-sha512-64-bytes-512-bit&#34;&gt;SHA256–32 bytes/256 bit và SHA512–64 bytes/512 bit&lt;/h2&gt;

&lt;p&gt;Đây là hai hàm hash được khuyên là nên dùng ở thời điểm hiện tại&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; hashlib.sha256(b&amp;quot;Pham Duy Tung&amp;quot;).hexdigest()
&#39;611b322b6b8ee570831c6061408ac5aa77fcdb572206d5d443855f5d3c1383c6&#39;
&amp;gt;&amp;gt;&amp;gt; len(hashlib.sha256(b&amp;quot;Pham Duy Tung&amp;quot;).digest())
32
&amp;gt;&amp;gt;&amp;gt; hashlib.sha512(b&amp;quot;Pham Duy Tung&amp;quot;).hexdigest()
&#39;ac1f6a2dd234bc15c1fa2be1db4e55ad4af8c476abb8e3d9ac3d4c74d3e151c23314e20925616e90a0bcb13a38b5531e064c586d65fed54504d713fdabee03f9&#39;
&amp;gt;&amp;gt;&amp;gt; len(hashlib.sha512(b&amp;quot;Pham Duy Tung&amp;quot;).digest())
64
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;near-duplicate-detection&#34;&gt;Near-Duplicate Detection&lt;/h1&gt;

&lt;p&gt;Các thuật toán được giới thiệu ở trên, khi chúng ta thay đổi giá trị đầu vào, dù chỉ một giá trị nhỏ thôi ở một vài vị trí nào đó, thì kết quả trả ra lại khác nhau khá lớn. Tuy nhiên, đôi khi chúng ta gặp những bài toán tìm nội dung tương tự nhau hoặc gần như tương tự nhau. Ví dụ giống như google crawler dữ liệu xác định những bài văn copy paste từ những trang web khác nhau, hoặc phát hiện đạo văn, phát hiện đạo nhạc &amp;hellip;&lt;/p&gt;

&lt;p&gt;Một thuật toán khá phổ biến nằm trong nhóm này là SimHash. Thuật toán được google sử dụng  để tìm ra các trang gần trùng nhau (theo wiki &lt;a href=&#34;https://en.wikipedia.org/wiki/SimHash&#34;&gt;https://en.wikipedia.org/wiki/SimHash&lt;/a&gt;). Tác giả của thuật toán là Moses Charikar.&lt;/p&gt;

&lt;p&gt;Để dùng Simhash, chúng ta phải cài đặt package từ kho của python&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from simhash import Simhash

&amp;gt;&amp;gt;&amp;gt; Simhash(&amp;quot;Pham Duy Tung&amp;quot;).value
17022061268703429674
&amp;gt;&amp;gt;&amp;gt; Simhash(&amp;quot;Pham Duy Tung1&amp;quot;).value
17184261516160517290

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một trong những lưu ý quan trọng khi sử dụng SimHash  ( tham khảo &lt;a href=&#34;https://stackoverflow.com/questions/49820228/how-to-compare-the-similarity-of-documents-with-simhash-algorithm/49831194#49831194&#34;&gt;https://stackoverflow.com/questions/49820228/how-to-compare-the-similarity-of-documents-with-simhash-algorithm/49831194#49831194&lt;/a&gt;)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SimHash thật sự hữu ích trong bài toán phát hiện văn bản trùng lắp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Để tìm văn bản trùng lắp chính xác, dúng ta có thể sử dụng các thuật toán đơn giản mà hiệu quả như md5, sha1sha1.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thuật toán phù hợp các văn bản lớn, không phù hợp cho các câu văn nhỏ.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Đoạn code bên dưới là một ví dụ được dùng để tìm các văn bản có đạo nội dung.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; #assuming that you have a dictionary with document id as the key and the document as the value: 
# documents = { doc_id: doc } you can do:

from simhash import simhash

def split_hash(str, num):
    return [ str[start:start+num] for start in range(0, len(str), num) ]

hashes = {}
for doc_id, doc in documents.items():
    hash = simhash(doc)

    # you can either use the whole hash for higher precision or split into chunks for higher recall
    hash_chunks = split_hash(hash, 4)

    for chunk in hash_chunks:
        if chunk not in hashes:
            hashes[chunk] = []
        hashes[chunk].append(doc_id)

# now you can print the duplicate documents:
for hash, doc_list in hashes:
    if doc_list &amp;gt; 1:
        print(&amp;quot;Duplicates documents: &amp;quot;, doc_list)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ngoài SimHash, còn một thuật toán hash khá nổi tiếng nữa cũng được google sử dụng trong việc cá nhân hóa người dùng, đó là MinHash. Ở các bài viết tiếp theo mình sẽ viết về thuật toán này.&lt;/p&gt;

&lt;h1 id=&#34;perceptual-hashing&#34;&gt;Perceptual Hashing&lt;/h1&gt;

&lt;p&gt;Loại hash cuối cùng chúng ta đề cập ở đây là  perceptual hashing. Loại hash này được sử dụng để phát hiện sự khác nhau trong tập hình ảnh hoặc trong video.&lt;/p&gt;

&lt;p&gt;Một ví dụ của các thuật toán thuộc nhóm là là được dùng để phát hiện các frame ảnh trùng lắp trong video. Thuật toán được dùng để loại bỏ những nội dung trùng lắp, giúp tiết kiệm lưu trữ. Hoặc dùng trong các thuật toán tóm tắt video.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/google_free_ds_1.png&#34; alt=&#34;Ảnh 1&#34; /&gt;
&lt;strong&gt;Ảnh 1&lt;/strong&gt;
&lt;img src=&#34;/post_image/google_free_ds_2.png&#34; alt=&#34;Ảnh 2&#34; /&gt;
&lt;strong&gt;Ảnh 2&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import hashlib
&amp;gt;&amp;gt;&amp;gt; from PIL import Image
&amp;gt;&amp;gt;&amp;gt; image1 = Image.open(&amp;quot;google_free_ds1.png&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; image1 = Image.open(&amp;quot;google_free_ds_1.png&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; image2 = Image.open(&amp;quot;google_free_ds_2.png&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; hashlib.sha256(image1.tobytes()).hexdigest()
&#39;c57d0b5b1ca64077b45bdb65f817497834675232a2fc2ed76d6b8aa7955126b9&#39;
&amp;gt;&amp;gt;&amp;gt; hashlib.sha256(image2.tobytes()).hexdigest()
&#39;02ea5e51b19cf3748f91f9bbe26976e9e14dca4b47e0aaff88ab20030a695f44&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Giá trị hash khác xa nhau, có vẻ chúng ta không thể nào sử dụng SHA256 trong bài toán này được. Lúc này, chúng ta sẽ tìm tới các thư viện thuộc nhóm Perceptual Hashing, một trong số chúng là ImageHash.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import imagehash
&amp;gt;&amp;gt;&amp;gt; hash1 = imagehash.average_hash(image1)
&amp;gt;&amp;gt;&amp;gt; hash2 = imagehash.average_hash(image2)
&amp;gt;&amp;gt;&amp;gt; hash1-hash2
24

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Giá trị hash của hai ảnh trên là khác nhau, nhưng sự khác nhau là rất ít. Chứng tỏ hai ảnh trên có thể là bản sao của nhau.&lt;/p&gt;

&lt;h1 id=&#34;kết-luận&#34;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Trong bài viết này, chúng ta đã đề cập qua các cách khác nhau để hash dữ liệu trong Python. Phụ thuộc vào bài toán, chúng ta sẽ sử dụng các thuật toán với các tham số phù hợp. Hi vọng bài viết này sẽ ít nhiều giúp ích được cho các bạn.&lt;/p&gt;

&lt;p&gt;Chú thích:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ảnh cover của bài viết là ảnh của chùm sao thất tinh bắc đẩu mình chụp từ trang &lt;a href=&#34;https://stellarium-web.org/&#34;&gt;https://stellarium-web.org/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;hash collision : Khi cho 2 input khác nhau vào hàm hash mà cùng ra một output -&amp;gt; collision.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nguồn bài viết:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/better-programming/how-to-hash-in-python-8bf181806141&#34;&gt;https://medium.com/better-programming/how-to-hash-in-python-8bf181806141&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu Non-maximum Suppression (NMS)</title>
      <link>/blog/2019-12-25-nms/</link>
      <pubDate>Fri, 13 Dec 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-12-25-nms/</guid>
      <description>

&lt;h2 id=&#34;đặt-vấn-đề&#34;&gt;Đặt vấn đề&lt;/h2&gt;

&lt;p&gt;Sau khi thực hiện object detection feed một ảnh qua mạng neural, chúng ta sẽ thu được rất nhiều proposals (như hình ở dưới). Ở trạng thái này, có rất nhiều proposals là boding box cho một object duy nhất, điều này dẫn tới việc dư thừa. Chúng ta sử dụng thuật toán Non-maximum suppression (NMS) để giải quyết bài toán này.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/proposals.JPG&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 1: Proposals box, hình được cắt từ bài báo&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;thuật-toán-nms&#34;&gt;Thuật toán NMS&lt;/h2&gt;

&lt;p&gt;Đầu vào:&lt;/p&gt;

&lt;p&gt;Tập danh sách các proposals box ký hiệu là B với B ={b1,b2,&amp;hellip;,bn}, với bi là proposal thứ i.&lt;/p&gt;

&lt;p&gt;Tập điểm của mỗi proposal box ký hiệu là S với S={s1,s2,&amp;hellip;,sn}, si là điểm confidence của box bi&lt;/p&gt;

&lt;p&gt;Giá trị ngưỡng overlap threshold N.&lt;/p&gt;

&lt;p&gt;Cả hai giá trị bi và si đều là output của mạng neural network.&lt;/p&gt;

&lt;p&gt;Đầu ra:&lt;/p&gt;

&lt;p&gt;Một tập các proposals box D là tập các proposals đã loại bỏ dư thừa tương ứng với từng object trong hình.&lt;/p&gt;

&lt;p&gt;Thuật toán:&lt;/p&gt;

&lt;p&gt;Bước 1: Khởi tạo tập output  D = {}&lt;/p&gt;

&lt;p&gt;Bước 2: Chọn ra proposal box có điểm confidence cao nhất trong tập S, loại box đó ra khỏi tập S, B và thêm nó vào tập D.&lt;/p&gt;

&lt;p&gt;Bước 3: Tính giá trị IOU giữa proposal box mới vừa loại ra ở bước 2 với toàn bộ proposal box trong tập B. Nếu có bất kỳ box nào đó có giá trị IOU lớn hơn giá trị ngưỡng N thì loại box đó ra khỏi B, S.&lt;/p&gt;

&lt;p&gt;Bước 4: Lặp lại bước 2 đến khi nào không còn box nào có trong tập B.&lt;/p&gt;

&lt;p&gt;Điểm yếu của thuật toán:&lt;/p&gt;

&lt;p&gt;Nếu bạn đọc kỹ thuật toán, bạn sẽ thấy rằng toàn bộ quá trình loai bỏ những box dư thừa đều phụ thuộc vào giá trị ngưỡng N. Việc chọn lựa giá trị N chính là chìa khóa thành công của mô hình. Tuy nhiên, việc chọn giá trị ngưỡng này trong các bài toán khá khó. Và với việc chỉ sử dụng giá trị N, chúng ta sẽ gặp trường hợp dưới đây.&lt;/p&gt;

&lt;p&gt;Giả sửa giá trị ngưỡng N bạn chọn là 0.5. Có nghĩa là nếu box có giá trị lớn IOU đều bị loại bỏ, ngay cả với trường hợp điểm score si của nó  có giá trị cao. Ngược lại, giả sử box có điểm score si thấp nhưng IOU của nó nhỏ hơn 0.5, ví dụ o.49, thì nó lại được nhận.&lt;/p&gt;

&lt;p&gt;Và để giải quyết bài toán này Navaneeth Bodla đã đề xuất một cải tiến nhỏ và đặt tên thuật toán là Soft-NMS. ý tưởng được đề ra như sau: Thay vì phải loại bỏ hoàn toàn proposal, chúng ta sẽ giảm giá trị confidence của box đi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/soft_mns.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;soft-nms, hình được cắt từ bài báo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Với giá trị si được cập nhật lại như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/soft_nms_si.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;soft-nms, hình được cắt từ bài báo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi bài viết. Hẹn gặp lại các bạn ở những bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Tham khảo&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@yusuken/object-detction-1-nms-ed00d16fdcf9&#34;&gt;https://medium.com/@yusuken/object-detction-1-nms-ed00d16fdcf9&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c&#34;&gt;https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1704.04503.pdf&#34;&gt;https://arxiv.org/pdf/1704.04503.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1705.02950.pdf&#34;&gt;https://arxiv.org/pdf/1705.02950.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu mạng AlexNet, mô hình giành chiến thắng tại cuộc thi ILSVRC 2012</title>
      <link>/blog/2019-05-27-alexnet/</link>
      <pubDate>Mon, 27 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-27-alexnet/</guid>
      <description>

&lt;p&gt;Trong bài viết này, chúng ta sẽ tìm hiểu mô hình AlexNet từ nhóm của giáo sư Hinton. Tới thời điểm hiện tại (2019-05-27), bài viết của giáo sư đã có hơn 40316 lượt trích dẫn. Bài báo này có bước đóng góp cực kỳ quan trọng, là một đột phá lớn trong lĩnh vực deep learning, mở đầu cho sự quay lại của mạng neural network và đóng góp trực tiếp vào thành công của những chương trình trí tuệ nhân tạo tại thời điểm hiện tại.&lt;/p&gt;

&lt;p&gt;Về bài báo gốc của tác giả, mình có để ở phần trích dẫn bên dưới. Các bạn có nhu cầu tìm hiểu có thể tìm và đọc. Theo ý kiến riêng của mình, đây là một bài báo &lt;em&gt;rất nên đọc và phải đọc&lt;/em&gt;. Trước đây mình đã có viết 1 bài về tập AlexNet nhưng chưa đầy đủ, bài đó mình chỉ giới thiệu phớt phớt qua mạng AlexNet. Trong bài viết này, mình sẽ trình bày kỹ hơn.&lt;/p&gt;

&lt;p&gt;Sơ lược một chút, tập dữ liệu ImageNet là tập dataset có khoảng 15 triệu hình ảnh có độ phân giải cao đã được gán nhãn (có khoảng 22000 nhãn). Cuộc thi ILSVRC  sử dụng một phần nhỏ của tập ImageNet với khoảng 1.2 triệu ảnh của 1000 nhãn (trung bình mỗi nhãn có khoảng 1.2 ngàn hình ảnh) làm tập train, 50000 ảnh làm tập validation và 150000 ảnh làm tập test (tập validation và tập test đều có 1000 nhãn thuộc tập train).&lt;/p&gt;

&lt;h1 id=&#34;kiến-trúc-mạng-alexnet&#34;&gt;Kiến trúc mạng AlexNet&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/alexnet_architecture.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Kiến trúc mô hình AlexNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mạng AlexNet bao gồm 8 lớp (tính luôn lớp input là 9), bao gồm:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Input&lt;/em&gt;: có kích thước 224x224x3 (Scale ảnh đầu vào về dạng 224x224x3, thực chất ảnh của tập ImageNet có size tùy ý)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ nhất&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Convolution Layer có kích thước 11x11x3 với stride size = 4 và pad = 0. Kết quả sau bước này ta được tập feature map có kích thước 55x55x96 (mình nghĩ là các bạn sẽ biết cách tính sao cho ra số 55, mình cũng đã đề cập vấn đề cách tính này ở 1 bài viết trước đây).

Tiếp theo là một Overlapping Max Pooling 3x3 có stride =2 =&amp;gt; feature maps = 27x27x96.

Tiếp theo là Local Response Normalization =&amp;gt; feature maps = 27x27x96.

Xong lớp thứ nhất
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ hai&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Convolutional Layer: 256 kernels có kích thước 5x5x48 (stride size = 1, pad = 2) =&amp;gt; 27x27x256 feature maps.

Overlapping Max Pooling 3x3 có stride =2 =&amp;gt; feature maps = 13x13x256.

Tiếp theo là Local Response Normalization =&amp;gt; feature maps = 13x13x256.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ ba&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Convolutional Layer: 384 kernels có kích thước 3x3x256 (stride size = 1, pad = 1) =&amp;gt; 13x13x384 feature maps.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ bốn&lt;/em&gt;: 384 kernels có kích thước 3x3x192 (stride size = 1, pad = 1) =&amp;gt; 13x13x384 feature maps.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ năm&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Convolutional Layer: 256 kernels có kích thước 3x3x192 (stride size = 1, pad = 1) =&amp;gt; 13x13x256 feature maps.

Overlapping Max Pooling 3x3 có stride =2 =&amp;gt; feature maps = 6x6x256.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ sáu&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Full connected (hay còn gọi là Dense layer) với 4096 neurals
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ bảy&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Full connected  với 4096 neurals
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Lớp thứ tám&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Full connected ra output 1000 neural (do có 1000 lớp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hàm độ lỗi được sử dụng là Softmax.&lt;/p&gt;

&lt;p&gt;Tổng cộng, chúng ta có 60 triệu tham số được sử dụng để huấn luyện.&lt;/p&gt;

&lt;h1 id=&#34;cải-tiến-của-mô-hình-để-giảm-error-rate&#34;&gt;Cải tiến của mô hình để giảm error rate&lt;/h1&gt;

&lt;h2 id=&#34;sử-dụng-relu-thay-cho-tanh&#34;&gt;Sử dụng ReLU thay cho TanH&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/relu_activation_function.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hàm kích hoạt ReLU và TanH&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các mô hình neural network trước khi bài báo ra đời thường sử dụng hàm Tanh làm hàm kích hoạt. Mô hình AlexNet không sử dụng hàm TanH mà giới thiệu một hàm kích hoạt mới là ReLU. ReLU giúp cho quá trình huấn luyện chạy nhanh hơn gấp 6 lần so với kiến trúc tương tự sử dụng TanH, góp một phần vào việc độ lỗi trên tập huấn luyện là 25%.&lt;/p&gt;

&lt;h2 id=&#34;local-response-normalization&#34;&gt;Local Response Normalization&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/local_response_norm_vs_batch_norm.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Local Response Normalization và Batch Normalization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Trong mạng AlexNet, nhóm tác giả sử dụng hàm chuẩn hóa là Local Response Normalization. Hàm này không phải là Batch Normalization mà các bạn hay sử dụng ở thời điểm hiện tại (xem hình ở trên, hai hàm có công thức tính toán hoàn toàn khác nhau). Việc sử dụng chuẩn hóa (Normalization) giúp tăng tốc độ hội tụ. Ngày nay, chúng ta không còn sử dụng Local Response Normalization nữa. Thay vào đó, chúng ta sử  dụng Batch Normalization làm hàm chuẩn hóa.&lt;/p&gt;

&lt;p&gt;Với việc sử dụng hàm chuẩn hóa Local Response Normalization, độ lỗi top-1 error rate giảm 1.4%, top-5 giảm 1.2%.&lt;/p&gt;

&lt;h2 id=&#34;overlapping-pooling&#34;&gt;Overlapping Pooling&lt;/h2&gt;

&lt;p&gt;Overlapping Pooling là pooling với stride nhỏ hơn kernel size. Một khái niệm ngược với Overlapping Pooling là Non-Overlapping Pooling với stride lớn hoăn hoặc bằng kernel.&lt;/p&gt;

&lt;p&gt;Mạng AlexNet sử dụng Overlapping Pooling ở hidden layer thứ 1, 2 và 5 (Kernel size = 3x3, stride =2).&lt;/p&gt;

&lt;p&gt;Với việc sử dụng overlapping pooling, top-1 error rates giảm 0.4%, top-5 error rate giảm 0.3%.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-data-augmentation&#34;&gt;Sử dụng Data Augmentation&lt;/h2&gt;

&lt;p&gt;Dữ liệu của tập huấn luyện khá nhiều, 1.2 triệu mẫu. Nhưng chia ra cho 1000 lớp thì mỗi lớp có khoảng 1200, khá khiêm tốn phải không. Cho nên, tác giả đã nghĩ ra một cách khá hay để tăng số lượng hình ảnh mà vẫn giữ được tính IID của dữ liệu, đó là sử dụng các phép biến đổi affine trên dữ liệu ảnh gốc để thu thêm nhiều ảnh hơn.&lt;/p&gt;

&lt;p&gt;Có hai dạng Data Augentation được tác giả sử dụng&lt;/p&gt;

&lt;p&gt;Dạng thứ nhất: Image translation và horizontal reflection (mirroring)&lt;/p&gt;

&lt;p&gt;Image translation được hiểu như sau: ảnh ImageNet gốc có kích thước 256x256 pixel, tác giả rút ra một ảnh con có kích thước 224x224 pixel, sau đó dịch qua trái 1 pixel và lấy 1 ảnh con tiếp theo có kích thước 224x224. Làm như vậy theo hàng, hết hàng làm theo cột. Cuối cùng tác giả có thể từ một bức hình 256x256 ban đầu rút trích thành 1024 hình có kích thước 224x224&lt;/p&gt;

&lt;p&gt;horizontal reflection (mirroring) được hiểu là lấy ảnh phản chiếu của ánh gốc qua đường chéo chính. Ví dụ con báo dang có hướng tai của nó từ trái qua phải, ta lấy horizontal reflection của ảnh đó thì sẽ được con báo hướng tai từ phải qua trái.&lt;/p&gt;

&lt;p&gt;Với việc kết hợp Image translation và horizontal reflection (mirroring), tác giả có thể rút tối đa 2048 bức ảnh khác nhau chỉ từ 1 bức ảnh gốc =&amp;gt; với hơn 1000 bức ảnh của 1 nhãn có thể sinh ra tối đa là 2048000 bức ảnh, một con số khá lớn phải không các bạn.&lt;/p&gt;

&lt;p&gt;Ở tập test, tác giả sử dụng 4 hình 224x224 ở bốn góc cộng với 1 hình 224x224 ở trung tâm =&amp;gt; được 5 hình, đem 5 hình đó sử dụng horizontal reflection thì thu được 10 hình cho mỗi file test.&lt;/p&gt;

&lt;p&gt;Dạng thứ hai: Thay đổi độ sáng&lt;/p&gt;

&lt;p&gt;Thực hiện tính PCA trên tập train. Với mỗi hình trên tập train, thay đổi giá trị độ sáng&lt;/p&gt;

&lt;p&gt;$$[p_1, p_2, p_3][\alpha_1 \gamma_1, \alpha_2 \gamma_2, \alpha_3 \gamma_3]^T$$&lt;/p&gt;

&lt;p&gt;với pi và gammai là giá trị trị riêng và vector riêng thứ i của ma trận hiệp phương sai 3x3 của ảnh, và alpha i là một giá trị ngẫu nhiên thuộc đoạn 1 và độ lệch chuẩn 0.1..&lt;/p&gt;

&lt;p&gt;Với việc sử dụng data augmentation, top-1 error rate giảm 1% độ lỗi.&lt;/p&gt;

&lt;h2 id=&#34;dropout&#34;&gt;Dropout&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/drop_out.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Dropout&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Với mỗi layer sử dụng dropout, mỗi neural sẽ có cơ hội không đóng góp vào feed forward và backpropagation. Do đó, mỗi neural đều có cơ hội  rất lớn đóng góp vào thuật toán, và chúng ta sẽ giảm thiểu tình trạng phụ thuộc vào một vài neural.&lt;/p&gt;

&lt;p&gt;Không sử dụng dropout trong tập quá trình test.&lt;/p&gt;

&lt;p&gt;Mạng AlexNet sử dụng giá trị xác xuất của dropout là 0.5  ở hai fully-connected layer. &lt;em&gt;Dopout được xem như là một kỹ thuật chuẩn hóa nhằm mục đích giảm overfitting.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-nhiều-gpu&#34;&gt;Sử dụng nhiều GPU&lt;/h2&gt;

&lt;p&gt;Tại năm 2012, nhóm tác giả sử dụng card đồ họa NIVIDIA GTX 580 có 3GB bộ nhớ RAM. Cho nên, để có thể huấn luyện được mô hình AlexNet trên GPU, mô hình cần sử dụng  2 GPU.&lt;/p&gt;

&lt;p&gt;vì vậy &lt;em&gt;việc sử dụng 2 hoặc nhiều GPU là do vấn đề thiếu bộ nhớ, chứ không phải là vấn đề tăng tốc quá trình train hơn so với 1 GPU&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ngoài ra, do giới hạn của GPU, nên mô hình AlexNet được tách ra làm 2 phần, mỗi phần được huấn luyện trên 1 GPU. Phiên bản 1 GPU của mô hình có tên là CaffeNet, và đòi hỏi chúng ta phải sử dụng GPU có bộ nhớ RAM lớn hơn hoặc bằng 6GB.&lt;/p&gt;

&lt;h1 id=&#34;một-số-chi-tiết-khác-về-các-learning-param&#34;&gt;Một số chi tiết khác về các learning param&lt;/h1&gt;

&lt;p&gt;Batch size: 128&lt;/p&gt;

&lt;p&gt;Momemtum: 0.9&lt;/p&gt;

&lt;p&gt;Weight Decay: 0.0005&lt;/p&gt;

&lt;p&gt;Learning rate: 0.01, giá trị learning rate sẽ giảm đi 10 lần nếu validation error rate không thay đổi trong 1 khoảng thời gian. Số lần giảm là 3.&lt;/p&gt;

&lt;p&gt;Epoch: 90&lt;/p&gt;

&lt;p&gt;Nhóm tác giả đã sử dụng 2 GPU 580 có  3GB GPU RAM và tốn 6 ngày để huấn luyện.&lt;/p&gt;

&lt;h1 id=&#34;kết-quả&#34;&gt;Kết quả&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/alexnet_result_1.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Độ lỗi của AlexNet trên ILSVRC 2010&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Trong cuộc thi ILSVRC 2010, AlexNet đạt độ chính xác top-1 error 37.5% và top-5 error là 17.0%, kết quả này tốt hơn vượt trội so với các cách tiếp cận khác.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/alexnet_result_2.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Độ lỗi của AlexNet trên ILSVRC 2012&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Đến cuộc thi ILSVRC 2012, độ lỗi của AlexNet trên tập validation giảm còn 18.2%.&lt;/p&gt;

&lt;p&gt;Nếu lấy trung bình của dự đoán trên 5 mạng AlexNet được huấn luyện khác nhau, độ lỗi giảm còn 16.4%. Các lấy trung bình trên nhiều hơn 1 mạng CNN là một kỹ thuật &lt;em&gt;boosting&lt;/em&gt; và được sử dụng trước đó ở bài toán phân loại số của mạng LeNet.&lt;/p&gt;

&lt;p&gt;Ở dòng số 3 là mạng AlexNet nhưng được thêm 1 convolution layer nữa (nên được ký hiệu là 1CNN*), độ lỗi trên tập validation giảm còn 16.4%.&lt;/p&gt;

&lt;p&gt;Nếu lấy kết quả trung bình của 2 mạng neural net được chỉnh sửa (thêm 1 convolution layer) và 5 mạng AlexNet gốc (=&amp;gt; chúng ta có 7CNN*), độ lỗi trên tập validation giảm xuống 15.4%&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/alexnet_result_3.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Demo kết quả top-5 của mạng AlexNet&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;mạng-caffenet&#34;&gt;Mạng CaffeNet&lt;/h1&gt;

&lt;p&gt;Mạng này là phiên bản kiến trúc 1-GPU của AlexNet. Kiến trúc của mạng caffeNet như hình bên dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/caffenet.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Mạng caffeNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bạn thấy đó, thay vì có 2 phần trên và dưới như mô ình AlexNet ở trên, mô hình CaffeNet chỉ có 1 phần. Ví dụ lớp hidden layer thứ 7 mạng AlexNet gồm 2 phần, mỗi phần có kích thước 2048, còn ở phiên bản CaffeNet thì đã gộp lại thành 1 phần.&lt;/p&gt;

&lt;p&gt;Tài liệu tham khảo&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34;&gt;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.image-net.org/challenges/LSVRC/&#34;&gt;http://www.image-net.org/challenges/LSVRC/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi bài viết, có chỗ nào bạn chưa rõ hoặc mình viết bị sai, các bạn vui lòng để lại comment để mình sửa lại cho đúng.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu mạng MobileNetV1</title>
      <link>/blog/2019-05-26-mobilenetv1/</link>
      <pubDate>Sat, 25 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-26-mobilenetv1/</guid>
      <description>

&lt;p&gt;Trong bài viết này, chúng ta sẽ tìm hiểu mô hình MobileNetV1 từ nhóm tác giả đến từ Google. Điểm cải tiến (chắc là cải tiến :) của mô hình là sử dụng một cách tính tích chập có tên là &lt;em&gt;Depthwise Separable Convolution&lt;/em&gt; để giảm kích thước mô hình và giảm độ phức tạp tính toán. Do đó, mô hình sẽ hữu ích khi chạy các ứng dụng trên di động và các thiết bị nhúng.&lt;/p&gt;

&lt;p&gt;Lý do:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mô hình có ít tham số hơn -&amp;gt; kích thước model sẽ nhỏ hơn.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mô hình có ít phép tính cộng trừ nhân chia hơn -&amp;gt; độ phức tạp sẽ nhỏ hơn.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hiện tại (2019-05-26), tại thời điểm viết bài, bài viết gốc của tác giả đã được 1594 lượt trích dẫn. Các bạn có thể tìm đọc bài báo gốc của tác giả tại trang &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;https://arxiv.org/abs/1704.04861&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/cimobilenetv1_citations.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Số lượt trích dẫn bài báo MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;chi-tiết-về-mạng-mobilenet&#34;&gt;Chi tiết về mạng MobileNet&lt;/h1&gt;

&lt;h2 id=&#34;mô-hình-kiến-trúc&#34;&gt;Mô hình kiến trúc&lt;/h2&gt;

&lt;p&gt;Kiến trúc mạng MobileNet được trình bày bên dưới. Hình bên dưới được trích từ bài báo gốc của tác giả&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenetv1_architecture.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Mô hình kiến trúc mạng MobileNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Diễn dịch ra ngôn ngữ tự nhiên, chúng ta thấy rằng mô hình có 30 lớp với các đặc điểm sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Lớp 1:  Convolution layer với stride bằng 2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lớp 2: Depthwise layer&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lớp 3: Pointwise layer&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lớp 4: Depthwise layer với stride bằng 2 (khác với bước 2, dw lớp 2 có stride size bằng 1)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lớp 5: Pointwise layer&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lớp 30: Softmax, dùng để phân lớp.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;depthwise-separable-convolution&#34;&gt;Depthwise Separable Convolution&lt;/h2&gt;

&lt;p&gt;Depthwise separable convolution  là một &lt;em&gt;depthwise convolution theo sau bởi một pointwise convolution&lt;/em&gt; như hình bên dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/depthwise_separable_convolution.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Cấu trúc của một Depthwise Separable Convolution&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Depthwise convolution: là một &lt;em&gt;channel-wise DK×DK spatial convolution&lt;/em&gt;. Ví dụ ở hình trên, ta có 5 channels (các bạn để ý cục đầu tiên có 5 khối hộp, cục thứ 2 là phân tách 5 khối hộp ra thành ma trận mxn, cục thứ 3 là spatial convolution có kích thước kxk, cục thứ 4 là kết quả sau khi convolution, cục thứ 5 là ráp 5 cái kết quả của convolution lại ), do đó chúng ta sẽ có 5 DK×DK spatial convolution tương ứng với 5 channel trên.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pointwise convolution: đơn giản là một convolution có kích thước 1x1 (như hình ở trên).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Với M là số lượng input channel, N là số lượng output channel, Dk là kernel size, Df là feature map size (với dataset ImageNet thì input có kích thước là 224, do đó feature map ban đầu có Df = 224), chúng ta có thể tính được:&lt;/p&gt;

&lt;p&gt;Chi phí tính toán của Depthwise convolution là :&lt;/p&gt;

&lt;p&gt;$$D_k \cdot D_k \cdot M \cdot D_f \cdot D_f$$&lt;/p&gt;

&lt;p&gt;Chi phí tính toán của Pointwise convolution là :&lt;/p&gt;

&lt;p&gt;$$M \cdot N \cdot D_f \cdot D_f$$&lt;/p&gt;

&lt;p&gt;Tổng chi phí tính toán của Depthwise Separable Convolution là:&lt;/p&gt;

&lt;p&gt;$$D_k \cdot D_k \cdot M \cdot D_f \cdot D_f + M \cdot N \cdot D_f \cdot D_f$$&lt;/p&gt;

&lt;p&gt;Nếu chúng ta không sử dụng Depthwise Separable Convolution mà sử dụng phép convolution như bình thường, chi phí tính toán là&lt;/p&gt;

&lt;p&gt;$$ D_k \cdot D_k \cdot M \cdot N \cdot D_f \cdot D_f$$&lt;/p&gt;

&lt;p&gt;Do đó, chi phí tính toán sẽ giảm:&lt;/p&gt;

&lt;p&gt;$$\frac{D_k \cdot D_k \cdot M \cdot D_f \cdot D_f + M \cdot N \dot D_f \cdot D_f}{D_k \cdot D_k \cdot M \cdot N \cdot D_f \cdot D_f} =  \frac{1}{N} + \frac{1}{D^2_k}$$&lt;/p&gt;

&lt;p&gt;Giả sử, chúng ta chọn kernel size Dk = 3, chúng ta sẽ giảm từ 8 đến 9 lần phép tính nhân =&amp;gt; giảm chi phí tính toán đi rất nhiều.&lt;/p&gt;

&lt;p&gt;Một chú ý nhỏ về kiến trúc ở đây, là sau mỗi convolution MobileNet sẽ sử dụng Batch Normalization (BN) và ReLU như hình bên dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/standard_convolution_vs_depthwise_seperable_convolution.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Standard Convolution bên trái, Depthwise separable convolution với BN và ReLU bên phải&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So sánh kết quả của việc sử dụng mạng 30 layer sử dụng thuần Convolution và mạng 30 layer sử dụng  Depthwise Separable Convolution (MobileNet) trên tập dữ liệu ImageNet, chúng ta có bảng kết quả bên dưới&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/standard_convolution_vs_depthwise_seperable_convolution_imagenetds.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Standard Convolution bên trái, Depthwise separable convolution với BN và ReLU bên phải&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MobileNet giảm 1% độ chính xác, nhưng số lượng tham số của mô hình và số lượng phép tính toán giảm đi rất rất nhiều, gần xấp xỉ 90%. Một con số đáng kinh ngạc.&lt;/p&gt;

&lt;h2 id=&#34;làm-mô-hình-gọn-nhẹ-hơn-nữa&#34;&gt;Làm mô hình gọn nhẹ hơn nữa&lt;/h2&gt;

&lt;p&gt;Với mong muốn làm mô hình gọn nhẹ hơn nữa, nhóm tác giả đã thêm vào hai tham số alpha và rho.&lt;/p&gt;

&lt;p&gt;Tham số alpha: Điều khiển số lượng channel (M và N).&lt;/p&gt;

&lt;p&gt;Chi phí tính toán của depthwise separable convolution khi sử dụng thêm tham số alpha.&lt;/p&gt;

&lt;p&gt;$$D_k \cdot D_k \cdot \alpha M \cdot D_f \cdot D_f + \alpha M \cdot \alpha N \cdot D_f \cdot D_f$$&lt;/p&gt;

&lt;p&gt;Giá trị alpha nằm trong đoạn [0,1], nhóm tác giả set giá trị alpha có bước nhảy là 0.25, các giá trị cần xét là 0.25, 0.5, 0.75, 1. Trường hợp alpha = 1 chính là mạng MobileNet baseline của mình. Trong trường hợp thay đổi alpha, số phép tính toán, số tham số, cũng giảm đi rất nhiều, và tất nhiên, độ chính xác cũng giảm đi tương ứng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_alpha_changes.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Mạng MobileNet với alpha thay đổi&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Phân tích kỹ hình ở trên, ta thấy rằng với alpha bằng  0.75 và 0.5 giá trị độ chính xác còn nằm ở mức miễn cưỡng có thể chấp nhận được. Nhưng với alpha bằng 0.25 thì khó mà có thể chấp nhận được kết quả đó. Việc giảm phép tính toán và số lượng tham số dẫn đến kết quả tệ như trên quả là một điều không nên. Mình nghĩ ở đây nhóm tác giả để con số để có ý nghĩa so sánh.&lt;/p&gt;

&lt;p&gt;Tham số rho: Tham số này được sử dụng để điều khiển độ phân giải của ảnh input.&lt;/p&gt;

&lt;p&gt;Chi phí tính toán của depthwise separable convolution khi sử dụng thêm tham số rho.&lt;/p&gt;

&lt;p&gt;$$D_k \cdot D_k \cdot \alpha M \cdot \rho D_f \cdot \rho D_f + \alpha M \cdot \alpha N \cdot \rho D_f \cdot \rho D_f$$&lt;/p&gt;

&lt;p&gt;Giá trị rho cũng nằm trong đoạn [0,1]. Nhóm tác giả sử dụng các giá trị độ phân giải là 224 (độ phân giải gốc, tương ứng với rho =1), 192, 160, 128.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_beta_changes.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Mạng MobileNet với rho thay đổi&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Giá trị độ chính xác thay đổi theo hướng giảm khá mượt. Việc thay đổi rho chỉ làm giảm số lượng phép tính toán, không làm giảm số lượng tham số. Việc giảm độ chính xác có thể lý giải lý do là có một số hình có kích thước nhỏ nên khi giảm kích thước sẽ làm mất những đặc trưng cần thiết của đối tượng cần xét.&lt;/p&gt;

&lt;h1 id=&#34;so-sánh-mobilenet-với-các-state-of-the-art-đương-thời&#34;&gt;So sánh MobileNet với các State-of-the-art đương thời&lt;/h1&gt;

&lt;p&gt;Khi so sánh 1.0 MobileNet-224 với GoogleNet và VGG 16 (hình bên dưới), chúng ta thấy rằng độ chính xác của cả 3 thuật toán là hầu như tương đương nhau. Nhưng 1.0 MobileNet-224 có số lượng tham số ít (75% so với GoogleNet) và số lượng phép toán nhỏ hơn rất nhiều =&amp;gt; chạy nhanh hơn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_compare_1.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;So sánh 1.0 MobileNet-224 với GoogleNet và VGG 16 trên tập ImageNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Với mô hình 0.50 MobileNet-160, chúng ta có thể so sánh với mô hình Squeezenet và AlexNet (mô hình thắng giải nhất cuộc thi ILSVRC 2012). Một lần nữa, mô hình 0.50 MobileNet-160 cho kết quả tốt hơn, nhưng có số lượng phép tính toán ít hơn rất nhiều (hơi đáng buồn là số lượng tham số của mô hình 0.50 MobileNet-160 khá cao, số lượng tham số gấp đôi so với AlexNet và gần bằng Squeezenet) =&amp;gt; 0.50 MobileNet-160 train nhanh hơn, predict cũng nhanh hơn so với Squeezenet và AlexNet, nhưng tốn bộ nhớ RAM hơn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_compare_2.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;So sánh 0.50 MobileNet-160 với Squeezenet và AlexNet trên tập ImageNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So với mô hình Inception-v3 (mô hình thắng giải nhất cuộc thi ILSVRC 2015), MobileNet cho kết quả khá tốt, nhưng số tham số và số lượng phép tính toán nhỏ hơn rất nhiều&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_compare_3.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;So sánh Mobile net và Inception-v3 trên tập Stanford Dog&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thí nghiệm ở dưới trên các tập dataset khác nhau chứng minh mức độ hiệu quả của MobileNet
&lt;img src=&#34;/post_image/mobilenet_compare_4.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;GPS Localization Via Photos&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_compare_5.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Face Attribute Classification&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_compare_6.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;MMicrosoft COCO Object Detection Dataset&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mobilenet_compare_7.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Face Recognition&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;kết-luận&#34;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;MobileNet cho kết quả tốt ngang ngữa các state-of-the-art thắng giải nhất ở quá khứ, nhưng với mô hình có số lượng tham số nhỏ hơn và số phép tính toán ít hơn. Điều này đạt được là nhờ vào việc sử dụng Depthwise Separable Convolution.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi bài viết, có chỗ nào bạn chưa rõ hoặc mình viết bị sai, các bạn vui lòng để lại comment để mình sửa lại cho đúng.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contour</title>
      <link>/blog/2019-05-26-contours/</link>
      <pubDate>Fri, 24 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-26-contours/</guid>
      <description>

&lt;h1 id=&#34;contour-là-gì&#34;&gt;Contour là gì&lt;/h1&gt;

&lt;p&gt;Các bạn có thể hiểu contour là &amp;ldquo;tập các điểm-liên-tục tạo thành một đường cong (curve) (boundary), và không có khoảng hở trong đường cong đó, đặc điểm chung trong một contour là các các điểm có cùng /gần xấu xỉ một giá trị màu, hoặc cùng mật độ. Contour là một công cụ hữu ích được dùng để phân tích hình dạng đối tượng, phát hiện đối tượng và nhận dạng đối tượng&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Để tìm contour chính xác, chúng ta cần phải &lt;em&gt;nhị phân hóa bức ảnh&lt;/em&gt; (nhớ là ảnh nhị phân nha các bạn, không phải ảnh grayscale đâu). Các kỹ thuật nhị phân hóa ảnh ở xử lý ảnh cơ bản có thể liệt kê đến là đặt ngưỡng, hoặc candy edge detection. Chúng ta sẽ không bàn kỹ về các cách đặt ngưỡng ( mặc dù có khá nhiều cách đặt ngưỡng, và trong opencv cũng có implement một vài phương pháp, nhưng nó không phải là mục tiêu của bài này, nên mình không đề cập ở đây) hoặc edge detection ở bài viết này, mà chúng ta sẽ đi vào các tìm contours bằng các sử dụng opencv luôn.&lt;/p&gt;

&lt;p&gt;Trong opencv, việc tìm một contour là việc &lt;em&gt;tìm một đối tượng có màu trắng trên nền đen&lt;/em&gt;. Cho nên, các bạn hãy nhớ rằng hãy set đối tượng thành màu trắng và để nền là màu đen, đừng làm ngược lại nha.&lt;/p&gt;

&lt;p&gt;Một lưu ý nhỏ là tại thời điểm mình viết bài viết này, mình sử dụng phiên bản opencv3.6. Các bạn có thể sử dụng phiên bản opencv mới hơn, nhưng có thể những sample code mình để bên dưới sẽ không work, do không tương thích.&lt;/p&gt;

&lt;h1 id=&#34;sử-dụng-contour-trong-opencv&#34;&gt;Sử dụng contour trong opencv&lt;/h1&gt;

&lt;p&gt;Opencv hỗ trợ cho chúng ta hàm để tìm contour của một bức ảnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;modifiedImage, contours, hierarchy = cv2.findContours(binaryImage, typeofContour, methodofContour)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong đó:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;contours: Danh sách các contour có trong bức ảnh nhị phân. Mỗi một contour được lưu trữ dưới dạng vector các điểm&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;hierarchy: Danh sách các vector, chứa mối quan hệ giữa các contour.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;modifiedImage: Ảnh sau khi sử dụng contour, thường chúng ta không xài đối số này&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;binaryImage: Ảnh nhị phân gốc. Một chú ý quan trọng ở đây là sau khi sử dụng hàm findContours thì giá trị của binaryImage cũng thay đổi theo, nên khi sử dụng bạn có thể áp dụng binaryImage.copy() để không làm thay đổi giá trị của binaryImage&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;typeofContour: có các dạng sau: RETR_EXTERNAL, RETR_LIST, RETR_CCOMP, RETR_TREE, RETR_FLOODFILL.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;methodofContour: Có các phương thức sau: CHAIN_APPROX_NONE, CHAIN_APPROX_SIMPLE, CHAIN_APPROX_TC89_L1, CHAIN_APPROX_TC89_KCOS.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ví dụ về các sử dụng hàm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import cv2

im = cv2.imread(&#39;test.jpg&#39;) # đọc ảnh màu
imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)  # chuyển ảnh màu sang dạng grayscale
ret,thresh = cv2.threshold(imgray,127,255,0) # nhị phân hóa bức ảnh bằng cách đặt ngưỡng, với giá trị của ngưỡng là 127
im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) # tìm contour

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Opencv hỗ trợ chúng ta hàm để vẽ contor lên bức ảnh, giúp chúng ta nhìn rõ ràng hơn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv2.drawContours(image, contours, contourIndex, colorCode, thickness)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;imgage: ảnh, có thể là ảnh grayscale hoặc ảnh màu.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;contours: danh sách các contour, là vector, nếu bạn muốn vẽ một contour, thì bạn phải cho nó vào trong một list.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;contourIndex Vị trí của contor, thông thường chúng ta để -1&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;colorCode: Giá trị màu của contour chúng ta muốn vẽ, ở dạng BGR, nếu bạn muốn vẽ contour màu xanh lá cây thì set là (0,255,0).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;thickness : độ dày của đường contour cần vẽ, giá trị thickness càng lớn thì đường contor vẽ càng bự&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;ví-dụ-đếm-số-lượng-quả-bóng-bay-trong-hình&#34;&gt;Ví dụ: Đếm số lượng quả bóng bay trong hình&lt;/h1&gt;

&lt;p&gt;Giả sử chúng ta có bức ảnh
&lt;img src=&#34;/post_image/colorfull_ballon.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Bong bóng bay&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Chúng ta thực hiện tìm contour của ảnh trên bằng cách&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import cv2

im = cv2.imread(&#39;colorfull_ballon.jpg&#39;)
imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) # chuyển ảnh xám thành ảnh grayscale
thresh = cv2.Canny(imgray, 127, 255) # nhị phân hóa ảnh
_, contours, _ = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)

cv2.drawContours(im, contours, -1, (0, 255, 0), 2) # vẽ lại ảnh contour vào ảnh gốc

# show ảnh lên
cv2.imshow(&amp;quot;ballons&amp;quot;, im)
cv2.waitKey(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/vietnam_coins_set_contours.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Contour màu xanh là đường curve bao quanh dữ liệu được rút trích được&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu về dropout trong deep learning, machine learning</title>
      <link>/blog/2019-05-05-deep-learning-dropout/</link>
      <pubDate>Sun, 05 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-05-deep-learning-dropout/</guid>
      <description>

&lt;h1 id=&#34;1-dropout-là-gì-nó-có-ý-nghĩa-gì-trong-mạng-neural-network&#34;&gt;1. Dropout là gì, nó có ý nghĩa gì trong mạng neural network&lt;/h1&gt;

&lt;p&gt;Theo Wikipedia, thuật ngữ &amp;ldquo;dropout&amp;rdquo; đề cập đến việc bỏ qua các đơn vị (unit) (cả hai hidden unit và visible unit) trong mạng neural network.&lt;/p&gt;

&lt;p&gt;Hiểu đơn giản là, trong mạng neural network, kỹ thuật dropout là việc chúng ta sẽ bỏ qua một vài unit trong suốt quá trình train trong mô hình, những unit bị bỏ qua được lựa chọn ngẫu nhiên. Ở đây, chúng ta hiểu &amp;ldquo;bỏ qua - ignoring&amp;rdquo; là unit đó sẽ không tham gia và đóng góp vào quá trình huấn luyện (lan truyền tiến và lan truyền ngược).&lt;/p&gt;

&lt;p&gt;Về mặt kỹ thuật, tại mỗi giai đoạn huấn luyện, mỗi node có xác suất bị bỏ qua là 1-p và xác suất được chọn là p&lt;/p&gt;

&lt;h1 id=&#34;2-tạo-sao-chúng-ta-cần-dropout&#34;&gt;2. Tạo sao chúng ta cần dropout&lt;/h1&gt;

&lt;p&gt;Giả sử rằng bạn hiểu hoàn toàn những gì đã nói ở phần 1, câu hỏi đặt ra là tại sao chúng ta cần đến dropout, tại sao chúng ta cần phải loại bỏ một vài các unit nào đó trong mạng neural network?&lt;/p&gt;

&lt;p&gt;Câu trả lời cho câu hỏi này là &lt;strong&gt;để chống over-fitting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Khi chúng ta sử dụng full connected layer, các neural sẽ phụ thuộc &amp;ldquo;mạnh&amp;rdquo; lẫn nhau trong suốt quá trình huấn luyện, điều này làm giảm sức mạng cho mỗi neural và dẫn đến bị over-fitting tập train.&lt;/p&gt;

&lt;h1 id=&#34;3-dropout&#34;&gt;3. Dropout&lt;/h1&gt;

&lt;p&gt;Đọc đến đây, bạn đã có một khái niệm cơ bản về dropout và động lực - động cơ để chúng ta sử dụng nó. Nếu bạn chỉ muốn có cái nhìn tổng quan về dropout trong neural network, hai sections trên đã cung cấp đầy đủ thông tin cho bạn, bạn có thể dừng tại đây. Phần tiếp theo, chúng ta sẽ nói kỹ hơn về mặt kỹ thuật của dropout.&lt;/p&gt;

&lt;p&gt;Trước đây, trong machine learning, người ta thường sử dụng regularization để ngăng chặn over-fititng. Regularization làm giảm over-fitting bằng cách thêm yếu tố &amp;ldquo;phạt&amp;rdquo; vào hàm độ lỗi (loss function).  Bằng việc thêm vào điểm phạt này, mô hình được huấn luyện sẽ giúp các features weights giảm đi sự phụ thuộc lẫn nhau. Đối với những ai đã sử dụng Logistic Regression rồi thì sẽ không xa lạ với thuật ngữ phạt L1(Laplacian) và L2 (Gaussian).&lt;/p&gt;

&lt;p&gt;Dropout là một kỹ thuật khác, một cách tiếp cận khác để regularization  trong mạng neural netwoks.&lt;/p&gt;

&lt;p&gt;Kỹ thuật dropout được thực hiện như sau:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trong pha train&lt;/strong&gt;: với mỗi hidden layer, với mỗi trainning sample, với mỗi lần lặp, chọn ngẫu nhiên p phần trăm số node và bỏ qua nó (bỏ qua luôn hàm kích hoạt cho các node bị bỏ qua).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trong pha test&lt;/strong&gt;: Sử dụng toàn bộ activations, nhưng giảm chúng với tỷ lệ p (do chúng ta bị miss p% hàm activation trong quá trình train).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/drop_out.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Mô tả về kiến trúc mạng có và không có dropout&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;4-một-số-đặc-điểm-rút-ra-được-khi-huấn-luyện-nhiều-mô-hình-khác-nhau-sử-dụng-dropout&#34;&gt;4. Một số đặc điểm rút ra được khi huấn luyện nhiều mô hình khác nhau sử dụng dropout&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Dropout ép mạng neural phải tìm ra nhiều robust features hơn, với đặc điểm là chúng phải hữu ích hơn, tốt hơn, ngon hơn khi kết hợp với nhiều neuron khác.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dropout đòi hỏi phải gấp đôi quá trình huấn luyện để đạt được sự hội tụ. Tuy nhiên, thời gian huấn luyện cho mỗi epoch sẽ ít hơn.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Với H unit trong mô hình, mỗi unit đều có xác xuất bị bỏ qua hoặc được chọn, chúng ta sẽ có 2^H mô hình có thể có. Trong pha test, toàn bộ network được sử dụng và mỗi hàm activation được giảm đi với hệ số p.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Một số nghiên cứu chỉ ra rằng, khi sử dụng Dropout và Batch Normalization (BN) cùng nhau thì kết quả rất tệ, trong cả lý thuyết và thực nghiệm, ví dụ nghiên cứu ở papper &amp;ldquo;Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift&amp;rdquo;, nguồn &lt;a href=&#34;https://arxiv.org/abs/1801.05134&#34;&gt;https://arxiv.org/abs/1801.05134&lt;/a&gt;, nhóm tác giả giải thích về mặt lý thuyết rằng: &amp;ldquo;đối với một neural, Dropout sẽ thay đổi phương sai của nó khi chúng ta chuyển trạng thái từ trian sang test. Còn BN thì không, BN vẫn tích luỹ đầy đủ thông tin trong quá trình huấn luyện. Do Dropout làm thay đổi phương sai nên sẽ xảy ra hiện tượng không đồng nhất về phương sai, dẫn đến hành vi suy luận không chắc chắn dẫn đến suy luận bị sai nhiều. Đặc biệt là khi kết hợp dropout và BN thì khiến cho suy luận càng sai lầm trầm trọng. &amp;ldquo;. Cho nên, trong một số trường hợp/bài toán chúng ta có thể dùng Dropout, trong một số trường hợp/ bài toán, người ta sử dụng BN và không sử dụng dropout.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Người ta thường dùng hệ số dropout là 0.5. Lý giải cho việc này, bạn có thể đọc bài báo &lt;a href=&#34;http://papers.nips.cc/paper/4878-understanding-dropout.pdf&#34;&gt;http://papers.nips.cc/paper/4878-understanding-dropout.pdf&lt;/a&gt;. Nói nôm là việc sử dụng giảm 50% của dropout giúp kết quả đạt được là tốt nhất so với các phương pháp chuẩn hoá khác.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;5-thực-nghiệm-trong-keras&#34;&gt;5. Thực nghiệm trong keras&lt;/h1&gt;

&lt;p&gt;Những vấn đề nói ở trên chỉ là lý thuyết. Bây giờ chúng ta sẽ bắt tay vào làm thực tế. Để xem thử dropout hoạt động như thế nào, chúng ta sẽ xây dựng mô hình deep net sử dụng keras và sử dụng tập dữ liệu cifar-10. Mô hình chúng ta xây dựng có 3 hidden layer với kích thước lần lượt là 64, 128, 256 và 1 full connected layer có kích thước 512 và output layer có kích thước 10 (do mình có 10 lớp).&lt;/p&gt;

&lt;p&gt;Chúng ta sử dụng hàm kích hoạt là ReLU trên các hidden layer và sử dụng hàm sigmoid trên output layer. Sử dụng hàm lỗi categorical cross-entropy.&lt;/p&gt;

&lt;p&gt;Trong trường hợp mô hình có sử dụng dropout, chúng ta sẽ set dropout ở tất cả các layer và thay đổi tỷ lệ dropout nằm trong khoảng từ 0.0 đến 0.9 với bước nhảy là 0.1.&lt;/p&gt;

&lt;p&gt;Mô hình setup với số epochs là 20. Bắt đầu xem nào.&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ load một vài thư viện cần thiết&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import os

import keras

from keras.datasets import cifar10
from keras.models  import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

from pylab import rcParams
rcParams[&#39;figure.figsize&#39;] = 20, 20

from keras.datasets import cifar10

(X_train, y_train), (X_test, y_test) = cifar10.load_data()


print(&amp;quot;Training data:&amp;quot;)
print(&amp;quot;Number of examples: &amp;quot;, X_train.shape[0])
print(&amp;quot;Number of channels:&amp;quot;,X_train.shape[3]) 
print(&amp;quot;Image size:&amp;quot;,X_train.shape[1], X_train.shape[2], X_train.shape[3])

print(&amp;quot;Test data:&amp;quot;)
print(&amp;quot;Number of examples:&amp;quot;, X_test.shape[0])
print(&amp;quot;Number of channels:&amp;quot;, X_test.shape[3])
print(&amp;quot;Image size:&amp;quot;,X_test.shape[1], X_test.shape[2], X_test.shape[3])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Training data:
Number of examples:  50000
Number of channels: 3
Image size: 32 32 3
Test data:
Number of examples: 10000
Number of channels: 3
Image size: 32 32 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta có 50000 hình train, và 10000 hình test. Mỗi hình là một ảnh RGB có kích thước 33x32x3 pixel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/cifar-10-overview.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;dataset cifar 10&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ chuẩn hoá dữ liệu. Đây là 1 bước quan trọng trước khi huấn luyện mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print( &amp;quot;mean before normalization:&amp;quot;, np.mean(X_train)) 
print( &amp;quot;std before normalization:&amp;quot;, np.std(X_train))

mean=[0,0,0]
std=[0,0,0]
newX_train = np.ones(X_train.shape)
newX_test = np.ones(X_test.shape)
for i in range(3):
    mean[i] = np.mean(X_train[:,i,:,:])
    std[i] = np.std(X_train[:,i,:,:])
    
for i in range(3):
    newX_train[:,i,:,:] = X_train[:,i,:,:] - mean[i]
    newX_train[:,i,:,:] = newX_train[:,i,:,:] / std[i]
    newX_test[:,i,:,:] = X_test[:,i,:,:] - mean[i]
    newX_test[:,i,:,:] = newX_test[:,i,:,:] / std[i]
        
    
X_train = newX_train
X_test = newX_test

print(&amp;quot;mean after normalization:&amp;quot;, np.mean(X_train))
print(&amp;quot;std after normalization:&amp;quot;, np.std(X_train))


&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;mean before normalization: 120.70756512369792
std before normalization: 64.1500758911213
mean after normalization: 0.9062499999999979
std after normalization: 0.4227421643271468

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Full code đoạn huấn luyện&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

# In[3]:Specify Training Parameters

batchSize = 512                   #-- Training Batch Size
num_classes = 10                  #-- Number of classes in CIFAR-10 dataset
num_epochs = 100                   #-- Number of epochs for training   
learningRate= 0.001               #-- Learning rate for the network
lr_weight_decay = 0.95            #-- Learning weight decay. Reduce the learn rate by 0.95 after epoch


img_rows, img_cols = 32, 32       #-- input image dimensions

Y_train = np_utils.to_categorical(y_train, num_classes)
Y_test = np_utils.to_categorical(y_test, num_classes)



batchSize = 512                   #-- Training Batch Size
num_classes = 10                  #-- Number of classes in CIFAR-10 dataset
num_epochs = 100                   #-- Number of epochs for training   
learningRate= 0.001               #-- Learning rate for the network
lr_weight_decay = 0.95            #-- Learning weight decay. Reduce the learn rate by 0.95 after epoch


img_rows, img_cols = 32, 32       #-- input image dimensions

Y_train = np_utils.to_categorical(y_train, num_classes)
Y_test = np_utils.to_categorical(y_test, num_classes)


# In[4]:VGGnet-10


from keras.layers import Conv2D
import copy
result = {}
y = {}
loss = []
acc = []
dropouts = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
for dropout in dropouts:
    print(&amp;quot;Dropout: &amp;quot;, (dropout))
    model = Sequential()                                               

    #-- layer 1
    model.add(Conv2D(64, (3, 3),                                    
                            border_mode=&#39;valid&#39;,
                            input_shape=( img_rows, img_cols,3))) 
    model.add(Dropout(dropout))  
    model.add(Conv2D(64, (3, 3)))
    model.add(Dropout(dropout))
    model.add(Activation(&#39;relu&#39;))                                       
    model.add(MaxPooling2D(pool_size=(2, 2)))

    ##--layer 2                        
    model.add(Conv2D(128, (3, 3)))
    model.add(Dropout(dropout)) 
    model.add(Activation(&#39;relu&#39;))                                       
    model.add(MaxPooling2D(pool_size=(2, 2)))

    ##--layer 3                         
    model.add(Conv2D(256, (3, 3)))
    model.add(Dropout(dropout)) 
    model.add(Activation(&#39;relu&#39;))                                       
    model.add(MaxPooling2D(pool_size=(2, 2)))

    ##-- layer 4
    model.add(Flatten())                                                
    model.add(Dense(512))                                               
    model.add(Activation(&#39;relu&#39;))                                                                           

    #-- layer 5
    model.add(Dense(num_classes))                                       

    #-- loss
    model.add(Activation(&#39;softmax&#39;))
    
    sgd = SGD(lr=learningRate, decay = lr_weight_decay)
    model.compile(loss=&#39;categorical_crossentropy&#39;,
                  optimizer=&#39;sgd&#39;,
                  metrics=[&#39;accuracy&#39;])
    
    model_cce = model.fit(X_train, Y_train, batch_size=batchSize, epochs=20, verbose=1, shuffle=True, validation_data=(X_test, Y_test))
    score = model.evaluate(X_test, Y_test, verbose=0)
    y[dropout] = model.predict(X_test)
    print(&#39;Test score:&#39;, score[0])
    print(&#39;Test accuracy:&#39;, score[1])
    result[dropout] = copy.deepcopy(model_cce.history)   
    loss.append(score[0])
    acc.append(score[1])



# In[5]: plot dropout 
import numpy as np                                                               
import matplotlib.pyplot as plt

width = 0.1

plt.bar(dropouts, acc, width, align=&#39;center&#39;)

plt.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=35)
plt.tick_params(axis=&#39;both&#39;, which=&#39;minor&#39;, labelsize=35)

plt.ylabel(&#39;Accuracy&#39;,size = 30)
plt.xlabel(&#39;Dropout&#39;, size = 30)
plt.show()


# In[6]: plot non drop out

import numpy as np                                                               
import matplotlib.pyplot as plt

width = 0.1

plt.bar(dropouts, loss, width, align=&#39;center&#39;,color = &#39;green&#39;)

plt.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=35)
plt.tick_params(axis=&#39;both&#39;, which=&#39;minor&#39;, labelsize=35)

plt.ylabel(&#39;Loss&#39;,size = 30)
plt.xlabel(&#39;Dropout&#39;, size = 30)
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/drop_out_result.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Kết quả&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nhìn hình kết quả ở trên, chúng ta có một số kết luận nhỏ như sau:&lt;/p&gt;

&lt;p&gt;Giá trị dropout tốt nhất là 0.2, khoảng dropout cho giá trị chấp nhận được là nằm trong đoạn từ 0 đến 0.5. Nếu dropout lớn hơn 0.5 thì kết quả hàm huấn luyện trả về khá tệ.&lt;/p&gt;

&lt;p&gt;Giá trị độ chính xác còn khá thấp =&amp;gt; 20 epochs là chưa đủ, cần huấn luyện nhiều hơn nữa.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Các kỹ thuật lấy mẫu</title>
      <link>/blog/2019-05-04-sampling-method/</link>
      <pubDate>Sat, 04 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-04-sampling-method/</guid>
      <description>

&lt;p&gt;Lấy mẫu dữ liệu là một kỹ thuật rất quang trọng trong thống kê, là yếu tố quan trọng góp phần xác định độ chính xác của research/ survey. Nếu có bất kỳ sai sót gì trong quá trình lấy mẫu, nó sẽ ảnh hưởng trực tiếp đến kết quả cuối cùng. Có rất nhiều kỹ thuật giúp chúng ta thu thập mẫu dựa trên nhu cầu và tình huống chúng ta cần. Bài viết này sẽ giải thích một số kỹ thuật phổ biến nhất.&lt;/p&gt;

&lt;p&gt;Để bắt đầu bài viết, chúng ta sẽ làm rõ mốt số khái niệm cơ bản là &lt;strong&gt;Quần thể - Population&lt;/strong&gt;,&lt;strong&gt;mẫu - Sample&lt;/strong&gt; và &lt;strong&gt;lấy mẫu - sampling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Quần thể - population là tập hợp của các cá thể có một hoặc một số đặc điểm chung. Kích thước của một quần thể là số lượng cá thể trong quần thể đó.&lt;/p&gt;

&lt;p&gt;Mẫu - sample là một tập con của quần thể. Quá trình chọn một mẫu được gọi là lấy mẫu -sampling. Kích thước mẫu là số lượng cá thể trong tập mẫu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/target-population.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 1: Ví dụ về lấy mẫu dữ liệu&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Có rất nhiều kỹ thuật lấy mẫu dữ liệu khác nhau, nhưng chúng ta có thể gom chúng vào 2 nhóm chính:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Lấy mẫu ngẫu nhiên - Probability Sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lấy mẫu phi ngẫu nhiên - non-probability sampling&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/probability-vs-non-probability-sampling.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 2: Ví dụ so về lấy mẫu ngẫu nhiên và lấy mẫu phi ngẫu nhiên&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sự khác biệt của hai nhóm trên là phương pháp lấy mẫu có sử dụng &amp;ldquo;hàm ngẫu nhiên&amp;rdquo; hay không. Với việc sử dụng hàm ngẫu nhiên, mỗi cá thể đều có cơ hội được lựa chọn ngang nhau và đều có cơ hội là một cá thể trong tập mẫu.&lt;/p&gt;

&lt;h2 id=&#34;lấy-mẫu-ngẫu-nhiên&#34;&gt;Lấy mẫu ngẫu nhiên&lt;/h2&gt;

&lt;p&gt;Những thuật toán trong nhóm này sử dụng hàm &amp;ldquo;ngẫu nhiên&amp;rdquo; để đảm bảo rằng mọi phần tử đều có cơ hội lựa chọn ngang nhau. Một tên khác của phương pháp này là random sampling.&lt;/p&gt;

&lt;p&gt;Một số phương pháp thuộc nhóm này&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Simple Random Sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stratified sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Systematic sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cluster Sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multi stage Sampling&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;simple-random-sampling&#34;&gt;Simple Random Sampling&lt;/h4&gt;

&lt;p&gt;Mỗi cá thể đều có cơ hội lựa chọn ngang nhau vào tập mẫu. Phương pháp này được sử dụng khi chúng ta không có bất kỳ thông tin gì về tập population.&lt;/p&gt;

&lt;p&gt;Ví dụ: Chọn ngẫu nhiên 20 sinh viên trong lớp học 50 sinh viên. Mỗi sinh viên đều có cơ hội được chọn ngang nhau là &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;50&lt;/sub&gt;.&lt;/p&gt;

&lt;h4 id=&#34;stratified-sampling&#34;&gt;Stratified sampling&lt;/h4&gt;

&lt;p&gt;Kỹ thuật này phân chia mỗi cá thể trong quần thể thành từng nhóm nhỏ dựa trên sự tương đồng (similarity), nghĩa là các cá thể trong cùng 1 nhóm sẽ đồng nhất với nhau về một khía cạnh nào đó, và sẽ không giống với các nhóm khác về khía cạnh đó. Và chúng ta sẽ chọn ngẫu nhiên các các thể trong mỗi nhóm. Ở phương pháp này, chúng ta cần thông tin cho trước về tập quần thể để tạo các nhóm con.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/stratified_sampling.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 2: lấy mẫu Stratified sampling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ở ví dụ trên, chúng ta sẽ chia tập quần thể thành các nhóm con mặc áo đỏ, mặc áo xanh, mặc áo vàng (phải biết trước được trong quần thể thằng nào mặc áo màu gì). Sau đó sẽ lựa chọn ngẫu nhiên 2 các thể trong mỗi nhóm.&lt;/p&gt;

&lt;h4 id=&#34;cluster-sampling&#34;&gt;Cluster Sampling&lt;/h4&gt;

&lt;p&gt;Toàn bộ tập quần thể sẽ được chia thành từ cụm hoặc thành từng phần. Sau đó chúng ta sẽ chọn ngẫu nhiên từng cụm. Tất cả các cá thể trong cụm đó sẽ được sử dụng làm tập mẫu. Các cụm được định danh dựa trên các yếu tố xác định trước. Ví dụ ở trong hình ở trên, các cụm được định danh dựa vào màu sắc của áo mà người đó mặc. Điểm khác biệt ở phương pháp này so với phương pháp ở trên là phương pháp ở trên lựa chọn ngẫu nhiên một số các cá thể trong mỗi cụm. Còn phương pháp này sẽ lựa chọn ngẫu nhiên các cụm, và chọn hết tất cả các các thể trong cụm đó.&lt;/p&gt;

&lt;p&gt;Một số chiến lược để lựa chọn cụm:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Single Stage Cluster Sampling&lt;/strong&gt;: Các cụm được lựa chọn ngẫu nhiên&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-state-cluster-sampling.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 3: Single Stage Cluster Sampling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two Stage Cluster Sampling&lt;/strong&gt;: Ở phương pháp này, chúng ta sẽ lựa chọn ngẫu nhiên các cụm, sau đó, trong mỗi cụm, chúng ta sẽ lựa chọn ngẫu nhiên các cá thể trong mỗi cụm&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/two-stage-cluster-sampling.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 4: Two Stage Cluster Sampling&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;systematic-clustering&#34;&gt;Systematic Clustering&lt;/h4&gt;

&lt;p&gt;Ở phương pháp này, việc lựa chọn cá thể là có quy luật và không ngẫu nhiên, từ cá thể đầu tiên. Các cá thể của tập mẫu được chọn ra từ tập quần thể dựa vào một quy luật nào đó. Đầu tiên, tất cả các cá thể trong tập quần thể phải được xắp xếp có thứ tự. Sau đó chúng ta sẽ lựa chọn ngẫu nhiên cá thể đầu tiên (mỗi cá thể đều có xác suất ngang nhau ở đây), và sử dụng quy luật nào đó để rút ra các cá thể tiếp theo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/systematic-clustering.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 5: Systematic Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Như ví dụ ở trên, chúng ta xắp xếp các nhân vật áo vàng, xanh, đỏ ngẫu nhiên tuỳ ý theo sự lựa chọn của người ta. Quy luật là cứ 4 người sẽ lấy người cuối. Ấn nút ngẫu nhiên &amp;hellip; ta được số 3. Vậy là cá thể đầu tiên là nhân vật ở vị trí số 3, tiếp theo sẽ là nhân vật ở vị trí 7, 11, 15,19, 5, &amp;hellip;&lt;/p&gt;

&lt;h4 id=&#34;multi-stage-sampling&#34;&gt;Multi-Stage Sampling&lt;/h4&gt;

&lt;p&gt;Phương pháp này là sự kết hợp của một hoặc nhiều phương pháp được mô tả ở trên.&lt;/p&gt;

&lt;p&gt;Quần thể được chia thành nhiều cụm (cluster) và mỗi cụm được chia vào từng nhóm con (subgrop - strata) dựa trên sự tương đồng =&amp;gt; chúng ta được một tập các cụm con được gọi là stratum. Chúng ta sẽ lựa nhọn một hoặc một vài strata trong stratum. Quá trình này sẽ được lặp đi lặp lại đến khi không còn cụm nào có thể phân chia được nữa.&lt;/p&gt;

&lt;p&gt;Ví dụ, các quốc gia có thể được phân chia thành từng bang, thành phố, thành thị, nông thôn. Và tất cả các khu vực có cùng ký tự đầu có thể được gom lại thành với nhau tạo thành một strata.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multi-stage-sampling.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 6: Multi-Stage Sampling&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;lấy-mẫu-phi-ngẫu-nhiên&#34;&gt;Lấy mẫu phi ngẫu nhiên&lt;/h2&gt;

&lt;p&gt;Những kỹ thuật nằm trong nhóm này không sử dụng hàm ngẫu nhiên. Kỹ thuật này phụ thuộc vào khả năng hiểu biết của các nhà nghiên cứu (researcher) trên tập quần thể họ đang có để chọn lựa cá thể cho tập mẫu. Kết quả của việc lấy mẫu có thể bị lệch.&lt;/p&gt;

&lt;p&gt;Một số phương pháp thuộc nhóm này là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Convenience Sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Purposive Sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Quota Sampling&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Referral /Snowball Sampling&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;convenience-sampling&#34;&gt;Convenience Sampling&lt;/h4&gt;

&lt;p&gt;Các cá thể được chọn dựa trên tính khả dụng của dữ liệu. Phương pháp này được sử dụng khi tính khả dụng của dữ liệu là hiếm và tốn kém. Do vậy, chúng ta sẽ lựa chọn mẫu dựa trên sự tiện lợi.&lt;/p&gt;

&lt;p&gt;Ví dụ, Các nhà nghiên cứu thường hay sử dụng phương pháp này trong các giai đoạn đầu của các nghiên cứu khảo sát, vì nó dễ dàng, nhanh chóng và cho ra kết quả nhanh.&lt;/p&gt;

&lt;h4 id=&#34;purposive-sampling&#34;&gt;Purposive Sampling&lt;/h4&gt;

&lt;p&gt;Phương pháp lấy mẫu này dựa trên mục đích của nghiên cứu. Chỉ chọn ra những cá thể trong quần thể phù hợp nhất với mục đích nghiên cứu .&lt;/p&gt;

&lt;p&gt;Ví dụ: Nếu chúng ta muốn hiểu được &amp;ldquo;suy nghĩ của những người quan tâm đến bằng thạc sỹ&amp;rdquo; thì tiêu chí lựa chọn cá thể là những người say yes trong câu hỏi &amp;ldquo;bạn có hứng thú với bậc thạc sỹ trong lĩnh vực &amp;hellip; không?&amp;rdquo;. Những người say &amp;ldquo;No&amp;rdquo; sẽ bị loại khỏi tập mẫu của chúng ta.&lt;/p&gt;

&lt;h4 id=&#34;quota-sampling&#34;&gt;Quota Sampling&lt;/h4&gt;

&lt;p&gt;Phương pháp lấy mẫu này phụ thuộc vào một số tiêu chuẩn thiết lập từ trước. Tỷ lệ của các nhóm cá thể trong tập mẫu phải giống hết trong tập quần thể. Các cá thể được chọn cho đến khi chúng đạt đúng tỷ lệ của một loại dữ liệu.&lt;/p&gt;

&lt;p&gt;Ví dụ: Giả sử chúng ta biết rằng trên trái đất này có 6 tỷ người, và 45% trong số đó là nam giới và 55% là nữ giới. Vậy thì chúng ta sẽ lấy mẫu làm sao cho tập mẫu chúng ta cũng phản ánh số đó, nghĩa là trong tập mẫu có 1000 người thì 45% trong số 1000 người đó phải là nam và 55% trong số 1000 người đó là nữ.&lt;/p&gt;

&lt;h4 id=&#34;referral-snowball-sampling&#34;&gt;Referral /Snowball Sampling&lt;/h4&gt;

&lt;p&gt;Kỹ thuật này được sử dụng khi chúng ta không biết gì về tập quần thể hoặc tập quần thể hiếm. Lúc đó chúng ta sẽ tìm ra cá thể đầu tiên trong quần thể, rồi nhờ cá thể đầu tiên đó gợi ý các cá thể tiếp theo với điều kiện thoả nhu cẫu lấy mẫu của nghiên cứu. Cứ tiếp tục như vậy thì kích thước của tập mẫu sẽ tăng lên theo cấp nhân như kích thước quả quả cầu tuyết, nên kỹ thuật này còn có tên gọi khác là Snowball Sampling.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/snowball-sampling.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 7: Ví dụ về Snowball Sampling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ví dụ: Trong tình huống, ngữ cảnh là bạn muốn làm 1 bài khảo sát về những người bị nhiễm HIV, những người này thường có khuynh hướng không cởi mở ở mức độ công cộng và khó cho chúng ta tiếp cận để thu thập thông tin trực tiếp từ họ.&lt;/p&gt;

&lt;p&gt;Nhóm khảo sát sẽ tiến hành liên hệ 1 người nào đó mà họ biết hoặc người nào đó xung phong làm cầu nối với các người bị nhiễm và thu thập thông tin từ họ (những người bị nhiễn tin tưởng người được xung phong hơn nhóm khảo sát. Vì nhóm khảo sát là người lạ).&lt;/p&gt;

&lt;p&gt;Hi vọng sau bài viết này, các bạn có thêm nhiều ý tưởng hơn nữa về việc lấy mẫu và các cách để lấy mẫu trong ứng dụng thực tế.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch và một số hình ảnh được lấy từ nguồn &lt;a href=&#34;https://towardsdatascience.com/sampling-techniques-a4e34111d808&#34;&gt;https://towardsdatascience.com/sampling-techniques-a4e34111d808&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PredictionIO Phần 1 - Hướng dẫn cài đặt</title>
      <link>/blog/2019-05-04-setup-predictio/</link>
      <pubDate>Fri, 03 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-04-setup-predictio/</guid>
      <description>

&lt;h1 id=&#34;1-giới-thiệu-về-predictionio&#34;&gt;1. Giới thiệu về PredictionIO&lt;/h1&gt;

&lt;p&gt;PredictionIO là một &amp;ldquo;open source Machine Learning Server built on top of a state-of-the-art open source stack&amp;rdquo; giúp cho các developers và các data scientists tạo ra các engine dự đoán trong học máy. PredictionIO giúp chúng ta&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Xây dựng và triển khai các ứng dụng, dịch vụ một cách nhanh chóng bằng cách tuỳ chỉnh lại các template đã sẵn có.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Trả lời các câu truy vấn động trong thời gian thực.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;huấn luyện và so sánh/đánh giá nhiều mô hình khác nhau dễ dàng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hợp nhất hoá dữ liệu từ nhiều nền tảng khác nhau hoặc trong thời gian thực để thực hiện phân tích dự đoán.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hỗ trợ các thư viện máy học và xử lý dữ liệu như Spark MLLib và OpenNLP&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tự xây dựng, triển khai, customize một mô hình machine learning&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2-cơ-chế-hoạt-động-của-predictionio&#34;&gt;2. Cơ chế hoạt động của PredictionIO&lt;/h1&gt;

&lt;p&gt;PredictionIO bao gồm các thành phần sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PredictionIO platform: là nền tảng open source được apache xây dựng sẵn giúp chúng ta triển khai, xây dựng, đánh giá các mô hình máy học.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Event Server: là nơi giúp chúng ta chuẩn hoá các sự kiện từ nhiều nguồn khác nhau&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Template Gallery: là nơi chúng ta download các engine template máy học về. PredictionIO hỗ trợ cho chúng ta rất nhiều template mẫu khác nhau. Chúng ta sẽ lần lượt tìm hiểu và implement ở các bài viết tiếp theo.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;event-server&#34;&gt;Event Server&lt;/h3&gt;

&lt;p&gt;PredictionIO Event Server chịu trách nhiệu thu thập dữ liệu từ các ứng dụng của bạn. Bạn có thể nhìn kỹ hơn ở hình bên dưới, các ứng dụng web, mobile app &amp;hellip; khi người dùng tương tác sẽ phát sinh các sự kiện (Event Data), ví dụ sự kiện người dùng thêm 1 đơn hàng vào giỏ hàng, người dùng xem sản phẩn A, người dùng xem sản phẩm C sau khi xem sản phẩm A&amp;hellip; Event Server sẽ ghi nhận lại đống dữ liệu này, chuẩn hoá lại. PredictionIO engine sau đó sẽ xây dựng mô hình dự đoán dựa trên các dữ liệu chúng ta thu thập được. Sau khi bạn có được mô hình tối ưu, chúng ta sẽ deploy các predict webservice, lắng nghe các truy vấn từ các ứng dụng và trả về kết quả trong thời gian thực.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/predictionio-event.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 1: Event server trong predictionio&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Event Server sẽ thu thập dữ liệu của bạn trong thời gian thực hoặc theo chu kỳ. Sau đó, nó sẽ chuẩn hoá dữ liệu hỗn độn của bạn từ nhiều nguồn khác nhau thành một dạng chuẩn chung. Event Server chủ yếu phục vụ hai mục đính chính:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cung cấp dữ liệu cho các engine để huấn luyện và đánh giá&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cung cấp dữ liệu dạng chuẩn để data analysis&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cũng giống như một database server, Event Server có thể được sử dụng để phục vụ cho nhiều ứng dụng khác nhau. Dữ liệu được phân tách cho các ứng dụng bằng &amp;ldquo;app_name&amp;rdquo; duy nhất. Cái này sẽ nói lại lúc xây dựng ứng dụng ở bên dưới.&lt;/p&gt;

&lt;p&gt;Khi một Event Server được triển khai, bạn có thể gửi dữ liệu cho một &amp;lsquo;app_name&amp;rsquo; cụ thể nào đó, app-name được định danh bằng access key. Dữ liệu được gửi đến Event Server sử dụng &lt;strong&gt;EventAPI&lt;/strong&gt; sử dụng giao thức http (tham khảo thêm ở &lt;a href=&#34;https://predictionio.apache.org/datacollection/eventapi/&#34;&gt;https://predictionio.apache.org/datacollection/eventapi/&lt;/a&gt;) hoặc sử dụng các PredictionIO SDK. Tham khảo thêm các SDK ở &lt;a href=&#34;https://predictionio.apache.org/sdk/&#34;&gt;https://predictionio.apache.org/sdk/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trong một số trường hợp, bạn muốn engine đọc dữ liệu từ một datastore nào đó thay vì Event Server. Bạn có thể thực hiện thông qua hướng dẫn ở &lt;a href=&#34;https://predictionio.apache.org/start/customize/&#34;&gt;https://predictionio.apache.org/start/customize/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;engine&#34;&gt;Engine&lt;/h3&gt;

&lt;p&gt;Engine là nơi chịu trách nhiệu đưa ra các quyết định. Nó gồm một hoặc nhiều thuật toán học máy học khác nhau. Các Engine sẽ huấn luyện dữ liệu và xây dựng các mô hình dự đoán. Sau đó sẽ phát triển thành các webservice. Các webservice sẽ nhận các truy vấn từ ứng dụng, dự đoán và trả về kết quả cho ứng dụng.&lt;/p&gt;

&lt;p&gt;PredictionIO&amp;rsquo;s  cung cấp cho chúng ta rất nhiều các template khác nhau đáp ứng gần như là đẩy đủ các mô hình máy học mà chúng ta cần. Bạn có thể dễ dàng tạo một mô hình máy học từ các template. Các thành phần của một template dược đặt tên là &lt;strong&gt;Data Source, Data Preparator, Algorithm(s), Serving&lt;/strong&gt;, các bạn có thể dễ dàng customize lại tuỳ thuộc nhu cầu của bạn.&lt;/p&gt;

&lt;h1 id=&#34;3-cài-đặt-predictionio-trên-môi-trường-ubuntu&#34;&gt;3. Cài đặt PredictionIO trên môi trường Ubuntu&lt;/h1&gt;

&lt;p&gt;Trong thời đại docker, các bạn có thể cài đặt PredictionIO dựa vào các docker được xây dựng sẵn đầy rẫy trên mạng, chúng giúp bạn đỡ tốn công sức hơn. Tuy nhiên, trong bài viết này, mình sẽ cài đặt từng thành phần PredictiIO trên ubuntu, không sử dụng docker.&lt;/p&gt;

&lt;h3 id=&#34;download-và-build-apache-prediction-io&#34;&gt;Download và build Apache Prediction IO&lt;/h3&gt;

&lt;p&gt;Chúng ta sẽ download Prediction IO từ trang github chính chủ. Phiên bản hiện tại là 0.14.0. Các bạn có thể lưu dữ liệu ở đâu tuỳ ý các bạn. Mình lưu ở thư mục &lt;strong&gt;/data/pio&lt;/strong&gt;. Và trong suốt bài viết này, mình sẽ lưu các thứ liên quan trong thư mục /data/pio. Các bạn có cài đặt theo hướng dẫn của mình thì nhớ sửa lại cho đúng đường dẫn của các bạn. Chúng ta sẽ clone nguồn từ trang github predictionio. và sẽ switch qua branch release. Đây là branch chính thành phẩm, các branch khác đang trong giai đoạn phát triển nên có thể build không được. Lúc các bạn làm có thể nó đã phát triển lên bản 15, 16 hoặc 1.0 gì đó rồi. Các bạn cứ tự tin sử dụng phiên bản mới nhất.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;git clone https://github.com/apache/predictionio.git
git checkout release/0.14.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;biên-dịch-prediction-io&#34;&gt;Biên dịch Prediction IO&lt;/h3&gt;

&lt;p&gt;Sau khi tải về bộ nguồn của Prediction IO, chúng ta sẽ tiền hành biên dịch. Quá trình biên dịch sẽ xảy ra khá lâu, các bạn kiên nhẫn chờ đợi&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cd predictionio
./make-distribution.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết thúc quá trình biên dịch, các bạn sẽ thấy dòng chữ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PredictionIO binary distribution created at PredictionIO-0.14.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vậy là chúng ta đã thành công. Việc tiếp theo là giải nén file PredictionIO-0.14.0.tar.gz để sử dụng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tar xvzf PredictionIO-0.14.0.tar.gz -C /data/pio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nhắc lại 1 lần nữa là do thời điểm hiện tại mình viết bài viết này, PredictionIO mới release bản 0.14.0 nên file tập tin sẽ là PredictionIO-0.14.0.tar.gz. Các bạn nhớ giải nén đúng với tên file ứng với phiên bản PredictionIO tương ứng nhé.&lt;/p&gt;

&lt;h3 id=&#34;download-và-giải-nén-các-dependencies&#34;&gt;Download và giải nén các Dependencies&lt;/h3&gt;

&lt;p&gt;Mình sẽ sử dụng Spark, ElasticSearch, Hbase và zookeeper, nên mình download hết về. Mình có thói quen sử dụng phiên bản mới nhất. Nên mình lên trang chủ và lấy link download mới nhất của chúng thôi. Tất cả các Dependencies mình dùng đều được bỏ vào trong thư mục vendors&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
cd PredictionIO-0.14.0
mkdir vendors
cd vendors
wget https://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz

wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.9.tar.gz

wget https://www.apache.org/dyn/closer.lua/hbase/2.1.4/hbase-2.1.4-bin.tar.gz

wget https://www-us.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz

tar xvzf spark-2.4.2-bin-hadoop2.7.tgz

tar xvzf elasticsearch-5.6.9.tar.gz

tar xvzf hbase-2.1.4-bin.tar.gz

tar xvzf zookeeper-3.4.14/zookeeper-3.4.14.tar.gz

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cấu-hình-chương-trình&#34;&gt;Cấu hình chương trình&lt;/h3&gt;

&lt;h5 id=&#34;cấu-hình-dependency&#34;&gt;Cấu hình dependency&lt;/h5&gt;

&lt;p&gt;Chúng ta sẽ cấu hình một chút để PredictionIO nhận ra các dependency của mình và cấu hình các dependency&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ chỉnh sửa file &lt;strong&gt;hbase-site.xml&lt;/strong&gt; của HBase&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nano /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/conf/hbase-site.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thay đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;configuration&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bằng đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:///data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.zookeeper.property.dataDir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/data/pio/PredictionIO-0.14.0/vendors/zookeeper-3.4.14&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ add đường dẫn java cho hbase&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nano /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/conf/hbase-env.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thêm đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;các bạn hãy thay đường dẫn java tương ứng với đường dẫn trong máy bạn. Nếu chưa có java thì các bạn hãy cài vào, nếu các bạn đã cài java mà không biết nó nằm ở đâu, các bạn có thể gọi lệnh bên dưới để xem đường dẫn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;update-alternatives --config java
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để chắc chắn rằng trong máy của bạn có cài java bạn hãy gọi lện &lt;strong&gt;java -version&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ví dụ trong máy mình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$java -version
openjdk version &amp;quot;1.8.0_191&amp;quot;
OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Các bạn cố gắng sử dụng phiên bản java mới nhất. Nó sẽ tương thích tốt hơn với phiên bản mới nhất của HBase, hoặc đọc phiên bản java đề nghị trong trang chủ HBase. Tránh trường hợp sử dụng phiên bản java quá cũ HBase không hỗ trợ.&lt;/p&gt;

&lt;h5 id=&#34;cấu-hình-prediction-io&#34;&gt;Cấu hình Prediction IO&lt;/h5&gt;

&lt;p&gt;Chỉnh sửa file &lt;strong&gt;pio-env.sh&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
nano /data/pio/PredictionIO-0.14.0/conf/pio-env.sh

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mặc định PredictionIO sử dụng PosgresSQl làm event server. Mình không dùng nó mà thay thế bằng HBASE và ELASTICSEARCH.&lt;/p&gt;

&lt;p&gt;Một số thay đổi mình sẽ liệt kê bên dưới&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;SPARK_HOME=$PIO_HOME/vendors/spark-2.3.2-bin-hadoop2.7

HBASE_CONF_DIR=$PIO_HOME/vendors/hbase-2.1.4/conf

PIO_STORAGE_REPOSITORIES_METADATA_NAME=pio_meta
PIO_STORAGE_REPOSITORIES_METADATA_SOURCE=ELASTICSEARCH

PIO_STORAGE_REPOSITORIES_EVENTDATA_NAME=pio_event
PIO_STORAGE_REPOSITORIES_EVENTDATA_SOURCE=HBASE

PIO_STORAGE_REPOSITORIES_MODELDATA_NAME=pio_model
PIO_STORAGE_REPOSITORIES_MODELDATA_SOURCE=LOCALFS

#Comment các dòng này lại, do không dùng postgres
# PIO_STORAGE_SOURCES_PGSQL_PASSWORD accordingly
# PIO_STORAGE_SOURCES_PGSQL_TYPE=jdbc
# PIO_STORAGE_SOURCES_PGSQL_URL=jdbc:postgresql://localhost/pio
# PIO_STORAGE_SOURCES_PGSQL_USERNAME=pio
# PIO_STORAGE_SOURCES_PGSQL_PASSWORD=pio

PIO_STORAGE_SOURCES_ELASTICSEARCH_HOME=$PIO_HOME/vendors/elasticsearch-5.6.9
PIO_STORAGE_SOURCES_HBASE_HOME=$PIO_HOME/vendors/hbase-2.1.4
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-khởi-chạy-hệ-thống&#34;&gt;4.Khởi chạy hệ thống&lt;/h1&gt;

&lt;p&gt;Chúng ta sẽ add path của PredictIO vào biến môi trường để sử dụng cho các lần sau&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
nano ~/.bashrc
erport PATH=/data/pio/PredictionIO-0.14.0/bin:$PATH

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hoặc có thể add path trong mỗi session&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PATH=$PATH:/data/pio/PredictionIO-0.14.0/bin; export PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ cấp quyền cho thư mục PredictionIO&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sudo chmod -R 775 /data/pio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nếu không cấp quyền write cho thư mục thì PredictionIO không thể write log file được.&lt;/p&gt;

&lt;p&gt;Chạy PredictionIO Server bằng cách gọi câu lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio-start-all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Stopping PredictionIO Event Server...
Stopping HBase...
stopping hbase.............
Stopping Elasticsearch...
tgdd@U1604:/data/pio/PredictionIO-0.14.0/bin$ pio-start-all
Starting Elasticsearch...
Starting HBase...
running master, logging to /data/pio/PredictionIO-0.14.0/vendors/hbase-2.1.4/bin/../logs/hbase-tgdd-master-U1604.out
Waiting 10 seconds for Storage Repositories to fully initialize...
Starting PredictionIO Event Server...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để kiểm tra hệ thống khi start có lỗi lầm gì không, chúng ta sử dụng lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[INFO] [Management$] Inspecting PredictionIO...
[INFO] [Management$] PredictionIO 0.14.0 is installed at /data/pio/PredictionIO-0.14.0
[INFO] [Management$] Inspecting Apache Spark...
[INFO] [Management$] Apache Spark is installed at /data/spark-2.3.2-bin-hadoop2.7
[INFO] [Management$] Apache Spark 2.3.2 detected (meets minimum requirement of 2.0.2)
[INFO] [Management$] Inspecting storage backend connections...
[INFO] [Storage$] Verifying Meta Data Backend (Source: ELASTICSEARCH)...
[INFO] [Storage$] Verifying Model Data Backend (Source: LOCALFS)...
[INFO] [Storage$] Verifying Event Data Backend (Source: HBASE)...
[INFO] [Storage$] Test writing to Event Store (App Id 0)...
[INFO] [HBLEvents] The table pio_event:events_0 doesn&#39;t exist yet. Creating now...
[INFO] [HBLEvents] Removing table pio_event:events_0...
[INFO] [Management$] Your system is all ready to go.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bạn thấy dòng chữ &lt;strong&gt;[INFO] [Management$] Your system is all ready to go.&lt;/strong&gt; thì yên tâm, hệ thống đã chạy thành công.&lt;/p&gt;

&lt;p&gt;Để stop hệ thống, các bạn gọi lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio-stop-all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả khi stop&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Stopping PredictionIO Event Server...
Stopping HBase...
stopping hbase.............
Stopping Elasticsearch...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vậy là chúng ta đã tiến hành cài đặt thành công PredictionIO Server rồi. Hẹn gặp bạn ở bài thứ hai, cài đặt các template cho PredictionIO và tiến hành dự đoán.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PredictionIO Phần 2 - Cài đặt chương trình demo</title>
      <link>/blog/2019-05-07-predictio-mini-demo1/</link>
      <pubDate>Fri, 03 May 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-05-07-predictio-mini-demo1/</guid>
      <description>

&lt;h1 id=&#34;1-tạo-chương-trình-đầu-tiên-bằng-predictionio&#34;&gt;1. Tạo chương trình đầu tiên bằng PredictionIO&lt;/h1&gt;

&lt;p&gt;Đầu tiên, các bạn hãy tạo thư mục template ở đâu đó. Mình sẽ tạo ở trong thư mục /data/pio. Đường dẫn của mình sẽ là /data/pio/template&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mdkir /data/pio/template
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ clone templte trên github về, các bạn thực hiện lệnh sau&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;git clone https://github.com/apache/predictionio-template-recommender.git
cd predictionio-template-recommender
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ tạo một app đầu tiên, mình đặt tên là ourrecommendation, các bạn thích đặt tên gì thì đặt nha.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio app new ourrecommendation

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để liệt kê danh sách app đang có trong hệ thống, các bạn dùng lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio app list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả trong máy mình tại thời điểm viết bài là&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[INFO] [Pio$]                 Name |   ID |                                                       Access Key | Allowed Event(s)
[INFO] [Pio$]    ourrecommendation |    1 | Z93rJZ7Xq2pXiQwVC6B5nRK6jRykcfyMI5huOijKbdDJeUeKEnVT-ph5nabptIX1 | (all)
[INFO] [Pio$] Finished listing 1 app(s).

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mình mới tạo app đầu tiên tên là ourrecommendation nên chỉ có 1 app trong hệ thống. Sau này sẽ có nhiều hơn. À, sau khi tạo app, thì hệ thống sẽ generate tự động cho app với một Access Key, ví dụ access key của app ourrecommendateion của mình là Z93rJZ7Xq2pXiQwVC6B5nRK6jRykcfyMI5huOijKbdDJeUeKEnVT-ph5nabptIX1. Các bạn sẽ có access key khác với access key của mình, nên đừng copy của mình về làm gì hết :).&lt;/p&gt;

&lt;p&gt;Sau khi khởi tạo app xong, chúng ta sẽ import data vào hệ thống. Ở đây, mình sẽ download dữ liệu mẫu từ nguồn &lt;a href=&#34;https://gist.githubusercontent.com/vaghawan/0a5fb8ddb85e03631dd500d7c8f0677d/raw/17487437dd8269588d9dd1ac859b129a43842ba5/data-sample.json&#34;&gt;https://gist.githubusercontent.com/vaghawan/0a5fb8ddb85e03631dd500d7c8f0677d/raw/17487437dd8269588d9dd1ac859b129a43842ba5/data-sample.json&lt;/a&gt;. Sau khi download về các bạn import dữ liệu vào hệ thống bằng lệnh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio import — appid 1 — input data-sample.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với appod 1 là id của ourrecommendation chúng ta vừa mới tạo. Nếu quên appid, các bạn có thể xem lại bằng lệnh &lt;strong&gt;pio app list&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Sau khi import thành công, chúng ta sẽ thay đổi giá trị của trường appname trong file engine.json thành tên của app mình, là ourrecommendation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nano engine.json

{
  &amp;quot;id&amp;quot;: &amp;quot;default&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;Default settings&amp;quot;,
  &amp;quot;engineFactory&amp;quot;: &amp;quot;org.example.recommendation.RecommendationEngine&amp;quot;,
  &amp;quot;datasource&amp;quot;: {
    &amp;quot;params&amp;quot; : {
      &amp;quot;appName&amp;quot;: &amp;quot;ourrecommendation&amp;quot;
    }
  },
  &amp;quot;algorithms&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;als&amp;quot;,
      &amp;quot;params&amp;quot;: {
        &amp;quot;rank&amp;quot;: 10,
        &amp;quot;numIterations&amp;quot;: 20,
        &amp;quot;lambda&amp;quot;: 0.01,
        &amp;quot;seed&amp;quot;: 3
      }
    }
  ]
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một lưu ý quang trọng là giá trị &amp;ldquo;org.example.recommendation.RecommendationEngine&amp;rdquo; trong &amp;ldquo;engineFactory&amp;rdquo; là của hệ thống. Và bạn đừng sửa, thay đổi chúng. Nói chung là ngoài giá trị của &amp;ldquo;appName&amp;rdquo; ra, bạn không nên thay đổi bất kỳ thức gì khác trong file  engine.json.&lt;/p&gt;

&lt;p&gt;Sau khi import file thành công. Chúng ta sẽ build app. Lệnh build có tác dụng kiểm tra lại hệ thống đã được cấu hình đúng và đủ chưa.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio build

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nếu build thành công, chúng ta sẽ thấy dòng chữ này.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
[INFO] [Engine$] Build finished successfully.
[INFO] [Pio$] Your engine is ready for training.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi build thành công, chúng ta sẽ tiến hành huấn luyện mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pio build

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và chờ đợi dòng này xuất hiện&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
[INFO] [CoreWorkflow$] Training completed successfully.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân nhóm các thuật toán học máy</title>
      <link>/blog/2019-04-19-deep-learning-view/</link>
      <pubDate>Fri, 19 Apr 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-19-deep-learning-view/</guid>
      <description>

&lt;p&gt;Ở bài trước mình đã trình bày định nghĩa và một số ứng dụng của Máy học (Machine Learning – ML), phân biệt ML với Trí tuệ nhân tạo (Artificial Intelligence – AI) cũng như mối quan hệ giữa AI, ML và Big Data. Từ bài viết này trở đi mình sẽ tập trung viết về ML, các thuật toán, cách sử dụng công cụ kèm theo một vài demo nhỏ giúp bạn đọc dễ hình dung và áp dụng. Để mở đầu cho chuỗi bài viết sắp tới, hôm nay mình sẽ trình bày cách phân nhóm các thuật toán ML.&lt;/p&gt;

&lt;p&gt;Với đa số mọi người, trước khi bắt đầu giải quyết một vấn đề nào đó, việc đầu tiên là chúng ta sẽ tìm hiểu xem liệu có ai đã gặp vấn đề này hoặc vấn đề tương tự như vậy hay không và cách họ giải quyết thế nào. Sau khi nắm được thông tin khái quát, công việc kế tiếp là chọn lựa và điều chỉnh giải pháp sao cho phù hợp với vấn đề của bản thân. Trong trường hợp vấn đề còn quá mới mẻ thì chúng ta mới phải bắt tay làm từ đầu, điều này hầu như rất hiếm, đặc biệt là trong thời đại công nghệ này, khi mà chỉ bằng một cú nhấp chuột, hàng ngàn thông tin, tư liệu về đề tài chúng ta quan tâm sẽ xuất hiện. Cũng giống như thế, ML hiện đã được nghiên cứu rộng khắp, rất nhiều công trình khoa học, thuật toán được cho ra đời. Với người mới bắt đầu mà nói thì chúng ta chưa cần phải làm gì cả ngoài việc nắm được các thuật toán cơ bản, đặc điểm của chúng để khi đối diện với một bài toán cụ thể trong thực tế chúng ta có thể biết được mình nên lựa chọn thuật toán nào cho phù hợp đã là điều rất tốt rồi.&lt;/p&gt;

&lt;p&gt;Mặc dù có rất nhiều thuật toán học nhưng dựa vào phương thức học (learning style) hoặc sự tương đồng (similarity) về hình thức hay chức năng mà chúng có thể được gom thành từng nhóm. Sau đây mình sẽ trình bày tổng quan cả hai cách phân nhóm thuật toán học này.&lt;/p&gt;

&lt;h1 id=&#34;1-phân-nhóm-dựa-trên-phương-thức-học&#34;&gt;1.    Phân nhóm dựa trên phương thức học&lt;/h1&gt;

&lt;p&gt;Xét theo phương thức học, các thuật toán ML được chia làm bốn nhóm, bao gồm “Học có giám sát” (Supervised Learning), “Học không giám sát” (Unsupervised Learning), “Học bán giám sát” (hay học kết hợp - Semi-supervised Learning) và “Học tăng cường” (Reinforcement Learning).&lt;/p&gt;

&lt;h2 id=&#34;a-học-có-giám-sát&#34;&gt;a.   Học có giám sát&lt;/h2&gt;

&lt;p&gt;Học có giám sát hay còn gọi là học có thầy là thuật toán dự đoán nhãn (label)/đầu ra (output) của một dữ liệu mới dựa trên tập dữ liệu huấn luyện mà trong đó mỗi mẫu dữ liệu đều đã được gán nhãn như minh hoạ ở Hình 1. Khi đó, thông qua một quá trình huấn luyện, một mô hình sẽ được xây dựng để cho ra các dự đoán và khi các dự đoán bị sai thì mô hình này sẽ được tinh chỉnh lại. Việc huấn luyện sẽ tiếp tục cho đến khi mô hình đạt được mức độ chính xác mong muốn trên dữ liệu huấn luyện. Điều này cũng giống như khi chúng ta đi học trên lớp, ta biết câu trả lời chính xác từ giáo viên (tập dữ liệu có nhãn) và từ đó ta sẽ sửa chữa nếu làm sai. Học có giám sát là nhóm phổ biến nhất trong các thuật toán ML.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/supervised-learning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 1: Supervised Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Một cách toán học, học có giám sát là khi chúng ra có một tập hợp biến đầu vào &lt;code&gt;$ X={x_1,x_2,…,x_N} $&lt;/code&gt; và một tập hợp nhãn tương ứng &lt;code&gt;$ Y={y_1,y_2,…,y_N} $&lt;/code&gt;, trong đó &lt;code&gt;$ x_i$&lt;/code&gt;, &lt;code&gt;$y_i $&lt;/code&gt; là các vector. Các cặp dữ liệu biết trước &lt;code&gt;$( x_i, y_i ) \in X \times Y $&lt;/code&gt; được gọi là tập dữ liệu huấn luyện (training data). Từ tập dữ liệu huấn luyện này, chúng ta cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập X sang một phần tử (xấp xỉ) tương ứng của tập Y:&lt;/p&gt;

&lt;p&gt;$$ y_i \approx f(x_i), \forall i=1, 2, …, N $$&lt;/p&gt;

&lt;p&gt;Mục đích là xấp xỉ hàm số &lt;code&gt;$f$&lt;/code&gt; thật tốt để khi có một dữ liệu x mới, chúng ta có thể tính được nhãn tương ứng của nó &lt;code&gt;$y=f(x)$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Ví dụ: Trong nhận dạng chữ số viết tay, ta có ảnh của hàng nghìn trường hợp ứng với mỗi chữ số được viết bởi nhiều người khác nhau. Ta đưa các bức ảnh này vào một thuật toán học và chỉ cho nó biết “mỗi bức ảnh tương ứng với chữ số nào”. Sau khi thuật toán tạo ra một mô hình, tức là một hàm số nhận đầu vào là một bức ảnh và cho ra kết quả là một chữ số. Khi nhận được một bức ảnh mới mà mô hình “chưa từng gặp qua” và nó sẽ dự đoán xem bức ảnh đó tương ứng với chữ số nào.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mnist-900x506.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 2: Ảnh minh hoạ cho tập dữ liệu chữ số viết tay - MNIST&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Đối với những ai sử dụng mạng xã hội Facebook thì khá quen thuộc với tính năng phát hiện khuôn mặt trong một bức ảnh, bản chất của thuật toán dò tìm các khuôn mặt này là một thuật toán học có giám sát với tập huấn luyện là vô số ảnh đã được gán nhãn là mặt người hay không phải mặt người.&lt;/p&gt;

&lt;p&gt;Các thuật toán học có giám sát còn được phân ra thành hai loại chính là phân lớp (Classification) và hồi quy (Regression).&lt;/p&gt;

&lt;h3 id=&#34;phân-lớp&#34;&gt;Phân lớp&lt;/h3&gt;

&lt;p&gt;Một bài toán được gọi là phân lớp nếu các nhãn của dữ liệu đầu vào được chia thành một số hữu hạn lớp (miền giá trị là rời rạc). Chẳng hạn như tính năng xác định xem một email có phải là spam hay không của Gmail; xác định xem hình ảnh của con vật là chó hay mèo. Hoặc ví dụ nhận dạng ký số viết tay ở trên cũng thuộc bài toán phân lớp, bao gồm mười lớp ứng với các số từ 0 đến 9. Tương tự cho ví dụ nhận dạng khuôn mặt với hai lớp là phải và không phải khuôn mặt, …&lt;/p&gt;

&lt;h3 id=&#34;hồi-quy&#34;&gt;Hồi quy&lt;/h3&gt;

&lt;p&gt;Một bài toán được xem là hồi quy nếu nhãn không được chia thành các nhóm mà là một giá trị thực cụ thể (miền giá trị là liên tục). Hầu hết các bài toán dự báo (giá cổ phiếu, giá nhà, …) thường được xếp vào bài toán hồi quy. Ví như, nếu một căn nhà rộng 150 m^2, có 7 phòng và cách trung tâm thành phố 10 km sẽ có giá là bao nhiêu? Lúc này kết quả dự đoán sẽ là một số thực.&lt;/p&gt;

&lt;p&gt;Nếu như phát hiện khuôn mặt là bài toán phân lớp thì dự đoán tuổi là bài toán hồi quy. Tuy nhiên dự đoán tuổi cũng có thể coi là phân lớp nếu ta cho tuổi là một số nguyên dương N và khi đó ta sẽ có N lớp khác nhau tính từ 1.
Một số thuật toán nổi tiếng thuộc về nhóm học có giám sát như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Phân lớp: k-Nearest Neighbors, mạng nơron nhân tạo, SVM, …&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hồi quy: Linear Regression, Logistic Regression, …&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;b-học-không-giám-sát&#34;&gt;b. Học không giám sát&lt;/h2&gt;

&lt;p&gt;Trái với Supervised learning, học không giám sát hay học không thầy là thuật toán dự đoán nhãn của một dữ liệu mới dựa trên tập dữ liệu huấn luyện mà trong đó tất cả các mẫu dữ liệu đều chưa được gán nhãn hay nói cách khác là ta không biết câu trả lời chính xác cho mỗi dữ liệu đầu vào như minh hoạ ở Hình 3. Điều này cũng giống như khi ta học mà không có thầy cô, sẽ không ai cho ta biết đáp án đúng là gì.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/unsupervisedlearning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 3: Unsupervised Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Khi đó, mục tiêu của thuật toán unsupervised learning không phải là tìm đầu ra chính xác mà sẽ hướng tới việc tìm ra cấu trúc hoặc sự liên hệ trong dữ liệu để thực hiện một công việc nào đó, ví như gom cụm (clustering) hoặc giảm số chiều của dữ liệu (dimension reduction) để thuận tiện trong việc lưu trữ và tính toán.&lt;/p&gt;

&lt;p&gt;Các bài toán Unsupervised learning tiếp tục được chia nhỏ thành hai loại là phân cụm (Clustering) và luật kết hợp (Association Rule).&lt;/p&gt;

&lt;h3 id=&#34;phân-cụm&#34;&gt;Phân cụm&lt;/h3&gt;

&lt;p&gt;Một bài toán phân cụm / phân nhóm toàn bộ dữ liệu X thành các nhóm/cụm nhỏ dựa trên sự liên quan giữa các dữ liệu trong mỗi nhóm. Chẳng hạn như phân nhóm khách hàng dựa vào độ tuổi, giới tính. Điều này cũng giống như việc ta đưa cho một đứa trẻ rất nhiều mảnh ghép với các hình dạng và màu sắc khác nhau, có thể là tam giác, vuông, tròn với màu xanh, đỏ, tím, vàng, sau đó yêu cầu trẻ phân chúng thành từng nhóm. Mặc dù ta không dạy trẻ mảnh nào tương ứng với hình nào hoặc màu nào, nhưng nhiều khả năng trẻ vẫn có thể phân loại các mảnh ghép theo màu sắc hoặc hình dạng.&lt;/p&gt;

&lt;h3 id=&#34;luật-kết-hợp&#34;&gt;Luật kết hợp&lt;/h3&gt;

&lt;p&gt;Là bài toán mà khi chúng ta muốn khám phá ra một quy luật dựa trên nhiều dữ liệu cho trước. Ví như những khách hàng mua mặt hàng này sẽ mua thêm mặt hàng kia; hoặc khan giả xem phim này sẽ có xu hướng thích xem phim kia, dựa vào đó ta có thể xây dựng những hệ thống gợi ý khách hàng (Recommendation System) nhằm thúc đẩy nhu cầu mua sắm hoặc xem phim&amp;hellip;.&lt;/p&gt;

&lt;p&gt;Một số thuật toán thuộc nhóm học không giám sát như Apriori (Association Rule), k-Means (Clustering), …&lt;/p&gt;

&lt;h2 id=&#34;c-học-bán-giám-sát&#34;&gt;c.   Học bán giám sát&lt;/h2&gt;

&lt;p&gt;Là bài toán mà khi tập dữ liệu đầu vào X là hỗn hợp các mẫu có nhãn và không có nhãn, trong đó số lượng có nhãn chỉ chiếm một phần nhỏ như minh hoạ ở Hình 4.&lt;/p&gt;

&lt;p&gt;Phần lớn các bài toán thực tế của ML thuộc nhóm này vì việc thu thập dữ liệu có nhãn tốn rất nhiều thời gian và có chi phí cao. Rất nhiều loại dữ liệu thậm chí cần phải có chuyên gia mới gán nhãn được, chẳng hạn như ảnh y học hoặc các cặp câu song ngữ. Ngược lại, dữ liệu chưa có nhãn có thể được thu thập với chi phí thấp từ internet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/semi-supervisedlearning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 4: Semi-supervised Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Với bài toán này, mô hình phải tìm hiểu các cấu trúc để tổ chức dữ liệu cũng như đưa ra dự đoán. Vì đặc điểm trung gian nên ta có thể sử dụng unsupervised learning để khám phá và tìm hiểu cấu trúc trong dữ liệu đầu vào, đồng thời sử dụng supervised learning để dự đoán cho dữ liệu không được gán nhãn. Sau đó đưa dữ liệu vừa dự đoán trở lại làm dữ liệu huấn luyện cho supervised learning và sử dụng mô hình sau khi huấn luyện để đưa ra dự đoán về dữ liệu mới.&lt;/p&gt;

&lt;p&gt;Một số thuật toán học tăng cường như: Self Training, Generative models, S3VMs, Graph-Based Algorithms, Multiview Algorithms, …&lt;/p&gt;

&lt;h2 id=&#34;d-học-tăng-cường&#34;&gt;d.   Học tăng cường&lt;/h2&gt;

&lt;p&gt;Học tăng tường hay học củng cố là bài toán giúp cho một hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất. Hiện tại, reinforcement learning chủ yếu được áp dụng vào Lý Thuyết Trò Chơi (Game Theory), các thuật toán cần xác định nước đi tiếp theo để đạt được điểm số cao nhất. Hình 5 là một ví dụ đơn giản sử dụng học tăng cường.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/reinforcementlearning.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 5: Minh hoạ cho học tăng cường được áp dụng trong lý thuyết trò chơi.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AlphaGo - một phần mềm chơi cờ vây trên máy tính được xây dựng bởi Google DeepMind hay chương trình dạy máy tính chơi game Mario là những ứng dụng sử dụng học tăng cường.&lt;/p&gt;

&lt;p&gt;Cờ vậy được xem là trò chơi có độ phức tạp cực kỳ cao với tổng số nước đi là xấp xỉ 1076110761, so với cờ vua là 1012010120, vì vậy thuật toán phải chọn ra một nước đi tối ưu trong số hàng tỉ tỉ lựa chọn. Về cơ bản, AlphaGo bao gồm các thuật toán thuộc cả Supervised learning và Reinforcement learning. Trong phần Supervised learning, dữ liệu từ các ván cờ do con người chơi với nhau được đưa vào để huấn luyện. Tuy nhiên, mục tiêu cuối cùng của AlphaGo không phải là chơi như con người mà phải thắng được con người. Vì vậy, sau khi học xong các ván cờ của con người, AlphaGo tự chơi với chính nó thông qua hàng triệu ván cờ để tìm ra các nước đi mới tối ưu hơn. Thuật toán trong phần tự chơi này được xếp vào loại Reinforcement learning.&lt;/p&gt;

&lt;p&gt;Đơn giản hơn cờ vây, tại một thời điểm cụ thể, người chơi game Mario chỉ cần bấm một số lượng nhỏ các nút (di chuyển, nhảy, bắn đạn) hoặc không cần bấm nút nào ứng với một chướng ngại vật cố định ở một vị trí cố định. Khi đó thuật toán trong ứng dụng dạy máy tính chơi game Mario sẽ nhận đầu vào là sơ đồ của màn hình tại thời điểm hiện hành, nhiệm vụ của thuật toán là tìm ra tổ hợp phím nên được bấm ứng với đầu vào đó. Việc huấn luyện này được dựa trên điểm số cho việc di chuyển được bao xa với thời gian bao lâu trong game, càng xa và càng nhanh thì điểm thưởng đạt được càng cao, tất nhiên điểm thưởng này không phải là điểm của trò chơi mà là điểm do chính người lập trình tạo ra. Thông qua huấn luyện, thuật toán sẽ tìm ra một cách tối ưu để tối đa số điểm trên, qua đó đạt được mục đích cuối cùng là cứu công chúa.&lt;/p&gt;

&lt;p&gt;Có nhiều cách khác nhau để thuật toán có thể mô hình hóa một vấn đề dựa trên sự tương tác của nó với dữ liệu đầu vào. Phân loại hoặc cách tổ chức thuật toán học máy này rất hữu ích vì nó buộc chúng ta phải suy nghĩ về vai trò của dữ liệu đầu vào và quy trình chuẩn bị mô hình và chọn một thuật toán phù hợp nhất cho vấn đề của chúng ta để có kết quả tốt nhất.&lt;/p&gt;

&lt;h1 id=&#34;2-phân-nhóm-dựa-trên-sự-tương-đồng&#34;&gt;2.    Phân nhóm dựa trên sự tương đồng&lt;/h1&gt;

&lt;p&gt;Dựa vào sự tương đồng về chức năng hay cách thức hoạt động mà các thuật toán sẽ được gom nhóm với nhau. Sau đây là danh sách các nhóm và các thuật toán theo từng nhóm.&lt;/p&gt;

&lt;h2 id=&#34;a-các-thuật-toán-hồi-quy-regression-algorithms&#34;&gt;a.   Các thuật toán hồi quy (Regression Algorithms)&lt;/h2&gt;

&lt;p&gt;Hồi quy là quá trình tìm mối quan hệ phụ thuộc của một biến (được gọi là biến phụ thuộc hay biến được giải thích, biến được dự báo, biến được hồi quy, biến phản ứng, biến nội sinh) vào một hoặc nhiều biến khác (được gọi là biến độc lập, biến giải thích, biến dự báo, biến hồi quy, biến tác nhân hay biến kiểm soát, biến ngoại sinh) nhằm mục đích ước lượng hoặc tiên đoán giá trị kỳ vọng của biến phụ thuộc khi biết trước giá trị của biến độc lập. Hình 6 tượng trưng cho ý tưởng của các thuật toán hồi quy.&lt;/p&gt;

&lt;p&gt;Ví dụ như, dự đoán rằng nếu tăng lãi suất tiền gửi thì sẽ huy động được lượng tiền gửi nhiều hơn, khi đó ngân hàng A cần biết mối quan hệ giữa lượng tiền gửi và lãi suất tiền gửi, cụ thể hơn họ muốn biết khi tăng lãi suất thêm 0.1% thì lượng tiền gửi sẽ tăng trung bình là bao nhiêu.&lt;/p&gt;

&lt;p&gt;Các thuật toán hồi quy phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Linear Regression&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Logistic Regression&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Locally Estimated Scatterplot Smoothing (LOESS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multivariate Adaptive Regression Splines (MARS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ordinary Least Squares Regression (OLSR)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stepwise Regression&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/regression-algorithn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 6: Regression Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;b-thuật-toán-dựa-trên-mẫu-instance-based-algorithms&#34;&gt;b.   Thuật toán dựa trên mẫu (Instance-based Algorithms)&lt;/h2&gt;

&lt;p&gt;Mô hình học tập dựa trên mẫu hay thực thể là bài toán ra quyết định dựa vào các trường hợp hoặc các mẫu dữ liệu huấn luyện được coi là quan trọng hay bắt buộc đối với mô hình.&lt;/p&gt;

&lt;p&gt;Nhóm thuật toán này thường xây dựng cơ sở dữ liệu về dữ liệu mẫu và so sánh dữ liệu mới với cơ sở dữ liệu bằng cách sử dụng thước đo tương tự để tìm kết quả phù hợp nhất và đưa ra dự đoán. Khi đó trọng tâm được đặt vào đại diện của các thể hiện được lưu trữ như minh hoạ ở Hình 7.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/instance-based-algorithms.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 7: Instance-based Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán dựa trên thực thể phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;k-Nearest Neighbor (kNN – k láng giềng gần nhất)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learning Vector Quantization (LVQ)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Locally Weighted Learning (LWL)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Self-Organizing Map (SOM)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;c-thuật-toán-chuẩn-hoá-regularization-algorithms&#34;&gt;c.   Thuật toán chuẩn hoá (Regularization Algorithms)&lt;/h2&gt;

&lt;p&gt;Các thuật toán chuẩn hoá ra đời từ sự mở rộng các phương pháp đã có (điển hình là các phương pháp hồi quy) bằng cách xử phạt các mô hình dựa trên mức độ phức tạp của chúng. Việc ưu tiên các mô hình đơn giản hơn cũng tốt hơn trong việc khái quát hóa. Hình 8 tượng trưng cho ý tưởng của thuật toán chuẩn hoá.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/regularization-algorithms.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 8: Regularization Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán chính quy phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Elastic Net&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Least Absolute Shrinkage and Selection Operator (LASSO)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Least-Angle Regression (LARS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ridge Regression&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;d-thuật-toán-cây-quyết-định-decision-tree-algorithms&#34;&gt;d.   Thuật toán cây quyết định (Decision Tree Algorithms)&lt;/h2&gt;

&lt;p&gt;Đây là phương pháp xây dựng mô hình ra quyết định dựa trên các giá trị thực của những thuộc tính trong dữ liệu. Sự quyết định được rẽ nhánh trong cấu trúc cây cho đến khi quyết định dự đoán được đưa ra cho một mẫu nhất định như minh hoạ ở Hình 9. Phương pháp này được sử dụng trong việc huấn luyện dữ liệu cho bài toán phân lớp và hồi quy. Vì sự nhanh chóng, chính xác nên phương pháp này rất được ưa chuộng trong ML.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/decision-tree-algorithm.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 9: Decision Tree Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán cây quyết định phổ biến nhất bao gồm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Chi-squared Automatic Interaction Detection (CHAID)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Classification và Regression Tree – CART&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Conditional Decision Trees&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;C4.5 và C5.0&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Decision Stump&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Iterative Dichotomiser 3 (ID3)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;M5&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;e-thuật-toán-bayes-bayesian-algorithms&#34;&gt;e.   Thuật toán Bayes (Bayesian Algorithms)&lt;/h2&gt;

&lt;p&gt;Đây là nhóm các thuật toán áp dụng Định lý Bayes cho bài toán phân loại và hồi quy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/bayessian-algorith.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 10: Bayesian Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Averaged One-Dependence Estimators (AODE)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bayesian Belief Network (BBN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bayesian Network (BN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gaussian Naive Bayes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multinomial Naive Bayes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Naive Bayes&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;f-thuật-toán-phân-cụm-clustering-algorithms&#34;&gt;f.   Thuật toán phân cụm (Clustering Algorithms)&lt;/h2&gt;

&lt;p&gt;Tất cả các phương pháp đều sử dụng các cấu trúc vốn có trong dữ liệu để tổ chức tốt nhất dữ liệu thành các nhóm có mức độ phổ biến tối đa dựa vào trọng tâm (centroid) và thứ bậc (hierarchal) như thể hiện ở Hình 11.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/clustering-algorithm.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 11: Clustering Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán phân cụm phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Expectation Maximisation (EM – cực đại hoá kỳ vọng)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hierarchical Clustering&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;k-Means&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;k-Medians&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;g-các-thuật-toán-luật-kết-hợp-association-rule-learning-algorithms&#34;&gt;g.   Các thuật toán luật kết hợp (Association Rule Learning Algorithms)&lt;/h2&gt;

&lt;p&gt;Đây là những thuật toán sẽ rút trích ra các quy tắc giải thích tốt nhất mối quan hệ giữa các biến trong dữ liệu. Các quy tắc này có thể giúp khám phá ra các tính chất quan trọng và hữu ích trong các tập dữ liệu lớn và cao chiều trong thương mại cùng các lĩnh vực khác. Hình 12 minh hoạ cho ý tưởng của thuật toán luật kết hợp.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/association-rule.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 12: Association Rule Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán luật kết hợp phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Apriori algorithm&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Eclat algorithm&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;FP-Growth algorithm&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;h-thuật-toán-mạng-nơron-nhân-tạo-artificial-neural-network-algorithms&#34;&gt;h.   Thuật toán mạng nơron nhân tạo (Artificial Neural Network Algorithms)&lt;/h2&gt;

&lt;p&gt;Mạng nơron nhân tạo là các mô hình được lấy cảm hứng từ cấu trúc và chức năng của mạng lưới thần kinh sinh học. Hình 13 minh hoạ cho một mạng truyền thẳng.
Nhóm thuật toán này có thể được sử dụng cho bài toán phân lớp và hồi quy với rất nhiều biến thể khác nhau cho hầu hết các vấn đề. Tuy nhiên, trong bài viết này mình chỉ trình bày các thuật toán cổ điển và phổ biến nhất:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Back-Propagation (mạng lan truyền ngược)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Perceptron (Mạng lan truyền thẳng)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multi-layer perceptron (Mạng truyền thẳng đa lớp)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hopfield Network&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Radial Basis Function Network (RBFN)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/neural-network-alg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 13: Artificial Neural Network Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;i-thuật-toán-học-sâu-deep-learning-algorithms&#34;&gt;i.   Thuật toán học sâu (Deep Learning Algorithms)&lt;/h2&gt;

&lt;p&gt;Thực chất Deep Learning là một bản cập nhật hiện đại cho Artificial Neural Networks nhằm khai thác khả năng tính toán của máy tính, tuy nhiên vì sự phát triển lớn mạnh của chúng nên mình tách ra thành một nhóm riêng.&lt;/p&gt;

&lt;p&gt;Deep Learning quan tâm đến việc xây dựng các mạng thần kinh lớn hơn, phức tạp hơn nhiều, và làm sao để khai thác hiệu quả các bộ dữ liệu lớn chứa rất ít dữ liệu đã được gán nhãn. Hình 14 minh hoạ cho ý tưởng của Deep learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/deep-learning-alg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 14: Deep Learning Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán học sâu phổ biến nhất là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Convolutional Neural Network (CNN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep Belief Networks (DBN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep Boltzmann Machine (DBM)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stacked Auto-Encoders&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;j-nhóm-thuật-toán-giảm-chiều-dữ-liệu-dimensionality-reduction-algorithms&#34;&gt;j.   Nhóm thuật toán Giảm chiều dữ liệu (Dimensionality Reduction Algorithms)&lt;/h2&gt;

&lt;p&gt;Giống như các phương pháp phân cụm, giảm không gian tìm kiếm và khai thác cấu trúc vốn có trong dữ liệu nhưng theo cách không giám sát hoặc để tóm tắt hay mô tả dữ liệu sử dụng ít thông tin hơn là mục tiêu của nhóm phương pháp này. Hình 15 minh hoạ cho việc giảm chiều dữ liệu.&lt;/p&gt;

&lt;p&gt;Điều này có thể hữu ích để trực quan hóa dữ liệu hoặc đơn giản hóa dữ liệu mà sau đó có thể được sử dụng trong phương pháp học có giám sát. Nhiều trong số các phương pháp này có thể được điều chỉnh để sử dụng trong phân lớp và hồi quy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/demension-reducion.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 15: Dimensional Reduction Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Các thuật toán Giảm chiều dữ liệu phổ biến như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Flexible Discriminant Analysis (FDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Linear Discriminant Analysis (LDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mixture Discriminant Analysis (MDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multidimensional Scaling (MDS)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Partial Least Squares Regression (PLSR)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Principal Component Analysis (PCA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Principal Component Regression (PCR)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Projection Pursuit&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Quadratic Discriminant Analysis (QDA)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sammon Mapping&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;k-thuật-toán-tập-hợp-ensemble-algorithms&#34;&gt;k.   Thuật toán tập hợp (Ensemble Algorithms)&lt;/h2&gt;

&lt;p&gt;Ensemble methods là những phương pháp kết hợp các mô hình yếu hơn được huấn luyện độc lập và phần dự đoán của chúng sẽ được kết hợp theo một cách nào đó để đưa ra dự đoán tổng thể như minh họa ở Hình 16.&lt;/p&gt;

&lt;p&gt;Nhóm thuật toán này khá mạnh và được nghiên cứu nhiều, đặc biệt là về cách để kết hợp các mô hình với nhau.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/ensemble-alg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 16: Ensemble Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Một số thuật toán phổ biến như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AdaBoost&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Boosting&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bootstrapped Aggregation (Bagging)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient Boosting Machines (GBM)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient Boosted Regression Trees (GBRT)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Random Forest&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Stacked Generalization (blending)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;l-các-thuật-toán-khác&#34;&gt;l.   Các thuật toán khác&lt;/h2&gt;

&lt;p&gt;Còn rất nhiều các thuật toán khác không được liệt kê ở đây, chẳng hạn như Support Vector Machines (SVM), mình đang phân vân rằng liệu thuật toán này nên được đưa vào nhóm nào đó hay đứng một mình. Nếu dựa vào danh sách các biến thể và mức độ phát triển thì SVM có thể được tách thành một nhóm riêng – nhóm thuật toán sử dụng véctơ hỗ trợ.&lt;/p&gt;

&lt;p&gt;Thêm vào đó, các thuật toán được hình thành từ các nhiệm vụ đặc biệt, hoăc các thuật toán từ những nhánh con đặc biệt của ML cũng không được liệt kê vào các nhóm, chẳng hạn như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Feature selection algorithms&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Algorithm accuracy evaluation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Performance measures&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Có dịp mình sẽ bổ sung hoặc đề cập đến những thuật toán này ở một bài viết khác.&lt;/p&gt;

&lt;p&gt;Mặc dù rất hữu ích (dựa vào nhóm, người dùng sẽ dễ dàng nhớ được bản chất của thuật toán) nhưng phương pháp phân nhóm này chưa hoàn hảo ở điểm có những thuật toán có thể phù hợp với nhiều danh mục như Learning Vector Quantization, vừa là phương pháp lấy cảm hứng từ mạng thần kinh (neural network), vừa là phương pháp dựa trên cá thể (instance-based). Hoặc là thuật toán có cùng tên mô tả bài toán và nhóm thuật toán như Hồi quy (Regression) và Phân cụm (Clustering). Đối với những trường hợp này ta có thể giải quyết bằng cách liệt kê các thuật toán hai lần hoặc bằng cách chọn nhóm một cách chủ quan. Để tránh trùng lặp các thuật toán và giữ cho mọi thứ đơn giản thì có lẽ chọn nhóm theo cách chủ quan sẽ phù hợp hơn.&lt;/p&gt;

&lt;p&gt;Để giúp các bạn dễ nhớ cũng như tổng kết cho phần này mình đã vẽ một sơ đồ các thuật toán phân theo nhóm và sắp xếp theo alphabet, các bạn có thể xem thểm ở Hình 17 bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/machine-learning-branch.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hình 17: Sơ đồ phân nhóm thuật toán theo sự tương đồng&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hy vọng bài viết này sẽ mang lại hữu ích cho bạn đọc, nhất là giúp bạn có dược cái nhìn tổng quan về những gì hiện có và một số ý tưởng về cách liên kết các thuật toán với nhau.&lt;/p&gt;

&lt;p&gt;Danh sách các nhóm và thuật toán được liệt kê trong bài viết chỉ đảm bảo được yếu tố phổ biến tuy nhiên sẽ không đầy đủ. Vậy nên nếu bạn biết thêm thuật toán hoặc nhóm nào chưa được liệt kê ở đây hoặc kể cả cách phân nhóm thuật toán khác, cũng như sau khi đọc mà các bạn có bất kỳ góp ý, câu hỏi giúp cải thiện bài viết tốt hơn, các bạn có thể để lại bình luận nhằm chia sẻ cùng mình và những bạn đọc khác nhé.&lt;/p&gt;

&lt;p&gt;Tài liệu tham khảo:
A Tour of Machine Learning Algorithms by Jason Brownlee  in Understand Machine Learning Algorithms&lt;/p&gt;

&lt;p&gt;Semi-Supervised Learning Tutorial by Xiaojin Zhu&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms&#34;&gt;https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Top 10 algorithms in data mining by Xindong Wu · Vipin Kumar · J. Ross Quinlan · Joydeep Ghosh · Qiang Yang · Hiroshi Motoda · Geoffrey J. McLachlan · Angus Ng · Bing Liu · Philip S. Yu · Zhi-Hua Zhou · Michael Steinbach · David J. Hand · Dan Steinberg.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân loại hoa sử dụng pretrain model</title>
      <link>/blog/2019-04-15-phan-loai-hoa/</link>
      <pubDate>Mon, 15 Apr 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-15-phan-loai-hoa/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Ở trong bài viết này, chúng ta sẽ sử dụng tập dữ liệu là tập dữ liệu ở ở link &lt;a href=&#34;https://www.kaggle.com/alxmamaev/flowers-recognition&#34;&gt;https://www.kaggle.com/alxmamaev/flowers-recognition&lt;/a&gt;. Tập dữ liệu này bao gồm 4242 hình cảnh của 5 loại  hoa hồng (rose), hoa mặt trời (sunflower), hoa bồ công anh (dandelion), hoa cúc (daisy) và hoa tulip. Nhóm tác giả đã thu thập dữ liệu dựa trên các trang web  flicr, google images, yandex. Tập hình ảnh được chia thành 5 lớp, mỗi lớp có khoảng 800 hình, có kích thước xấp xỉ 320x320 pixel. Các hình ảnh có kích thước không đồng nhất với nhau.&lt;/p&gt;

&lt;h2 id=&#34;thực-hiện&#34;&gt;Thực hiện&lt;/h2&gt;

&lt;p&gt;Dữ liệu sau khi giản nén có dạng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir/classname1/*.*
data_dir/classname2/*.*
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cấu trúc lưu trũ như này đúng với mô hình của mình nên chúng ta cần nên chúng ta không thay đổi gì về câu trúc nữa, tiến hành viết code&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ load dataset lên và tranform nó để đưa vào huấn luyện.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import os
from collections import defaultdict
import numpy as np
import scipy.misc


def preprocess_input(x0):
    x = x0 / 255.
    x -= 0.5
    x *= 2.
    return x


def reverse_preprocess_input(x0):
    x = x0 / 2.0
    x += 0.5
    x *= 255.
    return x


def dataset(base_dir, n):
    print(&amp;quot;base dir: &amp;quot;+base_dir)
    print(&amp;quot;n: &amp;quot;+str(n))
    n = int(n)
    d = defaultdict(list)
    for root, subdirs, files in os.walk(base_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            assert file_path.startswith(base_dir)
            
            suffix = file_path[len(base_dir):]
            
            suffix = suffix.lstrip(&amp;quot;/&amp;quot;)
            suffix = suffix.lstrip(&amp;quot;\\&amp;quot;)
            if(suffix.find(&#39;/&#39;)&amp;gt;-1): #linux
                label = suffix.split(&amp;quot;/&amp;quot;)[0]
            else: #window
                label = suffix.split(&amp;quot;\\&amp;quot;)[0]
            d[label].append(file_path)
    print(&amp;quot;walk directory complete&amp;quot;)
    tags = sorted(d.keys())

    processed_image_count = 0
    useful_image_count = 0

    X = []
    y = []

    for class_index, class_name in enumerate(tags):
        filenames = d[class_name]
        for filename in filenames:
            processed_image_count += 1
            if processed_image_count%100 ==0:
                print(class_name+&amp;quot;\tprocess: &amp;quot;+str(processed_image_count)+&amp;quot;\t&amp;quot;+str(len(d[class_name])))
            img = scipy.misc.imread(filename)
            height, width, chan = img.shape
            assert chan == 3
            aspect_ratio = float(max((height, width))) / min((height, width))
            if aspect_ratio &amp;gt; 2:
                continue
            # We pick the largest center square.
            centery = height // 2
            centerx = width // 2
            radius = min((centerx, centery))
            img = img[centery-radius:centery+radius, centerx-radius:centerx+radius]
            img = scipy.misc.imresize(img, size=(n, n), interp=&#39;bilinear&#39;)
            X.append(img)
            y.append(class_index)
            useful_image_count += 1
    print(&amp;quot;processed %d, used %d&amp;quot; % (processed_image_count, useful_image_count))

    X = np.array(X).astype(np.float32)
    #X = X.transpose((0, 3, 1, 2))
    X = preprocess_input(X)
    y = np.array(y)

    perm = np.random.permutation(len(y))
    X = X[perm]
    y = y[perm]

    print(&amp;quot;classes:&amp;quot;,end=&amp;quot; &amp;quot;)
    for class_index, class_name in enumerate(tags):
        print(class_name, sum(y==class_index),end=&amp;quot; &amp;quot;)
    print(&amp;quot;X shape: &amp;quot;,X.shape)

    return X, y, tags
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đoạn code trên khá đơn giản và dễ hiểu. Lưu ý ở đây là với những bức ảnh có tỷ lệ width và height &amp;gt; 2 thì mình sẽ loại chúng ra khỏi tập dữ liệu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ xây dựng mô hình dựa trên mô hình Resnet50 có sẵn của kares, do sử dụng pretrain model, nên n-1 lớp trước đó sẽ không được huấn luyện và chúng ta sẽ sử dụng dụng các weight có sẵn đã được huấn luyện trên tập ImageNet rút đặc trưng cho mô hình. Chúng ta chỉ cần thêm một lớp full connected và softmax để phân lớp các loại hoa, công việc của chúng ta hiện tại là tìm ra trọng số của lớp full connected cuối cùng (thay vì huấn luyện lại hết toàn bộ mô hình).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# create the base pre-trained model
def build_model(nb_classes):
    base_model = ResNet50(weights=&#39;imagenet&#39;, include_top=False)

    # add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    # let&#39;s add a fully-connected layer
    x = Dense(1024, activation=&#39;relu&#39;)(x)
    # and a logistic layer
    predictions = Dense(nb_classes, activation=&#39;softmax&#39;)(x)

    # this is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)

    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional ResNet50 layers
    for layer in base_model.layers:
        layer.trainable = False

    return model
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visualize một chút xíu về kiến trúc inceptionV3 mình đang dùng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, None, None, 6 9472        conv1_pad[0][0]
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, None, None, 6 0           bn_conv1[0][0]
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, None, None, 6 0           activation_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           pool1_pad[0][0]
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, None, None, 6 4160        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, None, None, 6 0           bn2a_branch2a[0][0]
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, None, None, 6 36928       activation_2[0][0]
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, None, None, 6 0           bn2a_branch2b[0][0]
__________________________________________________________________________________________________
res2a_branch2c (Conv2D)         (None, None, None, 2 16640       activation_3[0][0]
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, None, None, 2 16640       max_pooling2d_1[0][0]
__________________________________________________________________________________________________
bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]
                                                                 bn2a_branch1[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, None, None, 2 0           add_1[0][0]
__________________________________________________________________________________________________
res2b_branch2a (Conv2D)         (None, None, None, 6 16448       activation_4[0][0]
__________________________________________________________________________________________________
bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, None, None, 6 0           bn2b_branch2a[0][0]
__________________________________________________________________________________________________
res2b_branch2b (Conv2D)         (None, None, None, 6 36928       activation_5[0][0]
__________________________________________________________________________________________________
bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, None, None, 6 0           bn2b_branch2b[0][0]
__________________________________________________________________________________________________
res2b_branch2c (Conv2D)         (None, None, None, 2 16640       activation_6[0][0]
__________________________________________________________________________________________________
bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]
                                                                 activation_4[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, None, None, 2 0           add_2[0][0]
__________________________________________________________________________________________________
res2c_branch2a (Conv2D)         (None, None, None, 6 16448       activation_7[0][0]
__________________________________________________________________________________________________
bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, None, None, 6 0           bn2c_branch2a[0][0]
__________________________________________________________________________________________________
res2c_branch2b (Conv2D)         (None, None, None, 6 36928       activation_8[0][0]
__________________________________________________________________________________________________
bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, None, None, 6 0           bn2c_branch2b[0][0]
__________________________________________________________________________________________________
res2c_branch2c (Conv2D)         (None, None, None, 2 16640       activation_9[0][0]
__________________________________________________________________________________________________
bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]
                                                                 activation_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, None, None, 2 0           add_3[0][0]
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, None, None, 1 32896       activation_10[0][0]
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, None, None, 1 0           bn3a_branch2a[0][0]
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, None, None, 1 147584      activation_11[0][0]
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, None, None, 1 0           bn3a_branch2b[0][0]
__________________________________________________________________________________________________
res3a_branch2c (Conv2D)         (None, None, None, 5 66048       activation_12[0][0]
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, None, None, 5 131584      activation_10[0][0]
__________________________________________________________________________________________________
bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]
                                                                 bn3a_branch1[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, None, None, 5 0           add_4[0][0]
__________________________________________________________________________________________________
res3b_branch2a (Conv2D)         (None, None, None, 1 65664       activation_13[0][0]
__________________________________________________________________________________________________
bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, None, None, 1 0           bn3b_branch2a[0][0]
__________________________________________________________________________________________________
res3b_branch2b (Conv2D)         (None, None, None, 1 147584      activation_14[0][0]
__________________________________________________________________________________________________
bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, None, None, 1 0           bn3b_branch2b[0][0]
__________________________________________________________________________________________________
res3b_branch2c (Conv2D)         (None, None, None, 5 66048       activation_15[0][0]
__________________________________________________________________________________________________
bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]
__________________________________________________________________________________________________
add_5 (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]
                                                                 activation_13[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, None, None, 5 0           add_5[0][0]
__________________________________________________________________________________________________
res3c_branch2a (Conv2D)         (None, None, None, 1 65664       activation_16[0][0]
__________________________________________________________________________________________________
bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, None, None, 1 0           bn3c_branch2a[0][0]
__________________________________________________________________________________________________
res3c_branch2b (Conv2D)         (None, None, None, 1 147584      activation_17[0][0]
__________________________________________________________________________________________________
bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, None, None, 1 0           bn3c_branch2b[0][0]
__________________________________________________________________________________________________
res3c_branch2c (Conv2D)         (None, None, None, 5 66048       activation_18[0][0]
__________________________________________________________________________________________________
bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]
__________________________________________________________________________________________________
add_6 (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]
                                                                 activation_16[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, None, None, 5 0           add_6[0][0]
__________________________________________________________________________________________________
res3d_branch2a (Conv2D)         (None, None, None, 1 65664       activation_19[0][0]
__________________________________________________________________________________________________
bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, None, None, 1 0           bn3d_branch2a[0][0]
__________________________________________________________________________________________________
res3d_branch2b (Conv2D)         (None, None, None, 1 147584      activation_20[0][0]
__________________________________________________________________________________________________
bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, None, None, 1 0           bn3d_branch2b[0][0]
__________________________________________________________________________________________________
res3d_branch2c (Conv2D)         (None, None, None, 5 66048       activation_21[0][0]
__________________________________________________________________________________________________
bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]
__________________________________________________________________________________________________
add_7 (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, None, None, 5 0           add_7[0][0]
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, None, None, 2 131328      activation_22[0][0]
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, None, None, 2 0           bn4a_branch2a[0][0]
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, None, None, 2 590080      activation_23[0][0]
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, None, None, 2 0           bn4a_branch2b[0][0]
__________________________________________________________________________________________________
res4a_branch2c (Conv2D)         (None, None, None, 1 263168      activation_24[0][0]
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, None, None, 1 525312      activation_22[0][0]
__________________________________________________________________________________________________
bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]
__________________________________________________________________________________________________
add_8 (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]
                                                                 bn4a_branch1[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, None, None, 1 0           add_8[0][0]
__________________________________________________________________________________________________
res4b_branch2a (Conv2D)         (None, None, None, 2 262400      activation_25[0][0]
__________________________________________________________________________________________________
bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, None, None, 2 0           bn4b_branch2a[0][0]
__________________________________________________________________________________________________
res4b_branch2b (Conv2D)         (None, None, None, 2 590080      activation_26[0][0]
__________________________________________________________________________________________________
bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, None, None, 2 0           bn4b_branch2b[0][0]
__________________________________________________________________________________________________
res4b_branch2c (Conv2D)         (None, None, None, 1 263168      activation_27[0][0]
__________________________________________________________________________________________________
bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]
__________________________________________________________________________________________________
add_9 (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]
                                                                 activation_25[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, None, None, 1 0           add_9[0][0]
__________________________________________________________________________________________________
res4c_branch2a (Conv2D)         (None, None, None, 2 262400      activation_28[0][0]
__________________________________________________________________________________________________
bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, None, None, 2 0           bn4c_branch2a[0][0]
__________________________________________________________________________________________________
res4c_branch2b (Conv2D)         (None, None, None, 2 590080      activation_29[0][0]
__________________________________________________________________________________________________
bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, None, None, 2 0           bn4c_branch2b[0][0]
__________________________________________________________________________________________________
res4c_branch2c (Conv2D)         (None, None, None, 1 263168      activation_30[0][0]
__________________________________________________________________________________________________
bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]
__________________________________________________________________________________________________
add_10 (Add)                    (None, None, None, 1 0           bn4c_branch2c[0][0]
                                                                 activation_28[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, None, None, 1 0           add_10[0][0]
__________________________________________________________________________________________________
res4d_branch2a (Conv2D)         (None, None, None, 2 262400      activation_31[0][0]
__________________________________________________________________________________________________
bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, None, None, 2 0           bn4d_branch2a[0][0]
__________________________________________________________________________________________________
res4d_branch2b (Conv2D)         (None, None, None, 2 590080      activation_32[0][0]
__________________________________________________________________________________________________
bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, None, None, 2 0           bn4d_branch2b[0][0]
__________________________________________________________________________________________________
res4d_branch2c (Conv2D)         (None, None, None, 1 263168      activation_33[0][0]
__________________________________________________________________________________________________
bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]
__________________________________________________________________________________________________
add_11 (Add)                    (None, None, None, 1 0           bn4d_branch2c[0][0]
                                                                 activation_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, None, None, 1 0           add_11[0][0]
__________________________________________________________________________________________________
res4e_branch2a (Conv2D)         (None, None, None, 2 262400      activation_34[0][0]
__________________________________________________________________________________________________
bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, None, None, 2 0           bn4e_branch2a[0][0]
__________________________________________________________________________________________________
res4e_branch2b (Conv2D)         (None, None, None, 2 590080      activation_35[0][0]
__________________________________________________________________________________________________
bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, None, None, 2 0           bn4e_branch2b[0][0]
__________________________________________________________________________________________________
res4e_branch2c (Conv2D)         (None, None, None, 1 263168      activation_36[0][0]
__________________________________________________________________________________________________
bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]
__________________________________________________________________________________________________
add_12 (Add)                    (None, None, None, 1 0           bn4e_branch2c[0][0]
                                                                 activation_34[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, None, None, 1 0           add_12[0][0]
__________________________________________________________________________________________________
res4f_branch2a (Conv2D)         (None, None, None, 2 262400      activation_37[0][0]
__________________________________________________________________________________________________
bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, None, None, 2 0           bn4f_branch2a[0][0]
__________________________________________________________________________________________________
res4f_branch2b (Conv2D)         (None, None, None, 2 590080      activation_38[0][0]
__________________________________________________________________________________________________
bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, None, None, 2 0           bn4f_branch2b[0][0]
__________________________________________________________________________________________________
res4f_branch2c (Conv2D)         (None, None, None, 1 263168      activation_39[0][0]
__________________________________________________________________________________________________
bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]
__________________________________________________________________________________________________
add_13 (Add)                    (None, None, None, 1 0           bn4f_branch2c[0][0]
                                                                 activation_37[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, None, None, 1 0           add_13[0][0]
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, None, None, 5 524800      activation_40[0][0]
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, None, None, 5 0           bn5a_branch2a[0][0]
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_41[0][0]
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, None, None, 5 0           bn5a_branch2b[0][0]
__________________________________________________________________________________________________
res5a_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_42[0][0]
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, None, None, 2 2099200     activation_40[0][0]
__________________________________________________________________________________________________
bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]
__________________________________________________________________________________________________
add_14 (Add)                    (None, None, None, 2 0           bn5a_branch2c[0][0]
                                                                 bn5a_branch1[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, None, None, 2 0           add_14[0][0]
__________________________________________________________________________________________________
res5b_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_43[0][0]
__________________________________________________________________________________________________
bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, None, None, 5 0           bn5b_branch2a[0][0]
__________________________________________________________________________________________________
res5b_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_44[0][0]
__________________________________________________________________________________________________
bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, None, None, 5 0           bn5b_branch2b[0][0]
__________________________________________________________________________________________________
res5b_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_45[0][0]
__________________________________________________________________________________________________
bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]
__________________________________________________________________________________________________
add_15 (Add)                    (None, None, None, 2 0           bn5b_branch2c[0][0]
                                                                 activation_43[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, None, None, 2 0           add_15[0][0]
__________________________________________________________________________________________________
res5c_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_46[0][0]
__________________________________________________________________________________________________
bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, None, None, 5 0           bn5c_branch2a[0][0]
__________________________________________________________________________________________________
res5c_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_47[0][0]
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, None, None, 5 0           bn5c_branch2b[0][0]
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_48[0][0]
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]
__________________________________________________________________________________________________
add_16 (Add)                    (None, None, None, 2 0           bn5c_branch2c[0][0]
                                                                 activation_46[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, None, None, 2 0           add_16[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 2048)         0           activation_49[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 5)            5125        dense_1[0][0]
==================================================================================================
Total params: 25,691,013
Trainable params: 2,103,301
Non-trainable params: 23,587,712
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phần train lại sẽ có khoảng hơn 2 triệu tham số, phần layer ở trước đó không train là khoảng 23 triệu tham số.&lt;/p&gt;

&lt;p&gt;Chia tập dữ liệu ra thành 5 phần, 4 phần làm tập train, 1 phần làm tập validation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y, tags = dataset.dataset(data_directory, n)
nb_classes = len(tags)


sample_count = len(y)
train_size = sample_count * 4 // 5
X_train = X[:train_size]
y_train = y[:train_size]
Y_train = np_utils.to_categorical(y_train, nb_classes)
X_test  = X[train_size:]
y_test  = y[train_size:]
Y_test = np_utils.to_categorical(y_test, nb_classes)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;chúng ta tiến hành thực hiện ImageDataGenerator để có được nhiều dữ liệu mẫu hơn và chống overfit, trong keras đã có sẵn hàm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;datagen = ImageDataGenerator(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        zca_whitening=False,
        rotation_range=45,
        width_shift_range=0.25,
        height_shift_range=0.25,
        horizontal_flip=True,
        vertical_flip=False,
        channel_shift_range=0.5,
        zoom_range=[0.5, 1.5],
        brightness_range=[0.5, 1.5],
        fill_mode=&#39;reflect&#39;)
        
datagen.fit(X_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cuối cùng, chúng ta sẽ xây dựng mô hình và tiến hành huấn luyện, lưu mô hình. Quá trình này tốn hơi nhiều thời gian.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = net.build_model(nb_classes)
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&amp;quot;accuracy&amp;quot;])

# train the model on the new data for a few epochs

print(&amp;quot;training the newly added dense layers&amp;quot;)

samples_per_epoch = X_train.shape[0]//batch_size*batch_size
steps_per_epoch = samples_per_epoch//batch_size
validation_steps = X_test.shape[0]//batch_size*batch_size

model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),
            samples_per_epoch=samples_per_epoch,
            epochs=nb_epoch,
            steps_per_epoch = steps_per_epoch,
            validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size),
            validation_steps=validation_steps,
            )


net.save(model, tags, model_file_prefix)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử download một vài hình ảnh trên mạng về rồi test thử xem sao&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../post_image/flower-classifition_demo.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả khá tốt phải không các bạn.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dự đoán doanh số bán của các cửa hàng walmart</title>
      <link>/blog/2019-04-17-walmart-store-sales-forecasting/</link>
      <pubDate>Mon, 15 Apr 2019 00:09:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-17-walmart-store-sales-forecasting/</guid>
      <description>

&lt;h1 id=&#34;nghiên-cứu-dữ-liệu&#34;&gt;Nghiên cứu dữ liệu&lt;/h1&gt;

&lt;p&gt;Trong thực tế, Walmart đã chạy các chương trình khuyến mãi trong các ngày lễ lớn trong năm. Có 4 ngày lễ lớn đó là Siêu cúp bóng bầu dục Mỹ (Super Bowl - tổ chức vào chủ nhật đầu tiên của tháng Hai. Đây là một sự kiện thể thao lớn và ngày tổ chức Super Bowl được người Mỹ coi là ngày lễ quốc gia của Hoa Kỳ (theo wiki &lt;a href=&#34;https://vi.wikipedia.org/wiki/Super_Bowl)&#34;&gt;https://vi.wikipedia.org/wiki/Super_Bowl)&lt;/a&gt;), ngày lễ lao động (Labor Day - ngày một tháng 5), lễ tạ ơn (Thanksgiving, ngày lễ tạ ơn ở Mỹ được tổ chức vào ngày thứ Năm lần thứ tư của tháng 11, còn ở Canada ngày lễ tạ ơn được tổ chức vào ngày thứ hai lần thứ hai của tháng 10, theo wiki &lt;a href=&#34;https://en.wikipedia.org/wiki/Thanksgiving&#34;&gt;https://en.wikipedia.org/wiki/Thanksgiving&lt;/a&gt;), lễ giáng sinh (Christmas ngày 24 và 25 tháng 12 theo wiki &lt;a href=&#34;https://en.wikipedia.org/wiki/Christmas&#34;&gt;https://en.wikipedia.org/wiki/Christmas&lt;/a&gt; ). Những tuần có chứa những ngày lễ lớn này được đánh trọng số gấp 5 lần những tuần khác. Chúng ta phải xây dựng mô hình để mô hình hoá các tác động của việc giảm giá trong các tuần lễ này khi không có dữ liệu lịch sử đầy đủ.&lt;/p&gt;

&lt;p&gt;Tập dữ liệu được cung cấp bao gồm:&lt;/p&gt;

&lt;p&gt;Tập train: chứa dữ liệu số bán từ 05-02-2010 đến 01-11-2012. Các trường dữ liệu là: store number - mã cửa hàng, Dept number - mã sản phẩm, Date - Tuần, Weekly_Sales - số bán, IsHoliday - Nếu tuần đó có chứa các holidate thì đánh 1 ngược lại đánh 0.&lt;/p&gt;

&lt;p&gt;Tập test: Chứa dữ liệu test, có các cột thuộc tính như tập train&lt;/p&gt;

&lt;p&gt;Tập features: Chứa thông tin thêm về của hàng, bao gồm store - mã cửa hàng, Date - ngày, Temperature - Nhiệt độ, Fuel_Price - giá dầu (ở mỹ, mỗi khu vực khác nhau sẽ có giá nhiên liệu khác nhau), MarkDown1, MarkDown2,&amp;hellip; , MarkDown5 - một chỉ số gì đó mà tác giả không cung cấp định nghĩa cho chúng ta, CPI - chỉ số giá tiêu dùng, Unemployment - tình trạng thất nghiệp, IsHoliday - Tuần có chứa ngày nghỉ.&lt;/p&gt;

&lt;h1 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h1&gt;

&lt;p&gt;Mình sẽ import một số thư viện cần thiết&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd 
import numpy as np

#Do some statistics
from scipy.misc import imread
from scipy import sparse
import scipy.stats as ss
import math

#Nice graphing tools
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đọc các file data lên, merge các file lại với nhau&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

train = pd.read_csv(&#39;data/train.csv&#39;)
test = pd.read_csv(&#39;data/test.csv&#39;)
feature = pd.read_csv(&#39;data/features.csv&#39;)

train = train.merge(feature, how=&#39;left&#39;, on=[&#39;Store&#39;,&#39;Date&#39;])
test = test.merge(feature, how=&#39;left&#39;, on=[&#39;Store&#39;,&#39;Date&#39;])


# Merge in store info
stores = pd.read_csv(&amp;quot;data/stores.csv&amp;quot;)
train = train.merge(stores, how=&#39;left&#39;, on=&#39;Store&#39;)
test = test.merge(stores, how=&#39;left&#39;, on=&#39;Store&#39;)
print(train.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   Store  Dept        Date  Weekly_Sales  IsHoliday_x  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday_y Type    Size  Split
0      1     1  2010-02-05      24924.50        False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106        False    A  151315  Train
1      1     1  2010-02-12      46039.49         True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106         True    A  151315  Train
2      1     1  2010-02-19      41595.55        False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106        False    A  151315  Train
3      1     1  2010-02-26      19403.54        False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106        False    A  151315  Train
4      1     1  2010-03-05      21827.90        False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106        False    A  151315  Train
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mới có 5 dòng đầu tiên mà thấy các chỉ số markdown Nan rồi.&lt;/p&gt;

&lt;p&gt;Chúng ta tiến hành một số phân tích dữ liệu. À, Mình sẽ merge dữ liệu train và test lại rồi phân tích thống kê&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.concat([train,test],axis=0) # Join train and test

print(df.describe())

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;                 CPI           Dept     Fuel_Price      MarkDown1      MarkDown2      MarkDown3      MarkDown4      MarkDown5          Size          Store    Temperature   Unemployment   Weekly_Sales
count  498472.000000  536634.000000  536634.000000  265596.000000  197685.000000  242326.000000  237143.000000  266496.000000  536634.00000  536634.000000  536634.000000  498472.000000  421570.000000
mean      172.090481      44.277301       3.408310    7438.004144    3509.274827    1857.913525    3371.556866    4324.021158  136678.55096      22.208621      58.771762       7.791888   15981.258123
std        39.542149      30.527358       0.430861    9411.341379    8992.047197   11616.143274    6872.281734   13549.262124   61007.71180      12.790580      18.678716       1.865076   22711.183519
min       126.064000       1.000000       2.472000   -2781.450000    -265.760000    -179.260000       0.220000    -185.170000   34875.00000       1.000000      -7.290000       3.684000   -4988.940000
25%       132.521867      18.000000       3.041000    2114.640000      72.500000       7.220000     336.240000    1570.112500   93638.00000      11.000000      45.250000       6.623000    2079.650000
50%       182.442420      37.000000       3.523000    5126.540000     385.310000      40.760000    1239.040000    2870.910000  140167.00000      22.000000      60.060000       7.795000    7612.030000
75%       213.748126      74.000000       3.744000    9303.850000    2392.390000     174.260000    3397.080000    5012.220000  202505.00000      33.000000      73.230000       8.549000   20205.852500
max       228.976456      99.000000       4.468000  103184.980000  104519.540000  149483.310000   67474.850000  771448.100000  219622.00000      45.000000     101.950000      14.313000  693099.360000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phân tích một chút:&lt;/p&gt;

&lt;p&gt;Bỏ qua cột Dept và Store vì nó là mã sản phẩm và mã của hàng, người ta thích đặt số bao nhiêu thì đặt.&lt;/p&gt;

&lt;p&gt;Các chỉ số MarkDown có độ lệch chuẩn khá cao.&lt;/p&gt;

&lt;p&gt;Nhiệt độ min là -7.29, max là 101.95, trung bình là 58, nên không thể là độ C được, có thể là độ F&lt;/p&gt;

&lt;p&gt;Xem thử hệ số tương quan giữa các column như thế nào&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.set(style=&amp;quot;white&amp;quot;)

# Compute the correlation matrix
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={&amp;quot;shrink&amp;quot;: .5})

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/walmart-corr.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Hệ số tương quan giữa các cột trong dữ liệu&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Phân tích một chút, chúng ta thấy rằng MarkDown5 hầu như không có liên quan gì đến các column còn lại. Hệ số trải từ -0.3 đến 0.3 chứng tỏ mổi quan hệ giữa các cột là khá lỏng lẻo. Chỉ số giá tiêu dùng tương quan tỷ lệ nghịch với tình trạng thất nghiệp (hợp lý không nhỉ). Kích thước cửa hàng càng bự thì bán càng nhiều (ok hiển nhiên), sản phẩm có mã càng lớn thì bán càng nhiều (? có lẽ là sản phẩm mới, người mỹ thích mua sản phẩm mới chăng). Và một vấn đề quan trọng là giá nhiên liệu, isHoliday, nhiệt độ không có mối tương quan với weekly sales. Chỉ số CPI và tình trạng thất nghiệp cũng ảnh hưởng không lớn với  weekly sales.&lt;/p&gt;

&lt;p&gt;Thử plot lên hình ảnh về số lượng bán và kích thước cửa hàng xem sao&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter( df[&#39;Size&#39;],df[&#39;Weekly_Sales&#39;])
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/walmart-size-sales-coeff.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Tương quan giữa số bán và kích thước cửa hàng&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nhìn vào hình trên, chúng ta thấy rằng cửa hàng có kích thước nhỏ số bán cũng không tăng đột biến khi gặp ngày lễ, cửa hàng kích thước siêu bự có tỷ lệ đột biến thấp, cửa hàng trung trung có đột biến, ở khúc size 125000 và số bán là 700000. Chúng ta hãy xem những ngày có số bán lớn rơi vào ngày nào.
Dựa vào bảng desription ở phía trên đã phân tích, trung bình của số bán là 15981 và lệch chuẩn là 22711, cộng lại là  15981 + 22711 = 38692, nhìn trên đô thị thì phần đột biến khá lớn. Max là 700000, min là 0 (cái này nhìn hình, không phải số thực tế ở bảng mô tả), mình sẽ lấy ra những ngày có số bán lớn hơn 350000 (vượt qua ngưỡng trung bình + độ lệch chuẩn rất nhều -&amp;gt; ngoại lệ là đây) xem những ngày đó là ngày gì&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
print(df.loc[df[&#39;Weekly_Sales&#39;] &amp;gt;350000].head(10))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In ra top 10 thằng đầu tiên&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
               CPI        Date  Dept  Fuel_Price  IsHoliday_x  IsHoliday_y  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5    Size  Split  Store  Temperature Type  Unemployment  Weekly_Sales
37201   126.669267  2010-11-26    72       2.752         True         True        NaN        NaN        NaN        NaN        NaN  205863  Train      4        48.08    A         7.127     381072.11
37253   129.836400  2011-11-25    72       3.225         True         True     561.45     137.88   83340.33      44.04    9239.23  205863  Train      4        47.96    A         5.143     385051.04
88428   126.983581  2010-12-24     7       3.236        False        False        NaN        NaN        NaN        NaN        NaN  126512  Train     10        57.06    B         9.003     406988.63
95373   126.669267  2010-11-26    72       3.162         True         True        NaN        NaN        NaN        NaN        NaN  126512  Train     10        55.33    B         9.003     693099.36
95377   126.983581  2010-12-24    72       3.236        False        False        NaN        NaN        NaN        NaN        NaN  126512  Train     10        57.06    B         9.003     404245.03
95425   129.836400  2011-11-25    72       3.760         True         True     174.72     329.00  141630.61      79.00    1009.98  126512  Train     10        60.68    B         7.874     630999.19
115222  126.669267  2010-11-26    72       3.162         True         True        NaN        NaN        NaN        NaN        NaN  112238  Train     12        47.66    B        14.313     359995.60
115274  129.836400  2011-11-25    72       3.622         True         True    5391.83       8.00   63143.29      49.27    2115.67  112238  Train     12        53.25    B        12.890     360140.66
128984  182.544590  2010-12-24     7       3.141        False        False        NaN        NaN        NaN        NaN        NaN  200898  Train     14        30.59    A         8.724     356867.25
135665  182.783277  2010-11-26    72       3.039         True         True        NaN        NaN        NaN        NaN        NaN  200898  Train     14        46.15    A         8.724     474330.10

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nhìn vào bảng trên, chúng ta thấy rằng 10 ngày đầu tiên tập trung chủ yếu ở tháng 11 và tháng 12, tháng 12 là 24-25 tháng 12 -&amp;gt; ngày noel, còn tháng 11 là 25-26 tháng 11 (ngày gì vậy ta, trong mô tả không thấy)
Tra lịch thì ngày 25 tháng 11 năm 2011 trúng thứ sáu, tra trên mạng một thông tin khá quan trong là &amp;ldquo;Black Friday sẽ rơi vào khoảng ngày 23-29 tháng 11&amp;rdquo; -&amp;gt; không nghi ngờ gì nữa có thể là ngày này đây.
Thử tra tiếp ngày 26 tháng 11 năm 2010, cũng là thứ sáu luôn -&amp;gt; ngày black friday và ngày noel có sức mua điên cuồng quá.&lt;/p&gt;

&lt;p&gt;Mình dùng một kỹ thuật nhỏ là giảm dần số bán, để ra số bán tối thiểu mà ngày black friday và ngày nodel vẫn còn giữ vị trí thống trị. Kỹ thuật khá đơn giản thôi, từ giá trị 350000, mỗi lần mình sẽ giảm đi 10000, và đếm số lần xuất hiện của các ngày, nếu có ngày nào đó nằm ngoài tuần chứa black friday và nodel thì mình dừng. Sau một hồi tìm kiếm và số bán đã xuất hiện, đó là 290000&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(df.loc[df[&#39;Weekly_Sales&#39;] &amp;gt;290000,&amp;quot;Date&amp;quot;].value_counts())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;2010-11-26    16
2011-11-25    14
2010-12-24     8
2011-12-23     3
2010-02-05     1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;làm-sạch-dữ-liệu&#34;&gt;Làm sạch dữ liệu&lt;/h1&gt;

&lt;h3 id=&#34;xử-lý-missing-values&#34;&gt;Xử lý missing values&lt;/h3&gt;

&lt;p&gt;Một vấn đề khá quan trọng là trong tập dữ liệu này missing value khá nhiều, thử đếm số lượng null trong data cho ta biết được rằng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;CPI              38162
Date                 0
Dept                 0
Fuel_Price           0
IsHoliday_x          0
IsHoliday_y          0
MarkDown1       271038
MarkDown2       338949
MarkDown3       294308
MarkDown4       299491
MarkDown5       270138
Size                 0
Split                0
Store                0
Temperature          0
Type                 0
Unemployment     38162
Weekly_Sales    115064
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Các giá trị MarkDown bị null khá nhiều, cách đơn giản nhất là set 0 cho tất cả các giá trị null ( Mình lưu log lại những index null của các markdown).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.assign(md1_present = df[&#39;MarkDown1&#39;]notnull())
df = df.assign(md2_present = df[&#39;MarkDown2&#39;]notnull())
df = df.assign(md3_present = df[&#39;MarkDown3&#39;]notnull())
df = df.assign(md4_present = df[&#39;MarkDown4&#39;]notnull())
df = df.assign(md5_present = df[&#39;MarkDown5&#39;].notnull())

df.fillna(0, inplace=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;tạo-đặc-trưng&#34;&gt;Tạo đặc trưng&lt;/h1&gt;

&lt;p&gt;Đặc trưng holiday&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;IsHoliday&#39;] = &#39;IsHoliday_&#39; + df[&#39;IsHoliday_x&#39;].map(str)
holiday_dummies = pd.get_dummies(df[&#39;IsHoliday&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trưng ngày tháng&lt;/p&gt;

&lt;p&gt;Rút trích tháng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;DateType&#39;] = [datetime.strptime(date, &#39;%Y-%m-%d&#39;).date() for date in df[&#39;Date&#39;].astype(str).values.tolist()]
df[&#39;Month&#39;] = [date.month for date in df[&#39;DateType&#39;]]
df[&#39;Month&#39;] = &#39;Month_&#39; + df[&#39;Month&#39;].map(str)
Month_dummies = pd.get_dummies(df[&#39;Month&#39;] )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Rút trích ngày trước giáng sinh và black friday&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;Black_Friday&#39;] = np.where((df[&#39;DateType&#39;]==datetime(2010, 11, 26).date()) | (df[&#39;DateType&#39;]==datetime(2011, 11, 25).date()), &#39;yes&#39;, &#39;no&#39;)
df[&#39;Pre_christmas&#39;] = np.where((df[&#39;DateType&#39;]==datetime(2010, 12, 23).date()) | (df[&#39;DateType&#39;]==datetime(2010, 12, 24).date()) | (df[&#39;DateType&#39;]==datetime(2011, 12, 23).date()) | (df[&#39;DateType&#39;]==datetime(2011, 12, 24).date()), &#39;yes&#39;, &#39;no&#39;)
df[&#39;Black_Friday&#39;] = &#39;Black_Friday_&#39; + df[&#39;Black_Friday&#39;].map(str)
df[&#39;Pre_christmas&#39;] = &#39;Pre_christmas_&#39; + df[&#39;Pre_christmas&#39;].map(str)
Black_Friday_dummies = pd.get_dummies(df[&#39;Black_Friday&#39;] )
Pre_christmas_dummies = pd.get_dummies(df[&#39;Pre_christmas&#39;] )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thêm các đặc trưng vào trong dữ liệu&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
df = pd.concat([df,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies],axis=1)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thêm đặc trưng trung vị của từng loại cửa hàng vào từng tháng, do một số của hàng sẽ bị NA ở cột số bán ở một thời điểm nào đó, nên chúng ta replace số bán là 0 có vẻ không hợp lý lắm. Mình chọn cách là thay thế bằng trung bình của số bán trong tháng của cửa hàng cùng loại. Nhưng trước tiên thì tính trung bình số bán của từng loại cửa hàng cái đã.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
medians = pd.DataFrame({&#39;Median Sales&#39; :df.loc[df[&#39;Split&#39;]==&#39;Train&#39;].groupby(by=[&#39;Type&#39;,&#39;Dept&#39;,&#39;Store&#39;,&#39;Month&#39;,&#39;IsHoliday&#39;])[&#39;Weekly_Sales&#39;].median()}).reset_index()
print(medians.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;     Type    Dept    Store     Month        IsHoliday  Median Sales
0  Type_A  Dept_1  Store_1   Month_1  IsHoliday_False     17350.585
1  Type_A  Dept_1  Store_1  Month_10  IsHoliday_False     23388.030
2  Type_A  Dept_1  Store_1  Month_11  IsHoliday_False     19551.115
3  Type_A  Dept_1  Store_1  Month_11   IsHoliday_True     19865.770
4  Type_A  Dept_1  Store_1  Month_12  IsHoliday_False     39109.390
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;thêm dữ liệu vào trong data chính, loại bỏ NA và tạo key cho mỗi dòng để dễ dàng truy xuất&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.merge(medians, how = &#39;outer&#39;, on = [&#39;Type&#39;,&#39;Dept&#39;,&#39;Store&#39;,&#39;Month&#39;,&#39;IsHoliday&#39;])

# Fill NA
df[&#39;Median Sales&#39;].fillna(df[&#39;Median Sales&#39;].loc[df[&#39;Split&#39;]==&#39;Train&#39;].median(), inplace=True) 

# Create a key for easy access

df[&#39;Key&#39;] = df[&#39;Type&#39;].map(str)+df[&#39;Dept&#39;].map(str)+df[&#39;Store&#39;].map(str)+df[&#39;Date&#39;].map(str)+df[&#39;IsHoliday&#39;].map(str)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta sẽ dự đoán số bán của tuần kế tiếp dựa vào kết quả số bán của tuần hiện tại, nên trong dữ liệu sẽ lưu trên ngày của tuần trước đó để dễ truy xuất. Vì 1 tuần có 7 ngày, chúng ta sẽ lưu giá trị là ngày ở cột hiện tại - 7&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;DateLagged&#39;] = df[&#39;DateType&#39;]- timedelta(days=7)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và giờ đây, chúng ta sẽ lặp qua toàn bộ các dòng trên tập dữ liệu, kiểm tra xem có dòng nào số bán nan hông, nếu có thì sẽ thay bằng trung bình đã tính ở trên. Ở đây mình tạo một sorted dataset để truy xuất cho nhanh&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#Make a sorted dataframe. This will allow us to find lagged variables much faster!
sorted_df = df.sort_values([&#39;Store&#39;, &#39;Dept&#39;,&#39;DateType&#39;], ascending=[1, 1,1])
sorted_df = sorted_df.reset_index(drop=True) # Reinitialize the row indices for the loop to work

sorted_df[&#39;LaggedSales&#39;] = np.nan # Initialize column
sorted_df[&#39;LaggedAvailable&#39;] = np.nan # Initialize column
last=df.loc[0] # intialize last row for first iteration. Doesn&#39;t really matter what it is
row_len = sorted_df.shape[0]
for index, row in sorted_df.iterrows():
    lag_date = row[&amp;quot;DateLagged&amp;quot;]
    # Check if it matches by comparing last weeks value to the compared date 
    # And if weekly sales aren&#39;t 0
    if((last[&#39;DateType&#39;]== lag_date) &amp;amp; (last[&#39;Weekly_Sales&#39;]&amp;gt;0)): 
        sorted_df.set_value(index, &#39;LaggedSales&#39;,last[&#39;Weekly_Sales&#39;])
        sorted_df.set_value(index, &#39;LaggedAvailable&#39;,1)
    else:
        sorted_df.set_value(index, &#39;LaggedSales&#39;,row[&#39;Median Sales&#39;]) # Fill with median
        sorted_df.set_value(index, &#39;LaggedAvailable&#39;,0)

    last = row #Remember last row for speed
    if(index%int(row_len/10)==0): #See progress by printing every 10% interval
        print(str(int(index*100/row_len))+&#39;% loaded&#39;)

print(sorted_df[[&#39;Dept&#39;, &#39;Store&#39;,&#39;DateType&#39;,&#39;LaggedSales&#39;,&#39;Weekly_Sales&#39;,&#39;Median Sales&#39;]].head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;9% loaded
19% loaded
29% loaded
39% loaded
49% loaded
59% loaded
69% loaded
79% loaded
89% loaded
99% loaded
     Dept    Store    DateType  LaggedSales  Weekly_Sales  Median Sales
0  Dept_1  Store_1  2010-02-05     23510.49      24924.50      23510.49
1  Dept_1  Store_1  2010-02-12     24924.50      46039.49      37887.17
2  Dept_1  Store_1  2010-02-19     46039.49      41595.55      23510.49
3  Dept_1  Store_1  2010-02-26     41595.55      19403.54      23510.49
4  Dept_1  Store_1  2010-03-05     19403.54      21827.90      21280.40
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Công việc đơn giản tiếp theo là merge dữ liệu vào data chính và tính độ lệch giữa 2 tuần bán&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Merge by store and department
df = df.merge(sorted_df[[&#39;Dept&#39;, &#39;Store&#39;,&#39;DateType&#39;,&#39;LaggedSales&#39;,&#39;LaggedAvailable&#39;]], how = &#39;inner&#39;, on = [&#39;Dept&#39;, &#39;Store&#39;,&#39;DateType&#39;])
df[&#39;Sales_dif&#39;] = df[&#39;Median Sales&#39;] - df[&#39;LaggedSales&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và bây giờ , thay vì ta ước lượng weekly sales, chúng ta sẽ ước lượng độ lệch giữa week sales và median sales (đây là một cách trong những cách để tính điểm dừng của dữ liệu time series)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;Difference&#39;] = df[&#39;Median Sales&#39;] - df[&#39;Weekly_Sales&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;huấn-luyện-mô-hình&#34;&gt;Huấn luyện mô hình&lt;/h1&gt;

&lt;p&gt;Lựa chọn các đặc trưng huấn luyện&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;selector = [
    #&#39;Month&#39;,
    &#39;CPI&#39;,
    &#39;Fuel_Price&#39;,
    &#39;MarkDown1&#39;,
    &#39;MarkDown2&#39;,
    &#39;MarkDown3&#39;,
    &#39;MarkDown4&#39;,
    &#39;MarkDown5&#39;,
    &#39;Size&#39;,
    &#39;Temperature&#39;,
    &#39;Unemployment&#39;,
    
    
    
    &#39;md1_present&#39;,
    &#39;md2_present&#39;,
    &#39;md3_present&#39;,
    &#39;md4_present&#39;,
    &#39;md5_present&#39;,

    &#39;IsHoliday_False&#39;,
    &#39;IsHoliday_True&#39;,
    &#39;Pre_christmas_no&#39;,
    &#39;Pre_christmas_yes&#39;,
    &#39;Black_Friday_no&#39;,
    &#39;Black_Friday_yes&#39;,    
    &#39;LaggedSales&#39;,
    &#39;Sales_dif&#39;,
    &#39;LaggedAvailable&#39;
    ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tách dữ liệu train và test riêng ra&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
train = df.loc[df[&#39;Split&#39;]==&#39;Train&#39;]
test = df.loc[df[&#39;Split&#39;]==&#39;Test&#39;]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lấy ngẫu nhiên 20% dữ liệu ở tập train để validation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set seed for reproducability 
np.random.seed(42)
X_train, X_val, y_train, y_val = train_test_split(train[selector], train[&#39;Difference&#39;], test_size=0.2, random_state=42)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Huấn luyện bằng neural network sử dụng lstm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
adam_regularized = Sequential()

    # First hidden layer now regularized
    model.add(Dense(32,activation=&#39;relu&#39;,
                    input_dim=X_train.shape[1],
                    kernel_regularizer = regularizers.l2(0.01)))

    # Second hidden layer now regularized
    adam_regularized.add(Dense(16,activation=&#39;relu&#39;,
                       kernel_regularizer = regularizers.l2(0.01)))

    # Output layer stayed sigmoid
    adam_regularized.add(Dense(1,activation=&#39;linear&#39;))

    # Setup adam optimizer
    adam_optimizer=keras.optimizers.Adam(lr=0.01,
                    beta_1=0.9, 
                    beta_2=0.999, 
                    epsilon=1e-08)

    # Compile the model
    adam_regularized.compile(optimizer=adam_optimizer,
                  loss=&#39;mean_absolute_error&#39;,
                  metrics=[&#39;acc&#39;])

    # Train
    history=adam_regularized.fit(X_train, y_train, # Train on training set
                                 epochs=10, # We will train over 1,000 epochs
                                 batch_size=2048, # Batch size 
                                 verbose=0) # Suppress Keras output
    print(&#39;eval&#39;,model.evaluate(x=X_val,y=y_val))

    # Plot network
    plt.plot(history.history[&#39;loss&#39;], label=&#39;Adam Regularized&#39;)
    plt.xlabel(&#39;Epochs&#39;)
    plt.ylabel(&#39;loss&#39;)
    plt.legend()
    plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;eval:  [1457.0501796214685, 0.002312783168124545]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/walmart_keras_lstm_adam.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;strong&gt;Độ lỗi trên tập train&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Độ lỗi trên tập train giảm xuống đến gần 1450 thì đừng hẳn, không thể giảm được nữa&lt;/p&gt;

&lt;p&gt;Giá trị độ lệch trên tập evaluation là 1457.0501796214685&lt;/p&gt;

&lt;p&gt;Thử huấn luyện bằng random forest&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;regr = RandomForestRegressor(n_estimators=20, criterion=&#39;mse&#39;, max_depth=None, 
                        min_samples_split=2, min_samples_leaf=1, 
                        min_weight_fraction_leaf=0.0, max_features=&#39;auto&#39;, 
                        max_leaf_nodes=None, min_impurity_decrease=0.0, 
                        min_impurity_split=None, bootstrap=True, 
                        oob_score=False, n_jobs=1, random_state=None, 
                        verbose=2, warm_start=False)

    #Train on data
    regr.fit(X_train, y_train.ravel())
    y_pred_random = regr.predict(X_val)

    y_val = y_val.to_frame()

    # Transform forest predictions to observe direction of change
    direction_true1= y_val.values
    direction_predict = y_pred_random

    y_val[&#39;Predicted&#39;] = y_pred_random
    df_out = pd.merge(train,y_val[[&#39;Predicted&#39;]],how = &#39;left&#39;,left_index = True, right_index = True,suffixes=[&#39;_True&#39;,&#39;_Pred&#39;])
    df_out = df_out[~pd.isnull(df_out[&#39;Predicted&#39;])]

    df_out[&#39;prediction&#39;] = df_out[&#39;Median Sales&#39;]-df_out[&#39;Predicted&#39;]

    print(&amp;quot;Medians: &amp;quot;+str(sum(abs(df_out[&#39;Difference&#39;]))/df_out.shape[0]))
    print(&amp;quot;Random Forest: &amp;quot;+str(sum(abs(df_out[&#39;Weekly_Sales&#39;]-df_out[&#39;prediction&#39;]))/df_out.shape[0])) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
9% loaded
19% loaded
29% loaded
39% loaded
49% loaded
59% loaded
69% loaded
79% loaded
89% loaded
99% loaded
     Dept    Store    DateType  LaggedSales  Weekly_Sales  Median Sales
0  Dept_1  Store_1  2010-02-05     23510.49      24924.50      23510.49
1  Dept_1  Store_1  2010-02-12     24924.50      46039.49      37887.17
2  Dept_1  Store_1  2010-02-19     46039.49      41595.55      23510.49
3  Dept_1  Store_1  2010-02-26     41595.55      19403.54      23510.49
4  Dept_1  Store_1  2010-03-05     19403.54      21827.90      21280.40
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
building tree 1 of 20
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.5s remaining:    0.0s
building tree 2 of 20
building tree 3 of 20
building tree 4 of 20
building tree 5 of 20
building tree 6 of 20
building tree 7 of 20
building tree 8 of 20
building tree 9 of 20
building tree 10 of 20
building tree 11 of 20
building tree 12 of 20
building tree 13 of 20
building tree 14 of 20
building tree 15 of 20
building tree 16 of 20
building tree 17 of 20
building tree 18 of 20
building tree 19 of 20
building tree 20 of 20
[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  2.2min finished
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.1s finished
Medians: 1545.7406070759525
Random Forest: 1356.4670052620745

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trung bình lệch của random forest là 1356, giá trị này nhỏ hơn so với giá trị output của lstm trả về.&lt;/p&gt;

&lt;p&gt;Thử huấn luyện bằng XGBoost&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
param_dist = { &#39;max_depth&#39;:5}

    model = XGBRegressor(**param_dist)

    #Train on data
    model.fit(X_train, y_train.ravel())
    y_pred_random = model.predict(X_val)

    y_val = y_val.to_frame()

    # Transform forest predictions to observe direction of change
    direction_true1= y_val.values
    direction_predict = y_pred_random

    y_val[&#39;Predicted&#39;] = y_pred_random
    df_out = pd.merge(train,y_val[[&#39;Predicted&#39;]],how = &#39;left&#39;,left_index = True, right_index = True,suffixes=[&#39;_True&#39;,&#39;_Pred&#39;])
    df_out = df_out[~pd.isnull(df_out[&#39;Predicted&#39;])]

    df_out[&#39;prediction&#39;] = df_out[&#39;Median Sales&#39;]-df_out[&#39;Predicted&#39;]

    print(&amp;quot;Medians: &amp;quot;+str(sum(abs(df_out[&#39;Difference&#39;]))/df_out.shape[0]))
    print(&amp;quot;XGB Regressor: &amp;quot;+str(sum(abs(df_out[&#39;Weekly_Sales&#39;]-df_out[&#39;prediction&#39;]))/df_out.shape[0])) 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
Medians: 1545.7406070759525
XGB Regressor: 1354.1976755192593

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả cũng gần như bằng Random forest :).&lt;/p&gt;

&lt;p&gt;Giờ mình sẽ dùng random forest để tạo file submission&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

rf_model = RandomForestRegressor(n_estimators=80, criterion=&#39;mse&#39;, max_depth=None, 
                      min_samples_split=2, min_samples_leaf=1, 
                      min_weight_fraction_leaf=0.0, max_features=&#39;auto&#39;, 
                      max_leaf_nodes=None, min_impurity_decrease=0.0, 
                      min_impurity_split=None, bootstrap=True, 
                      oob_score=False, n_jobs=1, random_state=None, 
                      verbose=0, warm_start=False)

#Train on data
rf_model.fit(train[selector], train[&#39;Difference&#39;])
final_y_prediction = rf_model.predict(test[selector])

testfile = pd.concat([test.reset_index(drop=True), pd.DataFrame(final_y_prediction)], axis=1)
testfile[&#39;prediction&#39;] = testfile[&#39;Median Sales&#39;]-testfile[0]

submission = pd.DataFrame({&#39;id&#39;:pd.Series([&#39;&#39;.join(list(filter(str.isdigit, x))) for x in testfile[&#39;Store&#39;]]).map(str) + &#39;_&#39; +
                           pd.Series([&#39;&#39;.join(list(filter(str.isdigit, x))) for x in testfile[&#39;Dept&#39;]]).map(str)  + &#39;_&#39; +
                           testfile[&#39;Date&#39;].map(str),
                          &#39;Weekly_Sales&#39;:testfile[&#39;prediction&#39;]})

submission.to_csv(&#39;submission.csv&#39;,index=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau khi submit mô hình, mình đạt được kết quả là 4455.96312 trên private board, và 4419.17292 trên publish board. Đây là một kết quả khá tệ (đứng hạng khoảng top 300). Sau khi mình nhìn lại mô hình thì phát hiện một số vấn đề.&lt;/p&gt;

&lt;p&gt;Các đặc trưng trong file features.csv nó không có mối tương quan gì hết với số bán như phân tích ở trên -&amp;gt; mình mạnh dạng bỏ luôn file features.csv, không quan tâm đến nó nữa, tập trung vào file chính.&lt;/p&gt;

&lt;p&gt;Bỏ mấy cái lag luôn, thử forecast chính vào cái số bán luôn xem sao&lt;/p&gt;

&lt;p&gt;Với cửa hàng nào thì xây dựng mô hình cho cửa hàng và sản phẩm đó, không xây dựng một mô hình tổng quát áp dụng cho toàn cửa hàng. với những cửa hàng không có trong tập train hoặc những sản phẩm mà cửa hàng đó chưa bán trước đây (nói chung là không có trong tập train) thì mới áp dụng mô hình của toàn cửa hàng cho nó.&lt;/p&gt;

&lt;p&gt;Kết quả là mình đạt được 2736 trên private board và 2657.40087 trên publish board (top 30), kết quả trên vẫn làm cho mình chưa hài lòng lắm.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thử làm ứng dụng tô màu ảnh xám thành ảnh màu sử dụng tensorflow</title>
      <link>/blog/2019-04-16-colorfull-grayscale-to-color/</link>
      <pubDate>Sun, 14 Apr 2019 00:13:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-16-colorfull-grayscale-to-color/</guid>
      <description>

&lt;h2 id=&#34;thực-hiện&#34;&gt;Thực hiện&lt;/h2&gt;

&lt;p&gt;Đây là một bài toán tiếp cận bằng Deep Learning, nên việc thu thập nhiều dữ liệu có ý nghĩa rất quang trọng trong việc đóng góp vào độ chính xác của mô hình. Ở đây, chúng ta sẽ download tập dữ liệu ảnh của &lt;a href=&#34;http://places2.csail.mit.edu/download.html&#34;&gt;http://places2.csail.mit.edu/download.html&lt;/a&gt; và sử dụng mạng UNet để huấn luyện mô hình.&lt;/p&gt;

&lt;h2 id=&#34;thu-thập-hình-ảnh-và-tiền-xử-lý&#34;&gt;Thu thập hình ảnh và tiền xử lý&lt;/h2&gt;

&lt;p&gt;Dữ liệu sẽ được download tại địa chỉ &lt;a href=&#34;http://data.csail.mit.edu/places/places365/train_256_places365challenge.tar&#34;&gt;http://data.csail.mit.edu/places/places365/train_256_places365challenge.tar&lt;/a&gt;. Tập trên có kích thước 108 GB. Đây là tập ảnh thuộc hệ màu RGB. Chúng ta sẽ chuyển tập ảnh trên về hệ màu grayscale làm ảnh gốc cho quá trình huấn luyện. Có một mẹo nhỏ cho chúng ta rút ngắn quá trình huấn luyện nhưng vẫn đảm bảo được độ chính xác của mô hình là ngoài kênh màu RGB mà chúng ta hay xài, trên thế giới còn có kênh màu HSV, trong đó nếu chúng ta chuyển một ảnh ở kênh màu RGB về hệ màu HSV, và bỏ đi các giá trị H, S, chỉ giữ lại giá trị V, thì chất lượng ảnh xám của nó gần như là tương đương với ảnh grayscale sử dụng công thức &amp;ldquo;thần thánh&amp;rdquo; mà chúng ta được học ở môn xử lý ảnh grayscale =0.30*R + 0.59*G + 0.11*B&lt;/p&gt;

&lt;p&gt;Vì vậy, thay vì việc input là giá trị xám của ảnh, output là giá trị của các kênh màu RGB, chúng ta sẽ chuyển đổi bài toán lại là input là giá trị xám, output là giá trị H và S.&lt;/p&gt;

&lt;p&gt;Mô hình mạng Unet&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/unet.PNG&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mạng UNet là một mạng neural network được dùng khá phổ biến trong các cuộc thi phân đoạn ảnh, độ chính xác của nó so với các thuật toán khác là vượt trội hoàn toàn. Ở đây, chúng ta có 2 hướng tiếp cận, một là build một mạng Unet và random init weight rồi huấn luyện nó, cách thứ hai là build mạng unet sử dụng pretrain model rồi huấn luyện. Bởi vì đặc trưng của các pretrain model hoạt động khá tốt và được huấn luyện trên tập dataset lớn, nên mình sẻ sử dụng nó ở bài viết này. Song song đó, mình sẽ cung cấp một giải pháp kèm theo sử để sử dụng mạng mà không dùng pretrain model.&lt;/p&gt;

&lt;p&gt;Ú tưởng chính của mạng UNet tựa tựa như auto-encoder, từ ảnh gốc ban đầu, chúng sẽ được nén thông tin lại qua các phép biến đổi Conv2D (như các chú thích màu sắc của mũi tên trong hình trên), sau đó sẽ được &amp;ldquo;giải nén&amp;rdquo; về lại ảnh gốc ban đầu. Việc huấn luyện coi như là hoàn tất 100% nếu ảnh gốc với ảnh giải nén là là giống nhau hoàn toàn.&lt;/p&gt;

&lt;p&gt;Bài viết sẽ được cập nhật&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dự đoán giá cổ phiếu bằng mô hình mạng Echo State Networks</title>
      <link>/blog/2019-04-04-predicting-stock-prices-with-echo-state-networks---copy/</link>
      <pubDate>Thu, 04 Apr 2019 00:13:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-04-predicting-stock-prices-with-echo-state-networks---copy/</guid>
      <description>

&lt;p&gt;Trong cuốn The West Wing Script Book của Aaron Sorkin, ông ấy đã có một câu như thế này &amp;ldquo;There (is) order and even great beauty in what looks like total chaos. If we look closely enough at the randomness around us, patterns will start to emerge.&amp;rdquo;. Mình xin phép không dịch câu nói trên ra, bởi vì mình dịch khá tệ, và câu nói này khá nổi tiếng (đã được trích dẫn khá nhiều trên các bài viết của các bloger khác). Nhưng câu nói đó khá phù hợp với môi trường chứng khoán, nơi mà mọi thứ đều không rõ ràng và khá &amp;ldquo;hỗn loạn&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;dự-đoán-chuỗi-thời-gian&#34;&gt;Dự đoán chuỗi thời gian&lt;/h2&gt;

&lt;p&gt;Giá cổ phiếu trên thị trường chứng khoán thường được quy vào bài toán là time series. Các công ty đầu tư hoặc các nhà nghiên cứu, các nhà đầu tư hiện nay thường sử dụng phương pháp stochastic hoặc các cải tiến của phương pháp stochastic (ví dụ mô hình ARIMA, RegARIMA,&amp;hellip;) để đưa ra các dự đoán hợp lý phù hợp với các giá trị quá khứ. Mục tiêu cuối cùng là tìm ra một mô hình khả dĩ nhất để phản ánh quy luật của thị trường và sử dụng nó để sinh ra lợi nhuận (trở nên giàu có hơn :)).&lt;/p&gt;

&lt;h2 id=&#34;các-thuộc-tính-của-time-series&#34;&gt;Các thuộc tính của time series&lt;/h2&gt;

&lt;p&gt;Một trong các thuộc tính của chuỗi thời gian là tính dừng (stationary). Một chuỗi time series được gọi là có tính dừng nếu các thuộc tính có ý nghĩa thống kê của nó (ví dụ như là trung bình, độ lệch chuẩn) không đổi theo thời gian. Ở đây, chúng ta luận bàn nho nhỏ một chút vì sao tính dừng rất quang trọng trong chuỗi thời gian.&lt;/p&gt;

&lt;p&gt;Trước hết, hầu hết các mô hình về time series hiện tại được xây dựng trên một giả định tính dừng của chuỗi thời gian. Có nghĩa là nếu chuỗi thời gian ở trong quá khứ có một hành vi nào đó, thì khả năng cao là nó sẽ lặp lại trong tương lai. Ngoài ra, các lý thuyết liên quan đến tính dừng của chuỗi time series đã được các nhà nghiên cứu khai thác một cách triệt để và dễ ràng implement hơn là các lý thuyết về non-stationary trong time series.&lt;/p&gt;

&lt;p&gt;Tính dừng được định nghĩa bằng các tiêu chí rõ ràng và nghiêm ngặt. Tuy nhiên, trong bài toán thực tế, chúng ta có thể giả định rằng một chuỗi time series được coi là có tính dừng nếu các thuộc tính thống kê không đổi theo thời gian, nghĩa là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Giá trị trung bình không thay đổi. Nếu giá trị trung bình thay đổi, chuỗi thời gian sẽ có khuynh hướng đi lên hoặc đi xuống. Hình ảnh bên dưới, mô tả trực quan một chuỗi thời gian có tính dừng (trung bình không thay đổi), và một chuỗi thời gian không có tính dừng (trung bình thay đổi).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/const_mean_stationary_series.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Giá trị phương sai không thay đổi. Thuộc tính này còn được gọi là đồng đẳng (homoscedasticity). Hình bên dưới mô tả một chuỗi có phương sai thay đổi (không có tính dừng) và một chuỗi có phương sai bất biến (có tính dừng).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/const_variance_stationary_series.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tính tự tương tự không phụ thuộc vào thời gian&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/autocorrelation_stationary_series.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;vì-sao-chúng-ta-lại-quan-tâm-đến-tính-dừng-của-dữ-liệu&#34;&gt;Vì sao chúng ta lại quan tâm đến tính dừng của dữ liệu&lt;/h2&gt;

&lt;p&gt;Chúng ta quan tâm đến tính dừng của dữ liệu, đơn giản là bởi vì nếu dữ liệu không có tính dừng, chúng ta không thể xây dựng mô hình chuỗi thời gian (như đã nói ở trên, các nghiên cứu hiện nay đều dựa trên một cơ sở là dữ liệu có tính dừng). Trong trường hợp bạn có trong tay dữ liệu thuộc dạng time series, và một tiêu chí nào đó trong 3 tiêu chí mình đã liệu kê ở trên bị vi phạm, suy ra là dữ liệu của bạn không có tính dừng. Bạn phải chuyển đổi dữ liệu bạn đang có để cho nó có tính dừng. May mắn rằng cũng có nhiều nghiên cứu thực hiện việc này, ví dụ như &amp;ldquo;khử xu hướng (detrending)&amp;rdquo;, khử sai biệt (differencing)&amp;hellip;&lt;/p&gt;

&lt;p&gt;Nếu bạn mới chỉ bắt đầu phân tích chuỗi thời gian, bạn sẽ thấy việc làm trên khá là stupid. Lý thuyết tốt nhất hiện nay cho chuỗi thời gian là chia nhỏ nó ra thành các thành phần như là xu hướng (linear trend), mùa vụ (seasonal), chu kỳ, và yếu tố ngẫu nhiên. Dự đoán cho từng phần một, sau đó lấy tổng chúng lại.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/arima.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Đối với những ai đã quen thuộc với biến đổi Fourier, thì sẽ dễ dàng &amp;ldquo;cảm&amp;rdquo; hơn cái mình vừa nói ở trên.&lt;/p&gt;

&lt;h2 id=&#34;cách-xác-định-tính-dừng-của-dữ-liệu&#34;&gt;Cách xác định tính dừng của dữ liệu&lt;/h2&gt;

&lt;p&gt;Khá khó để xác định một biểu đồ chuỗi time series có tính dừng hay không (quan sát biểu đồ bằng mắt). Cho nên chúng ta sẽ sử dụng kiểm định Dickey-Fuller. Đây là một kiểm định thống kê để kiểm tra xem chuỗi dữ liệu có tính dừng hay không. Với giả thuyết null là chuỗi time series là một chuỗi không có tính dừng. Nếu giá trị  nhỏ hơn một ngưỡng p-value nào đó (thường là 0.05), chúng ta có quyền bác bỏ giả định null, và nói rằng chuỗi thời gian đang có là có tính dừng. Ở bài viết này, mình không đề cập đến mô hình kiểm định - vốn được học trong môn xác xuất thống kê. Các bạn có nhu cầu tìm hiểu thì có thể search trên google hoặc là xem lại sách xác suất thống kê.&lt;/p&gt;

&lt;h2 id=&#34;phương-pháp-dự-đoán-chuỗi-thời-gian-cơ-bản&#34;&gt;Phương pháp dự đoán chuỗi thời gian cơ bản&lt;/h2&gt;

&lt;p&gt;Phương pháp cơ bản nhất, đơn giản nhất, và để áp dụng nhất dược sử dụng để dự đoán chuỗi thời gian là moving average. Mô hình này thực hiện tính trung bình của t giá trị cuối cùng làm giá trị dự đoán của điểm tiếp theo. Ví dụ như để dự đoán giá chứng khoán của ngày thứ 2 của tuần tiếp theo, chúng ta sẽ lấy trung bình giá đóng của của 5 ngày trước đó (giá từ thứ hai đến thứ sáu tuần này).&lt;/p&gt;

&lt;p&gt;Đến đây, các bạn đã có một số hiểu biết về time series. Một mô hình khá nổi tiếng là ARIMA đã được sử dụng nhiều để phân tích và dự báo. Cách thực hiện của mô hình trên được trình bày tóm gọn trong hình mô tả bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/arima1.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;phương-pháp-dự-đoán-dựa-vào-mạng-neural-network&#34;&gt;Phương pháp dự đoán dựa vào mạng neural network&lt;/h2&gt;

&lt;p&gt;Thực tế, có rất nhiều mạng neural network đã được áp dụng để dự đoán mô hình chứng khoán. Các bạn có thể tìm đọc lại các bài viết trước đây của mình về sử dụng LSTM trong dự báo chứng khoán. Mô hình chứng khoán bằng mạng neural network nói chung phải đối mặt với một vấn đề khá &amp;ldquo;xương xẩu&amp;rdquo; là xử lý nhiễu và vanishing gradients. Trong đó, việc xử lý vanishing gradients là quan trọng nhất. Bản chất của mạng neural network là tối ưu hoá hàm lan truyền ngược bằng cách sử dụng đạo hàm giữa các lớp layer để chúng &amp;lsquo;học&amp;rsquo;. Trải qua nhiều layer, giá trị của đạo hàm sẽ càng ngày nhỏ dần vào xấp xỉ bằng 0. Giả sử chúng ta có một mô hình có 100 lớp hidden layer, chúng ta nhân 100 lần số 0.1 với nhau và boom, giá trị cuối cùng chung ta nhận được là 0, nghĩa là chúng ta chẳng học được cái gì cả.&lt;/p&gt;

&lt;p&gt;May mắn thay, tới thời điểm hiện tại, chúng ta có 3 cách để xử lý vấn đề trên:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Clipping gradients&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LSTM (Long Short Term Memory) hoặc GRU (Gate Recurrent Units)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Echo states RNNs&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kỹ thuật clipping gradients sử dụng một mẹo là khi giá trị đạo hàm quá lớn hoặc quá nhỏ, chúng ta sẽ không lấy đạo hàm nữa. Kỹ thuật này thoạt nhìn có vẻ hay, nhưng nó không thể ngăn chúng ta mất mát thông tin và đây là một ý tưởng khá tệ.&lt;/p&gt;

&lt;p&gt;RNN (LSTM hoặc GRU) là một kỹ thuật khác là điều chỉnh các kết nối theo một vài quy luật nhất định, ví dụ output của layer tầng 1 có thể là input của layer tầng 10, chứ không nhất thiết là input của layer tầng 2 như cách thông thường. Kỹ thuật này khá tốt về mặt lý thuyết. Tuy nhiên, có một vấn đề khá lớn khi sử dụng là chúng ta phải tính toán kỹ các kết nối để đảm bảo hệ thống hoạt động ổn đinh. Mô hình được xây dựng trên kỹ thuật này khá bự, làm cho thuật toán chạy chậm. Ngoài ra, tính hội tụ của thuật toán không được đảm bảo. Mô hình LSTM đơn giản mình có để ở hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/lstm.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mạng echo states network, là một mô hình mới được nghiên cứu gần đây, bản chất nó là một mảng recurrent neural network với các hidden layer liên kết &amp;ldquo;lỏng lẻo&amp;rdquo; với nhau. Lớp này được gọi là &amp;lsquo;reservoir&amp;rsquo; (như hình mô tả bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/echo_state_network.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Trong mô hình mạng  echo state network, chúng ta chỉ cần huấn luyện lại trọng số của lớp output, việc này giúp chúng ta rút ngắn thời gian huấn luyện mô hình, và tăng tốc qusa trình training.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-mạng-echo-state-networks&#34;&gt;Sử dụng mạng Echo State Networks&lt;/h2&gt;

&lt;p&gt;Về nguyên lý hoạt động của mô hình này, mình sẽ không đề cập ở đây. Chủ đề về mạng Echo State Networks mình sẽ nghiên cứu kỹ lưỡng và đề cập ở trong bài viết sắp tới. Mục tiêu của bài viết này là sử dụng mô hình Echo State Networks trong bài toán time series.&lt;/p&gt;

&lt;h4 id=&#34;dự-doán-chuỗi-time-series&#34;&gt;Dự doán chuỗi time series&lt;/h4&gt;

&lt;p&gt;Trước tiên, chúng ta sẽ import một số thư viện cần thiết, thư viện ESN đã có sẵn tại đường dẫn pyESN, các bạn download về rồi dùng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings(&#39;ignore&#39;)

# This is the library for the Reservoir Computing got it by: https://github.com/cknd/pyESN
from pyESN import ESN 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo chúng ta sẽ đọc file&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
data = open(&amp;quot;amazon.txt&amp;quot;).read().split()
data = np.array(data).astype(&#39;float64&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta sẽ xây dựng một mô hình ESN đơn giản&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
n_reservoir= 500
sparsity=0.2
rand_seed=23
spectral_radius = 1.2
noise = .0005


esn = ESN(n_inputs = 1,
      n_outputs = 1, 
      n_reservoir = n_reservoir,
      sparsity=sparsity,
      random_state=rand_seed,
      spectral_radius = spectral_radius,
      noise=noise)
      
      ```
      
Để đơn giản, mình sẽ tạo mô hình với dữ liệu tào lao như sau:input là một vector toàn số 1, output là các điểm dữ liệu của mình. Cho mô hình ESN học với số lượng phần tử là 1500, sau đó sẽ dự đoán 10 điểm dữ liệu tiếp theo. Với bước nhảy là 10, lặp 10 lần. Sau quá trình lặp, mình thu được 100 điểm dự đoán


```python
trainlen = 1500
future = 10
futureTotal=100
pred_tot=np.zeros(futureTotal)

for i in range(0,futureTotal,future):
    pred_training = esn.fit(np.ones(trainlen),data[i:trainlen+i])  # dữ liệu từ ngày i đến ngày i + trainlen
    prediction = esn.predict(np.ones(future))
    pred_tot[i:i+future] = prediction[:,0] # dự đoán cho ngày i+ trainlen + 1 đến ngày i + trainlen + future
    
    
    ```
    
Vẽ mô hình cùi mía của mình mới làm lên để xem dữ liệu dự đoán và dữ liệu thực tế chênh lệch như thế nào

```python
plt.figure(figsize=(16,8))
plt.plot(range(1000,trainlen+futureTotal),data[1000:trainlen+futureTotal],&#39;b&#39;,label=&amp;quot;Data&amp;quot;, alpha=0.3)
#plt.plot(range(0,trainlen),pred_training,&#39;.g&#39;,  alpha=0.3)
plt.plot(range(trainlen,trainlen+futureTotal),pred_tot,&#39;k&#39;,  alpha=0.8, label=&#39;Free Running ESN&#39;)

lo,hi = plt.ylim()
plt.plot([trainlen,trainlen],[lo+np.spacing(1),hi-np.spacing(1)],&#39;k:&#39;, linewidth=4)

plt.title(r&#39;Ground Truth and Echo State Network Output&#39;, fontsize=25)
plt.xlabel(r&#39;Time (Days)&#39;, fontsize=20,labelpad=10)
plt.ylabel(r&#39;Price ($)&#39;, fontsize=20,labelpad=10)
plt.legend(fontsize=&#39;xx-large&#39;, loc=&#39;best&#39;)
sns.despine()
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/echo_state_network_p1.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Độ phức tạp của mô hình là khá nhỏ khi so với mô hình RNN. Lý do là về bản chất, chúng ta chỉ huấn luyện trên trọng số của output layer, nó là một hàm tuyến tính. Do vậy, độ phức tạp tính toán chỉ giống như là việc tính một hàm hồi quy tuyến tính. Trong thực tế, độ phức tạp tính toán sẽ là O(N) với N là ố lượng hidden unit trong reservoir.&lt;/p&gt;

&lt;h4 id=&#34;tối-ưu-hoá-các-tham-số-hyper-parameters&#34;&gt;Tối ưu hoá các tham số Hyper parameters&lt;/h4&gt;

&lt;p&gt;Ở phần trước, chúng ta set đại các tham số spectral_radius = 1.2 và noise = .0005. Trong thực tế, chúng ta phải tìm các siêu tham số này bằng cách tìm ra mô hình trả về MSE là nhỏ nhất.&lt;/p&gt;

&lt;p&gt;Sử dụng kỹ thuật Grid Search với ngưỡng spectrum_radius nằm trong đoạn [0.5, 1.5] và noise nằm trong đoạn  noise [0.0001, 0.01], chú ý là các bạn có thể search ở đoạn lớn hơn. Kết quả thu được:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def MSE(yhat, y):
    return np.sqrt(np.mean((yhat.flatten() - y)**2))
    
    n_reservoir= 500
sparsity   = 0.2
rand_seed  = 23
radius_set = [0.9,  1,  1.1]
noise_set = [ 0.001, 0.004, 0.006]

radius_set = [0.5, 0.7, 0.9,  1,  1.1,1.3,1.5]
noise_set = [ 0.0001, 0.0003,0.0007, 0.001, 0.003, 0.005, 0.007,0.01]



radius_set_size  = len(radius_set)
noise_set_size = len(noise_set)

trainlen = 1500
future = 2
futureTotal= 100

loss = np.zeros([radius_set_size, noise_set_size])

for l in range(radius_set_size):
    rho = radius_set[l]
    for j in range(noise_set_size):
        noise = noise_set[j]

        pred_tot=np.zeros(futureTotal)

        esn = ESN(n_inputs = 1,
          n_outputs = 1, 
          n_reservoir = n_reservoir,
          sparsity=sparsity,
          random_state=rand_seed,
          spectral_radius = rho,
          noise=noise)

        for i in range(0,futureTotal,future):
            pred_training = esn.fit(np.ones(trainlen),data[i:trainlen+i])
            prediction = esn.predict(np.ones(future))
            pred_tot[i:i+future] = prediction[:,0]
        
        loss[l, j] = MSE(pred_tot, data[trainlen:trainlen+futureTotal])        
        print(&#39;rho = &#39;, radius_set[l], &#39;, noise = &#39;, noise_set[j], &#39;, MSE = &#39;, loss[l][j] )
        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 20.367056799629353)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 22.44956008062169)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 24.574909979223666)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 25.862558649155638)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 29.882933676750657)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 32.63942614291128)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 36.441245548726)
(&#39;rho = &#39;, 0.5, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 44.77637915282457)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 19.560517902720054)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 20.12742795009036)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 20.81801427735713)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 21.26142619965559)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 23.270880660885513)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 26.061347331527354)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 30.298361979419834)
(&#39;rho = &#39;, 0.7, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 39.17074955771047)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 18.612970860501118)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 18.681815816990774)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.835785386862582)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.982346096338105)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 20.81632098844061)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 24.60968377490799)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 30.231007189936882)
(&#39;rho = &#39;, 0.9, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 41.28587340583505)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 18.23852181110818)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 18.27010615150326)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.36078059388596)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.47920006882226)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 20.613227951906246)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 25.153712109142973)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 31.700838835741898)
(&#39;rho = &#39;, 1, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 44.23736750779224)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 17.981571756431556)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 18.009398312163942)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.09054736889828)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.218795249276663)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 20.82610561349463)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 26.272452530336505)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 33.91532767431614)
(&#39;rho = &#39;, 1.1, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 48.22002405965967)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 17.72839068197909)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 17.799908079894703)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 17.92917208443474)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.143905288756557)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 22.20343747458126)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 30.05977704513729)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 40.56654468067572)
(&#39;rho = &#39;, 1.3, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 59.43231026660687)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.0001, &#39;, MSE = &#39;, 17.627409489404897)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.0003, &#39;, MSE = &#39;, 17.835052829116567)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.0007, &#39;, MSE = &#39;, 18.100099619981393)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.001, &#39;, MSE = &#39;, 18.481406587483956)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.003, &#39;, MSE = &#39;, 24.887601182697498)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.005, &#39;, MSE = &#39;, 36.34166374510305)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.007, &#39;, MSE = &#39;, 50.99612645577753)
(&#39;rho = &#39;, 1.5, &#39;, noise = &#39;, 0.01, &#39;, MSE = &#39;, 75.94229622771246)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả thu được là giá trị MSE tốt nhất là spectrum radius =  1.5 và nnoise  = 0.0001&lt;/p&gt;

&lt;p&gt;Thử dự đoán giá cổ phiếu của tập đoàn thế giới di động (Mã cổ phiếu MWG) xem sao&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/echo_state_network_mwg.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ở hình trên, mình không tiến hành grid search mà lấy lại các hyper parameters cũ để huấn luyện mô hình. Kết quả như hình trên  mình thấy cũng khá tốt rồi, nên mình không tiến hành grid search lại để tìm kết quả tốt hơn.&lt;/p&gt;

&lt;p&gt;Dựa vào kết quả chúng ta thu được, có thể nói rằng mô hình ESN dự đoán khá tốt dữ liệu thuộc dạng time series với độ hỗn loạn cao. Đây là một kết luận nhỏ của mình dựa vào bằng chứng trên việc mình test trên tập dữ liệu ngẫu nhiên mà mình có.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Các lý do mạng neural network không hoạt động không chính xác</title>
      <link>/blog/2019-04-02-37-reason-neural-network-not-working/</link>
      <pubDate>Tue, 02 Apr 2019 00:13:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-02-37-reason-neural-network-not-working/</guid>
      <description>

&lt;p&gt;Bạn huấn luyện một hình mất hơn 12 tiếng đồng hồ. Mọi thứ khá ổn: loss function giảm. Nhưng khi bạn mang mô hình ra predict thì điều tồi tệ nhất xảy ra: Tất cả trả về đều là 0, không có cái nào nhận dạng chính xác cả. &amp;ldquo;Điều gì đã xảy ra, bạn đã làm gì sai?&amp;rdquo;. Bạn hỏi máy tính, nó không trả lời bạn. Bạn đập bàn, đập ghế trong cơn tức giận và chẳng giải quyết được điều gì cả.&lt;/p&gt;

&lt;p&gt;Có rất nhiều nguyên nhân gây ra vấn đề này. Việc cần làm của các bạn là phải tìm ra chính xác nguyên nhân và &amp;ldquo;sửa&amp;rdquo; nó, sau đó tốn hơn 12 tiếng đồng hồ để huấn luyện lại :), rồi lại sửa &amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;hướng-dẫn-ban-đầu&#34;&gt;Hướng dẫn ban đầu&lt;/h2&gt;

&lt;p&gt;Nếu bạn gặp tình trạng như phần mô tả ở trên, bạn hãy thực hiện các bước mình mô tả bên dưới thử xem vấn đề của bạn là gì?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bắt đầu huấn luyện mô hình bằng một mô hình đơn giản mà bạn biết chắc rằng nó hoạt động tốt với tập dữ liệu bạn đang có. Ví dụ, trong bài toán object detection, hãy sử dụng mô hình VGG. Và bạn hãy cố gắng sửa dụng standard loss nếu có thể.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bỏ qua những thứ râu ria như là regularization hoặc data augmentation. Hãy tập trung vào xây dựng một mô hình cho một kết quả khả quan cái đã, sau đó mới cải tiến bằng các thứ râu ria trên sau.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu bạn finetuning một mô hình, bạn hãy kiểm tra thật kỹ quá trình tiền xử lý dữ liệu. Chắc chắn rằng quá trình tiền xử lý của bạn giống y chang quá trình tiền xử lý của mô hình gốc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chắc chắn 100% rằng giá trị đầu vào là đúng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bắt đầu bằng một tập sample nhỏ (từ 2 đến 20 mẫu). Huấn luyện nó đến khi bị overfit và bổ sung thêm mẫu huấn luyện sau khi mô hình của bạn bị overfit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bổ sung thên các yếu tố râu ria như augmentation/regularization,  custom loss functions, thử với một mô hình phức tạp hơn.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nếu những cách trên vẫn không thành công. Mô hình vẫn trả về giá trị zero. Bạn có thể mắc phải một số lỗi được liệt kê bên dưới.&lt;/p&gt;

&lt;p&gt;Kiểm tra rằng dữ liệu của bạn đưa vào mạng neural netwok thật sự có ý nghĩa và đúng. Ví dụ, hãy đảm bảo rằng bạn không nhầm lẫn / swap giá trị giữa width và height của hình ảnh, hoặc một lý do nào đó bạn đưa vào một zero image, hoặc bạn chỉ huấn luyện duy nhất một batch (ví dụ dữ liệu bạn lớn, chia làm 10 batch, và code nhầm sao đó chỉ đưa input là batch số 1 vào).&lt;/p&gt;

&lt;p&gt;Một trường hợp nữa là khi input và output của bạn chẳng có mối liên hệ gì với nhau, và không cách nào nhận biết rằng nó phụ thuộc nhau bởi vì bản chất của dữ liệu là như vậy, hoặc input của bạn đang có chưa đủ chứng cứ để suy ra output. Một ví dụ của trường hợp này là giá chứng khoáng.&lt;/p&gt;

&lt;p&gt;Kiểm tra kỹ dữ liệu train để đảm bảo không có đánh nhãn sai&lt;/p&gt;

&lt;p&gt;Kiểm tra xem dữ liệu có bị mất cân bằng không. Hãy sử dụng các kỹ thuật để cân bằng lại dữ liệu.&lt;/p&gt;

&lt;p&gt;Đảm bảo rằng trong 1 batch chứa dữ liệu của nhiều hơn 1 nhãn. Hãy xáo trộn ngẫu nhiên dữ liệu để tránh lỗi này.&lt;/p&gt;

&lt;p&gt;Bài báo &lt;a href=&#34;https://arxiv.org/abs/1609.04836&#34;&gt;https://arxiv.org/abs/1609.04836&lt;/a&gt; chỉ ra rằng khi bạn huấn luyện mô hình với batch size lớn có thể làm giảm tính tổng quát của mô hình.&lt;/p&gt;

&lt;p&gt;Khoá học CS231 đã chỉ ra một lỗi khá phổ biến: &amp;ldquo;Bất kỳ một quá trình tiền xử lý nào cũng phải thực hiện trên tập train, và sau đó áp dụng vào tập validation,test&amp;rdquo;. Ví dụ, chúng ta tính trung bình trên toàn bộ dữ liệu, rồi sau đó chia tập dữ liệu thành train, test, predict là không đúng. Hành động đúng là chia tập dữ liệu thành train, test, vali trước, sau đó tính giá trị trung bình trên từng kênh màu trên tập train, rồi mới lấy giá trị trung bình đó áp cho tập test và tập validate.&lt;/p&gt;

&lt;p&gt;Một vấn đề khác có thể là &amp;ldquo;Look for correct loss at chance performance&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;Ví dụ, với tập dữ liệu CIFAR-10 sử dụng softmax classifier, ở lần đầu tiên, giá trị loss mong đợi của chúng ta là 2.303, bởi vì có 1 thằng đúng, 10 thằng sai, xác suất là &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt; = 0.1. softmax loss là -ln(0.1) = 2.302.&lt;/p&gt;

&lt;p&gt;Với dữ liệu CIFAR-10 dùng SVM, ở lần lặp đầu tiên, giá trị loss chúng ta kỳ vọng là 9 (với mỗi lớp sai, giá trị margin sẽ là 1).&lt;/p&gt;

&lt;p&gt;Nếu các giá trị trả ra không giống như mong đợi, vấn đề xảy ra là do giá trị init không đúng.&lt;/p&gt;

&lt;p&gt;Một vấn đề nữa là khi tăng giá trị regularization thì cũng đồng thời tăng giá trị loss. =&amp;gt; Nếu loss không tăng =&amp;gt; có vấn đề.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch từ &lt;a href=&#34;https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?fbclid=IwAR1Qj6jJW87oKi_LR7xWMDOMZDTx8xwLZEhCCMuvOw63ztwdD1MknZVjm_Q&#34;&gt;https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?fbclid=IwAR1Qj6jJW87oKi_LR7xWMDOMZDTx8xwLZEhCCMuvOw63ztwdD1MknZVjm_Q&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trí tuệ nhân tạo, Máy học, Dữ liệu lớn</title>
      <link>/blog/2019-04-02-deep-learning-view/</link>
      <pubDate>Tue, 02 Apr 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-04-02-deep-learning-view/</guid>
      <description>

&lt;p&gt;Trong vài năm trở lại đây (khoảng từ 2013) truyền thông trong và ngoài nước có khá nhiều bài viết giật tít về “Cách mạng công nghiệp lần thứ tư” hay “Thời đại công nghiệp 4.0”. Cùng với các cụm từ này, “Trí tuệ nhân tạo”, “Máy học”, “Dữ liệu lớn” lại được nhắc đến với tần suất cao hơn. Vậy thì những thuật ngữ này có ý nghĩa gì và giữa chúng có mối liên hệ nào với nhau hay không? Trong bài viết này, chúng ta sẽ cùng tìm hiểu.&lt;/p&gt;

&lt;h2 id=&#34;trí-tuệ-nhân-tạo&#34;&gt;Trí tuệ nhân tạo&lt;/h2&gt;

&lt;p&gt;Năm 2016, trong “Trận thách đấu của Google DeepMind” được tổ chức tại Hàn Quốc, AlphaGo  (một phần mềm chơi cờ vây trên máy tính được xây dựng bởi Google DeepMind) đã dành chiến thắng &lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt; ván trước Lee Sedol (người từng 18 lần vô địch giải cờ vây thế giới) là sự kiện quan trọng khiến con người có thể tin tưởng vào tương lai và sức mạnh của &lt;em&gt;trí tuệ nhân tạo&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Sau khi trận đấu kết thúc, chính phủ Hàn Quốc công bố rằng họ sẽ đầu từ 863 triệu USD (khoảng 1 nghìn tỷ won) vào nghiên cứu trí tuệ nhân tạo trong vòng vài năm tiếp theo.&lt;/p&gt;

&lt;p&gt;Tính tới nay, lượng dữ liệu các trận đấu cờ vây được nhận vào giúp AlphaGO có kinh nghiệm tương đương với 80 năm chơi cờ vây liên tục. Một con số đáng ngạc nhiên và ngưỡng mộ.&lt;/p&gt;

&lt;p&gt;Như vậy trí tuệ nhân tạo là gì?&lt;/p&gt;

&lt;p&gt;Trí tuệ nhân tạo (AI - Artificial Intelligence) là một nhánh nghiên cứu trong lĩnh vực khoa học máy tính và từ lâu đã được rất nhiều các nhà nghiên cứu quan tâm. Thuật ngữ AI được đặt bởi nhà khoa học máy tính người Mỹ - John McCarthy vào năm 1956 tại Hội nghị Dartmouth. Cho đến thời điểm hiện tại thì có khá nhiều những phát biểu khác nhau về AI bởi các chuyên gia, chẳng hạn như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AI là khoa học nghiên cứu giúp tạo ra máy tính có khả năng suy nghĩ, đầy trí tuệ như tên của chính nó (Haugeland, 1985).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AI là khoa học nghiên cứu các hoạt động trí não thông qua các mô hình tính toán (Chaniaka và McDemott, 1985).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AI là khoa học nghiên cứu cách để máy tính có thể thực hiện được những công việc mà con người làm tốt hơn máy (Rich và Knight, 1991).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AI là khoa học nghiên cứu các mô hình máy tính có thể nhận thức, lập luận và hành động (Winston, 1992).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AI là khoa học nghiên cứu các hành vi thông minh mô phỏng các vật thể nhân tạo (Nilsson, 1998)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AI là khoa học nghiên cứu các hành vi thông minh nhằm giải quyết các vấn đề được đặt ra đối với các chương trình máy tính (Học viện Kỹ thuật Quân sự).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Như vậy, từ những định nghĩa trên chúng ta có thể rút ra định nghĩa tổng quát rằng &lt;em&gt;trí tuệ nhân tạo hay trí thông minh nhân tạo là trí tuệ được biểu diễn bởi bất kỳ một hệ thống nhân tạo nào. Hệ thống đó sẽ mô phỏng các quá trình hoạt động trí tuệ của con người, bao gồm quá trình học tập, lập luận và tự sửa lỗi&lt;/em&gt;. Do đó, trí thông minh nhân tạo liên quan đến cách hành xử, sự học hỏi và khả năng thích ứng thông minh của máy móc nói chung và máy tính nói riêng.&lt;/p&gt;

&lt;p&gt;Cách đây vài năm, đối với phần đông chúng ta – những người không nghiên cứu chuyên sâu về AI sẽ cho rằng AI là một phương thức để nhân bản con người bằng máy móc và được ứng dụng trong chế tạo robot. Tuy nhiên AI hiện tại không phải chỉ là những con robot mà nó có thể biểu hiện dưới bất cứ hình dạng nào, thậm chí vô hình vô dạng, nhằm cung cấp lời giải cho các vấn đề của cuộc sống thực tế trên hầu hết các lĩnh vực, chẳng hạn như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trong lĩnh vực chăm sóc sức khỏe&lt;/strong&gt;: AI góp phần cải thiện tình trạng sức khỏe bệnh nhân, và giúp giảm chi phí điều trị. Một trong những hệ thống công nghệ chăm sóc sức khỏe tốt nhất phải kể đến là IBM Watson, được mệnh danh là “Bác sĩ biết tuốt” khi mà hệ thống này có khả năng hiểu được các ngôn ngữ tự nhiên và có khả năng phản hồi các câu hỏi được yêu cầu hoặc cho phép bệnh nhân tra cứu thông tin về tinh hình sức khoẻ của mình. IBM Watson có thể lướt duyệt cùng lúc hàng triệu hồ sơ bệnh án để cung cấp cho các bác sĩ những lựa chọn điều trị dựa trên bằng chứng chỉ trong vòng vài giây nhờ khả năng tổng hợp dữ liệu khổng lồ và tốc độ xử lý mạnh mẽ. “Bác sĩ biết tuốt” khai thác dữ liệu bệnh nhân và các nguồn dữ liệu sẵn có khác nhằm tạo ra giả thuyết và từ đó xậy dựng một lược đồ điểm tin cậy giúp “Bác sĩ thật” đưa ra quyết định điều trị cuối cùng. Ngoài ra, ứng dụng AI nổi bậc khác trong lĩnh vực này cần phải kể đến là chatbot - chương trình máy tính trực tuyến để trả lời các câu hỏi và hỗ trợ khách hàng, sắp xếp các cuộc hẹn hoặc trợ giúp bệnh nhân thông qua quá trình thanh toán và các trợ lý y tế ảo cung cấp phản hồi y tế cơ bản.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trong lĩnh vực kinh doanh&lt;/strong&gt;: Các tác vụ mà con người thực hiện lặp đi lặp lại giờ đây đã được tự động hoá quy trình bằng robot. Các thuật toán Machine Learning được tích hợp trên các nền tảng phân tích và CRM (Customer Relationship Management - quản lý quan hệ khách hàng) để khám phá các thông tin về cách phục vụ khách hàng tốt hơn. Chatbots được tích hợp trên các trang web nhằm cung cấp dịch vụ ngay lập tức cho khách hàng. Một số hệ thống trợ lý ảo nổi tiếng giúp sắp xếp, nhắc cuộc họp, tìm kiếm thông tin như Google Assistant, Alexa, Siri. Hiện nay các hệ thống này đã bắt đầu được tích hợp vào trong các thiết bị gia dụng như máy giặt, tủ lạnh, lò vi sóng, … giúp người sử dụng có thể điều khiển thiết bị bằng câu lệnh thoại.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trong lĩnh vực giáo dục&lt;/strong&gt;: Công nghệ thực tế ảo làm thay đổi cách dạy và học. Sinh viên có thể đeo kính VR và có cảm giác như đang ngồi trong lớp nghe giảng bài hay nhập vai để chứng kiến những trận đánh giả lập, ngắm nhìn di tích, điều này giúp mang lại cảm xúc và ghi nhớ sâu sắc nội dung học. Hoặc khi đào tạo nghề phi công, học viên đeo kính sẽ thấy phía trước là cabin và học lái máy bay như thật để thực hành giúp giảm thiểu rủi ro trong quá trình bay thật.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trong lĩnh vực tài chính&lt;/strong&gt;: AI áp dụng cho các ứng dụng tài chính cá nhân như Mint hay Turbo Tax giúp tăng cường các định chế tài chính.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trong lĩnh vực pháp luật&lt;/strong&gt;: Quá trình khám phá, chọn lọc thông qua các tài liệu trong luật pháp thường áp đảo đối với con người. Tự động hóa quá trình này giúp tiết kiệm thời gian và quá trình làm việc hiệu quả hơn. Các trợ lý ảo giúp trả lời các câu hỏi đã được lập trình sẵn.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trong lĩnh vực sản xuất&lt;/strong&gt;: Đây là lĩnh vực đi đầu trong việc kết hợp robot vào luồng công việc. Robot công nghiệp được sử dụng để thực hiện các nhiệm vụ đơn lẻ và đã được tách ra khỏi con người. Xe tự động lái Tesla là một ứng dụng điển hình trong lĩnh vực này.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trong lĩnh vực bảo mật thông tin&lt;/strong&gt;: rất nhiều hệ thống nhận diện và bảo mật thông minh được xây dựng, phải kể đến như FaceID - bảo mật thông qua nhận diện khuôn mặt của Apple, Facebook với khả nhận diện khuôn mặt để gợi ý tag. Bên cạnh các nước phương Tây thì Trung Quốc hiện đang là quốc gia đi đầu trong việc sử dụng AI để nhận diện và quản lý công dân.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Từ những ứng dụng trên ta có thể thấy rằng nói đến AI là nói về não bộ chứ không phải là nói về một cơ thể, là phần mềm chứ không phải là phần cứng.&lt;/p&gt;

&lt;h2 id=&#34;dữ-liệu-lớn&#34;&gt;Dữ liệu lớn&lt;/h2&gt;

&lt;p&gt;Một cách tổng quát thì dữ liệu là thông tin dưới dạng ký hiệu, chữ viết, chữ số, hình ảnh, âm thanh hoặc dạng tương tự.
Từ thế kỷ thứ 3 trước CN, Thư viện Alexandria được coi là nơi chứa đựng toàn bộ kiến thức của loài người. Ngày nay, tổng lượng dữ liệu trên toàn thế giới đủ để chia đều cho mỗi đầu người một lượng nhiều gấp 320 lần lượng dữ liệu mà các sử gia tin rằng Thư viện Alexandria từng lưu trữ – ước tính vào khoảng 120 exabyte. Các nhà thống kê cho rằng, nếu tất cả những dữ liệu này được ghi vào đĩa CD và xếp chồng chúng lên nhau thì sẽ có tới 5 chồng đĩa mà mỗi chồng đều có độ cao bằng khoảng cách từ Trái Đất đến Mặt Trăng.&lt;/p&gt;

&lt;p&gt;Sự bùng nổ dữ liệu này chỉ mới xuất hiện gần đây. Cách đây không lâu, vào năm 2000, chỉ một phần tư lượng dữ liệu lưu trữ trên toàn thế giới ở dạng kỹ thuật số, ba phần tư còn lại được người ta lưu trên giấy tờ, phim, và các phương tiện analog khác. Nhưng do lượng dữ liệu kỹ thuật số bùng nổ quá nhanh – cứ 3 năm lại tăng gấp đôi, làm cho tỉ lệ này nhanh chóng đảo ngược. Hiện nay, chỉ dưới 2% tổng lượng dữ liệu chưa được chuyển sang lưu trữ ở dạng kỹ thuật số.&lt;/p&gt;

&lt;p&gt;Dưới đây là một vài ví dụ nhỏ minh hoạ cho sự dùng nổ của dữ liệu hiện nay:&lt;/p&gt;

&lt;p&gt;Theo Forbes, lượng dữ liệu mà người dùng tạo ra mỗi ngày là 2.5 tỷ tỷ bytes, một con số rất đáng kinh ngạc và dự đoán con số này sẽ tiếp tục bùng nổ nữa cùng với sự phát triển của Internet vạn vật (IoT – Internet of thing), khi mà hệ thống các thiết bị thông minh được kết nối và tương tác với nhau cũng như tương tác với người dùng, đồng thời thu thập dữ liệu. Dự báo có khoảng 200 tỷ thiết bị như thế vào năm 2020. Giả sử chỉ xét đến thiết bị tìm kiếm bằng giọng nói, hiện tại:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Có 33 triệu thiết bị qua giọng nói đang lưu thông.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;8 triệu người dùng điều khiển giọng nói mỗi tháng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Các câu lệnh tìm kiếm bằng giọng nói trên Google trong năm 2016 tăng 35 lần so với năm 2008.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Theo thống kê, hiện nay có hơn 7 tỷ người sử dụng internet. Trung bình Google xử lý hơn 40.000 tìm kiếm mỗi giây (tức khoảng 3.5 tỷ tìm kiếm mỗi ngày, nếu tính cả những cổ máy tìm kiếm khác ngoại trừ Google thì con số này lên tới 5 tỷ lượt/ngày, 100 tỷ lượt/tháng) và những con số này sẽ tiếp tục tăng lên theo từng giây.&lt;/p&gt;

&lt;p&gt;Rất đông người yêu thích các phương tiện truyền thông xã hội và dĩ nhiên việc sử dụng chúng cũng sẽ tạo ra dữ liệu. Theo báo cáo Data Never Sleép 5.0 của Domo, trên các phương tiện truyền thông cứ mỗi một phút sẽ có (nguồn &lt;a href=&#34;http://www.internetlivestats.com/google-search-statistics/):&#34;&gt;http://www.internetlivestats.com/google-search-statistics/):&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;527.760 bức ảnh được chia sẻ bởi người sử dụng Snapchat .&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;456.000 tweet được gửi lên Twitter.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;46.740 bức ảnh được đăng bởi người dùng Instagram.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hơn 120 người có công việc ổn định tham gia LinkedIn.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Với khoảng 2 tỷ người dùng, Facebook vẫn là mạng xã hội lớn nhất hành tinh và dưới đây là các số liệu liên quan đến Facebook (nguồn  &lt;a href=&#34;http://newsroom.fb.com/company-info/):&#34;&gt;http://newsroom.fb.com/company-info/):&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hơn 900 triệu người thật sự sử dụng Facebook mỗi ngày, 82.8% trong số đó ở ngoài Mỹ và Canada.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;307 triệu / 2 tỷ là người Châu Âu.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cứ mỗi giây lại có 5 tài khoản mới được tạo ra.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;510.000 bình luận được đăng tải và 293.000 trạng thái được cập nhật mỗi phút.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hơn 300 triệu bức ảnh được tải lên mỗi ngày.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;15.000 ảnh GIF được gửi thông qua Facebook Messenger.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cũng thuộc sở hữu của Facebook, Instagram cũng có những con số ấn tượng:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;600 triệu người dùng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;400 triệu người hoạt động mỗi ngày.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;100 triệu người sử dụng tính năng Stories mỗi ngày.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Liên quan đến số lượng người dùng và dữ liệu chúng ta không thể không nhắc đến Youtube khi mà cứ mỗi một phút sẽ có khoảng 300 giờ  video được đăng tải trên Youtube (nguồn &lt;a href=&#34;https://www.youtube.com/yt/about/press/&#34;&gt;https://www.youtube.com/yt/about/press/&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Trong thời đại công nghệ, việc thông qua các trang web hẹn hò để tìm nửa còn lại không còn là điều xa lạ. Với hơn 20 tỷ lượt kết đôi, Tinder xứng đáng là nhịp cầu công nghệ thành công bậc nhất hiện tại. Cứ mỗi phút trôi qua Tinder có khoảng 990.000 lượt vuốt và hơn 26 triệu lượt hẹn hò mỗi ngày.&lt;/p&gt;

&lt;p&gt;Ngoài việc liên kết, trao đổi với nhau qua mạng xã hội, trong công việc mọi người thường sử dụng email, skype để thư từ, liên lạc. Tính đến năm 2019 có khoảng 9 tỷ người sử dụng email và dưới đây là một vài con số thống kê các sự kiện xảy ra trong một phút:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Người dùng gửi đi 16 triệu văn bản.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;156 triệu email được gửi đi với khoảng 16 triệu văn bản.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;103.447.520 thư rác được gửi đi.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;154.200 cuộc gọi Skype.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Không còn quá khó khăn trong việc lưu giữ các khoảnh khắc, ngày nay khi mà bất cứ ai cũng có thể sở hữu một chiếc điện thoại thông minh (smartphone) và ai cũng là nhiếp ảnh gia, cứ như thế có hàng nghìn tỷ bức ảnh được cho ra đời và lưu trữ trên điện thoại.&lt;/p&gt;

&lt;p&gt;Thông qua những ví dụ vừa nêu có thể chúng ta sẽ nghĩ rằng dữ liệu lớn thuần tuý chỉ là vấn đề về kích cỡ, và nếu điều này là đúng thì dữ liệu bao nhiêu được cho là “lớn”?&lt;/p&gt;

&lt;p&gt;Để trả lời câu hỏi này ta quay lại một chút về lịch sử của thuật ngữ “Big Data”. Không giống với AI và ML, Big Data không phải là một ngành khoa học chính thống mà chỉ là một thuật ngữ truyền thông mới xuất hiện trong vài năm trở lại đây. Nó không khác gì thuật ngữ “kỷ nguyên phần mềm” hay “cách mạng công nghiệp”. Mặc dù thuật ngữ này mới xuất hiện nhưng khối lượng dữ liệu tích tụ kể từ khi mạng Internet xuất hiện vào cuối thế kỷ trước cũng không phải là nhỏ từ ví dụ về thư viện Alexandria. Vậy thì câu hỏi đặt ra là tại sao với khối lượng khổng lồ như thế mà thời đó vẫn không gọi là Big Data? Câu trả lời là mặc dù được bao quanh bởi dữ liệu khổng lồ nhưng ở thời điểm đó con người không biết làm gì với chúng ngoài lưu trữ và sao chép. Cho đến khi các nhà khoa học nhận ra rằng trong đống dữ liệu này đang ẩn chứa một khối lượng tri thức khổng lồ. Những tri thức ấy có thể giúp ta hiểu thêm về con người và xã hội. Chẳng hạn như từ danh sách các bộ phim yêu thích của một cá nhân, chúng ta có thể rút ra được sở thích xem phem của người đó và gợi ý những bộ phim cùng thể loại. Hoặc từ danh sách tìm kiếm của cộng đồng mạng chúng ta sẽ biết được vấn đề nóng hổi nhất đang được quan tâm và sẽ tập trung đăng tải nhiều tin tức hơn về vấn đề đó, …&lt;/p&gt;

&lt;p&gt;Như vậy, &lt;em&gt;bùng nổ thông tin không phải là lý do duy nhất dẫn đến sự ra đời của cụm từ Big Data mà Big Data chỉ thực sự bắt đầu khi chúng ta hiểu được giá trị của thông tin ẩn chứa trong dữ liệu và có đủ tài nguyên cũng như công nghệ để có thể khai tác chúng trên quy mô lớn&lt;/em&gt;. Và không có gì ngạc nhiên khi Máy học chính là thành phần mấu chốt của công nghệ đó.&lt;/p&gt;

&lt;h2 id=&#34;máy-học-và-mối-quan-hệ-với-trí-tuệ-nhân-tạo-cùng-dữ-liệu-lớn&#34;&gt;Máy học và mối quan hệ với Trí tuệ nhân tạo cùng Dữ liệu lớn&lt;/h2&gt;

&lt;p&gt;Để máy tính có khả năng suy nghĩ và trí tuệ như con người thì đòi hỏi máy tính phải có khả năng “học” mà không cần phải lập trình để thực hiện các tác vụ cụ thể đó. Về phía các nhà nghiên cứu AI, họ muốn xem thử liệu máy tính có thể học dữ liệu như thế nào? Từ đó thuật ngữ Máy học hay Học máy (ML – Machine Learning) được hình thành. Mặc dù không có nhiều định nghĩa như AI nhưng ML lại có 2 định nghĩa khá tường minh như sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Máy học là ngành học cung cấp cho máy tính khả năng học hỏi mà không cần được lập trình một cách rõ ràng (Arthur Samuel, 1959).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Theo Giáo sư Tom Mitchell – Carnegie Mellon University: Máy học là 1 chương trình máy tính được nói là học hỏi từ kinh nghiệm E từ các tác vụ T và với độ đo hiệu suất P nếu hiệu suất của nó áp dụng trên tác vụ T và được đo lường bởi độ đo P tăng từ kinh nghiệm E.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Một vài ví dụ minh hoạ cho định nghĩa của Tom Mitchell:&lt;/p&gt;

&lt;p&gt;•   Ví dụ 1: Giả sử như ta muốn máy tính xác định một tin nhắn có phải là SPAM hay không thì:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tác vụ T: Xác định 1 tin nhắn có phải SPAM hay không?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kinh nghiệm E: Xem lại những tin nhắn được đánh dấu là SPAM xem có những đặc tính gì để có thể xác định nó là SPAM.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Độ đo P: Là phần trăm số tin nhắn SPAM được phân loại đúng.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;•   Ví dụ 2: Chương trình nhận dạng chữ số viết tay (bao gồm các chữ số từ 0 đến 9)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tác vụ T: nhận dạng được ảnh chứa ký tự số.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kinh nghiệm E: Đặc trưng để phân loại ký tự số từ tập dữ liệu số cho trước.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Độ đo P: Độ chính xác của quá trình nhận dạng.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;mối-quan-hệ-giữa-ml-với-ai-và-big-data&#34;&gt;Mối quan hệ giữa ML với AI và Big Data&lt;/h2&gt;

&lt;p&gt;Trong phần 1 và phần 2 chúng ta luôn thấy sự xuất hiện của ML, đây là lý do vì sao mình không tách riêng mối quan hệ giữa các khái niệm này ra một phần riêng mà để chung trong nội dung của ML. Vậy thì mối liên hệ đó là gì?&lt;/p&gt;

&lt;p&gt;Một cách hàn lâm thì AI là ngành khoa học được sinh ra với mục tiêu là làm cho máy tính có được trí thông minh như con người. Mục tiêu này vẫn khá mơ hồ vì không phải ai cũng đồng ý với một định nghĩa thống nhất về trí thông minh. Các nhà khoa học phải định nghĩa một số mục tiêu cụ thể hơn, một trong số đó là việc làm cho máy tính lừa được Turing Test. Turing Test được tạo ra bởi Alan Turing (1912 – 1954), người được xem là cha để của ngành khoa học máy tính hiện đại, nhằm phân biệt xem người đối diện có phả là người hay không.&lt;/p&gt;

&lt;p&gt;Như vậy, AI thể hiện một của mục tiêu con người, trong khi ML là một phương tiện được kỳ vọng sẽ giúp con người đạt được mục tiêu đó. Và trên thực tế thì ML đã mang nhân loại đi rất xa trên quãng đường chinh phục AI. Dù có mối quan hệ chặc chẽ với nhau nhưng chúng không hẳn là trùng khớp vì môt bên là mục tiêu (AI), một bên là phương tiện (ML). Chinh phục AI mặc dù vẫn là mục đích tối thượng của ML, nhưng hiện tại ML tập trung vào những mục tiêu ngắn hạn hơn như làm cho máy tính có khả năng nhận thức cơ bản của con người như nghe, nhìn, hiểu được ngôn ngữ, giải toán, lập trình, …, các khả năng này ứng với các lĩnh vực cụ thể trong AI như:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Thị giác máy tính (computer vision): mục tiêu của lĩnh vực này là làm cho máy tính có thể nhìn như con người. Những ứng dụng quan trọng có thể kể đến trong lĩnh vực này như là nhận dạng chữ/ chứ số viết tay, nhận dạng khuôn mặt, dáng đi, cử chỉ, phân loại loài hoa, nhãn hiệu, phát hiện đồ vât, …. Từ tập hình ảnh ban đầu, các thuật toán ML sẽ tiến hành xử lý, phân tích để rút ra các đặc trưng chính giúp nhận dạng đối tượng hoặc phân biệt các đối tượng với nhau.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Xử lý Ngôn ngữ tự nhiên (Natural Language Processing – NLP): Mục tiêu là giúp cho máy tính có thể hiểu như con người. Dịch máy là một trong những ứng dụng điển hình của NLP, dịch nội dung của một đoạn văn bản từ ngôn ngữ này sang ngôn ngữ khác (Google Translate). Xuất phát từ “Từ điển” hoặc tập các cặp câu song ngữ, tập luật ngữ pháp của mỗi ngôn ngữ được tạo bởi người có chuyên môn về những ngôn ngữ đó, các thuật toán máy học sẽ tiến hành phân tích để tách câu, tách từ, xác định từ loại, phân tích cú pháp để từ đó lấy ra ngữ nghĩa phù hợp rồi ghép lại với nhau và cho ra nội dung ở ngôn ngữ tương ứng. Ngoài ra, tóm tắt văn bản dựa vào các từ khoá của từng lĩnh vực cũng là một bài toán ML rất được quan tâm trong vài năm trở lại đây, khi mà mỗi ngày lượng tin tức cần phải đọc là quá nhiều.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Xử lý tiếng nói (Speech Language Processing): nhằm làm cho máy tính có thể nghe được như người. Tổng hộp tiếng nói (text to speech) để đọc sách cho người khiếm thị, tạo sub cho các video (speech to text) để hỗ trợ cho người khiếm thính hoặc hỗ trợ cho việc học ngôn ngữ; nhận dạng giọng nói (speech recognition) giúp phát hiện tội phạm là một số ứng dụng điển hình trong lĩnh vực này.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thay vì cố gắng “dạy” máy tính cách làm một việc gì đó, chẳng hạn như lái xe hơi, điều mà các chuyên gia AI cần làm là cung cấp “đủ” dữ liệu cho một máy tính để nó có thể tính ra xác suất của tất cả mọi thứ mà người ta muốn tính toán, ví như xác suất người đi đường gặp đèn giao thông màu xanh, màu đỏ, màu vàng, … thì chuẩn xác hơn.&lt;/p&gt;

&lt;p&gt;Do đó, nhiệm vụ thực sự của ML trong AI là “học” mà thực chất của việc học này là rút trích thông tin hữu ích cho từng bài toán trong “tập dữ liệu” cho trước. Lúc này mối quan hệ giữa ML và Big Data sẽ được bộc lộ, đó là nếu khối lượng dữ liệu của Big Data càng gia tăng thì ML sẽ phát triển hơn, có khả năng rút trích được nhiều thông tin giá trị hơn hay dự đoán chính xác hơn, ngược lại thì giá trị của Big Data phụ thuộc vào khả năng khai thác tri thức từ dữ liệu của ML, vì nó sẽ thực sự là Big Data khi khối lượng dữ liệu đó mang lại thông tin hữu ích.&lt;/p&gt;

&lt;p&gt;Việc sử dụng những khối lượng thông tin theo cách này đòi hỏi chúng ta phải có sự thay đổi trong cách tiếp cận dữ liệu. Một là thu thập và sử dụng thật nhiều dữ liệu thay vì chấp nhận lấy những mẫu thống kê với số lượng nhỏ như các nhà thống kê vẫn làm từ hơn một thế kỷ nay. Hai là không nhất thiết phải kén chọn sàng lọc ra dữ liệu sạch, vì kinh nghiệm thực tiễn cho thấy rằng một chút sai lệch trong thông tin vẫn có thể chấp nhận được, và việc sử dụng một lượng khổng lồ những dữ liệu ô hợp đem lại nhiều ích lợi hơn là dữ liệu tuy chính xác nhưng dung lượng quá ít. Ba là trong nhiều trường hợp, chúng ta không nhất thiết phải cố tìm ra nguyên nhân đằng sau các hiện tượng.Ví dụ, không cần phải cố tìm hiểu chính xác vì sao một cỗ máy bị hỏng, thay vào đó các nhà nghiên cứu có thể thu thập và phân tích thật nhiều dữ liệu về chúng cùng tất cả mọi thứ liên quan, từ đó rút ra quy luật làm cơ sở dự đoán các sự vật, sự việc trong tương lai.&lt;/p&gt;

&lt;p&gt;Dưới đây là một số tài liệu mình đã sử dụng để tham khảo trong qua trình viết bài:&lt;/p&gt;

&lt;p&gt;Introduction to Machine Learning of Alex Smola and S.V.N. Vishwanathan.&lt;/p&gt;

&lt;p&gt;Artificial Intelligence (third edition) of The McGraw-Hill Companies, write by Elaine Rich, Kevin Knight and Shivashankar B Nair.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://i4iam.files.wordpress.com/2013/08/artificial-intelligence-by-rich-and-knight.pdf&#34;&gt;https://i4iam.files.wordpress.com/2013/08/artificial-intelligence-by-rich-and-knight.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_intelligence&#34;&gt;https://en.wikipedia.org/wiki/Artificial_intelligence&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://searchenterpriseai.techtarget.com/definition/AI-Artificial-Intelligence&#34;&gt;https://searchenterpriseai.techtarget.com/definition/AI-Artificial-Intelligence&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://vienthongke.vn/tin-tuc/43-tin-tuc/2176-thoi-dai-cua-du-lieu-lon-big-data&#34;&gt;http://vienthongke.vn/tin-tuc/43-tin-tuc/2176-thoi-dai-cua-du-lieu-lon-big-data&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu Mask R-CNN và ví dụ phân vùng quả bóng bay sử dụng deep learning</title>
      <link>/blog/2019-03-25-mask-rcnn-balloon/</link>
      <pubDate>Mon, 25 Mar 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-03-25-mask-rcnn-balloon/</guid>
      <description>

&lt;h2 id=&#34;bắt-đầu&#34;&gt;Bắt đầu&lt;/h2&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ download tập dataset balloon tại &lt;a href=&#34;https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip&#34;&gt;https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip&lt;/a&gt;, giải nén và bỏ trong thư mục datasets. Tiếp đó, các bạn donwload file balloon.py và visualize.py về. File đầu tiên hỗ trợ chúng ta đọc dữ liệu của dataset balloon và file thứ hai hỗ trợ visualize hình ảnh một cách trực quan. Cả hai file mình đều lấy mã nguồn của Matterport trên &lt;a href=&#34;https://github.com/matterport/Mask_RCNN/&#34;&gt;https://github.com/matterport/Mask_RCNN/&lt;/a&gt; Tiến hành import các thư viện cần thiết về.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import sys
import itertools
import math
import logging
import json
import re
import random
from collections import OrderedDict
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.lines as lines
from matplotlib.patches import Polygon


import balloon
import utils
import visualize

config = balloon.BalloonConfig()
BALLOON_DIR = &amp;quot;datasets/balloon&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thông tin của tập train bao gồm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = balloon.BalloonDataset()
dataset.load_balloon(BALLOON_DIR, &amp;quot;train&amp;quot;)

# Must call before using the dataset
dataset.prepare()

print(&amp;quot;Image Count: {}&amp;quot;.format(len(dataset.image_ids)))
print(&amp;quot;Class Count: {}&amp;quot;.format(dataset.num_classes))
for i, info in enumerate(dataset.class_info):
    print(&amp;quot;{:3}. {:50}&amp;quot;.format(i, info[&#39;name&#39;]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image Count: 61
Class Count: 2
  0. BG
  1. balloon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vậy là có tổng cộng 61 hình train. Dữ liệu được đánh làm 2 nhãn, một nhãn là background, một nhãn là balloon.&lt;/p&gt;

&lt;h2 id=&#34;visualize-dữ-liệu&#34;&gt;Visualize dữ liệu&lt;/h2&gt;

&lt;p&gt;Chúng ta sẽ load một vài hình lên xem người ta đã mask dữ liệu như thế nào. Ở đây, với mỗi hình ảnh, mình sẽ load 1 hình gốc và 4 hình của 4 quả bóng tương ứng trong hình, nếu trong hình có nhiều hơn 4 quả bóng thì chỉ vẽ 4 quả bóng đầu tiên&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

n_col = 5

# Load and display random samples
fig, axs = plt.subplots(nrows=4, ncols=n_col, figsize=(9.3, 6),subplot_kw={&#39;xticks&#39;: [], &#39;yticks&#39;: []})
fig.subplots_adjust(left=0.03, right=0.97, hspace=0.3, wspace=0.05)
image_ids = np.random.choice(dataset.image_ids, 4)
# for image_id in image_ids:
# for ax, image_id in zip(axs.flat, image_ids):

for index in range(0,4):
    image_id = image_ids[index]

    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    print(mask.shape)
    print(len(class_ids))

    axs.flat[index*n_col].imshow(image)
    axs.flat[index*n_col].set_title(&#39;img&#39;)

    for sub_index in range(0,len(class_ids)):
        if sub_index &amp;gt;= n_col:
            break
        axs.flat[index*n_col +1 + sub_index].imshow(mask[:,:,sub_index])
        axs.flat[index*n_col + 1+sub_index].set_title(str(dataset.class_names[class_ids[sub_index]]))


plt.tight_layout()
plt.show()


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mask-rnn-1.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Các bạn có thể sử dụng hàm display_top_masks của tác giả Mask R-CNN để xem thử, hàm của họ hơi khác của mình một chút.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
image_ids = np.random.choice(dataset.image_ids, 4)
for image_id in image_ids:
    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-image2.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;bounding-boxes&#34;&gt;Bounding Boxes&lt;/h2&gt;

&lt;p&gt;Chúng ta có 2 cách để lấy Bounding Boxes của các hình. Một là lấy trực tiếp từ tập dataset (đối với những dataset có lưu bounding box), hai là rút trích bounding box từ các toạ độ mask. Chúng ta nên thực hiện cách hai, lý do là chúng ta sẽ dùng các kỹ thuật Data Generator để sinh nhiều ảnh hơn cung cấp cho thuật toán train. Lúc này, việc tính lại bounding box sẽ dễ dàng hơn.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Load random image and mask.
image_id = random.choice(dataset.image_ids)
image = dataset.load_image(image_id)
mask, class_ids = dataset.load_mask(image_id)

# Compute Bounding box
bbox = utils.extract_bboxes(mask)

# Display image and additional stats
print(&amp;quot;image_id &amp;quot;, image_id, dataset.image_reference(image_id))

# Display image and instances
visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mask-rcnn-3.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;resize-images&#34;&gt;Resize Images&lt;/h2&gt;

&lt;p&gt;Các ảnh trong tập train có các kích thước khác nhau. Các bạn có thể xem các hình ở trên, có ảnh có kích thước này, có ảnh có kích thước kia. Chúng ta sẽ resize chúng về cùng một kích thước (ví dụ 1024x1024) để làm đầu vào cho tập huấn luyện. Và chúng ta sẽ sử dụng zero padding để lấp đầy những khoảng trống của những ảnh không đủ kích thước.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;


# Load random image and mask.
image_id = np.random.choice(dataset.image_ids, 1)[0]
image = dataset.load_image(image_id)
mask, class_ids = dataset.load_mask(image_id)
original_shape = image.shape
# Resize
image, window, scale, padding, _ = utils.resize_image(
    image, 
    min_dim=config.IMAGE_MIN_DIM, 
    max_dim=config.IMAGE_MAX_DIM,
    mode=config.IMAGE_RESIZE_MODE)
mask = utils.resize_mask(mask, scale, padding)
# Compute Bounding box
bbox = utils.extract_bboxes(mask)

# Display image and additional stats
print(&amp;quot;image_id: &amp;quot;, image_id, dataset.image_reference(image_id))
print(&amp;quot;Original shape: &amp;quot;, original_shape)
print(&amp;quot;Resize shape: &amp;quot;, image.shape)
# Display image and instances
visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image_id:  9 datasets/balloon\train\15290896925_884ab33fd3_k.jpg
Original shape:  (1356, 2048, 3)
Resize shape:  (1024, 1024, 3)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-image4.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Lưu ý một điều là ở đây, mình sử dụng random image, nên nếu các bạn chạy lại câu lệnh như mình thì kết quả ra phần nhiều sẽ khác mình. Tuy nhiên, Resize shape luôn là (1024, 1024, 3).&lt;/p&gt;

&lt;h2 id=&#34;mini-masks&#34;&gt;Mini Masks&lt;/h2&gt;

&lt;p&gt;Một vấn đề khá nghiêm trọng ở đây là chúng ta cần khá nhiều bộ nhớ để lưu các masks. Numpy sử dụng 1 byte để lưu 1 giá trị bit. Do đó, với kích thước ảnh là 1024x1024, chúng ta cần 1MB bộ nhớ ram để lưu trữ. Nếu chúng ta có tập dataset tầm 1000 bức ảnh thì cần đến 1GB bộ nhớ, khá là lớn. Ngoài việc tốn bộ nhớ lữu trữ, chúng còn làm chậm tốc độ huấn luyện mô hình nữa.&lt;/p&gt;

&lt;p&gt;Để cải tiến, chúng ta có thể sử dụng một trong hai cách sau:
- Cách thứ nhất: Thay vì lưu toàn bộ mask của toàn bức ảnh, chúng ta chỉ lưu những pixel của mask trong bounding box. Với việc sử dụng cách này, chúng ta sẽ tiết kiệm kha khá bộ nhớ chính.
- Cách thứ hai: Chúng ta có thể resize mask về một kích thước chuẩn nào đó, ví dụ 48x48 pixel. Với những mask có kích thước lớn hơn 48x48, chúng sẽ bị mất thông tin.&lt;/p&gt;

&lt;p&gt;Mình không thích cách thứ hai cho lắm. Tuy nhiên, theo lý giải của nhóm tác giả Mask R-CNN, thì hầu hết việc gán các đường biên (object annotations) thường không chính xác cho lắm (thừa hoặc thiếu một vài chỗ), cho nên, việc mất mát thông tin với lượng nhỏ này hầu như là không đáng kể.&lt;/p&gt;

&lt;p&gt;Để đánh giá hiệu quả của hàm mask resizing, chúng ta sẽ chạy đoạn code bên dưới và xem ảnh kết quả. Đoạn code trên mình sử dụng 2 hàm compose_image_meta và load_image_gt của tác giả ở đường dẫn &lt;a href=&#34;https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py&#34;&gt;https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py&lt;/a&gt;. Mình có modify lại hàm load_image_gt một chút để hợp với ý mình hơn.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;############################################################
#  Data Formatting
############################################################

def compose_image_meta(image_id, original_image_shape, image_shape,
                       window, scale, active_class_ids):
    &amp;quot;&amp;quot;&amp;quot;Takes attributes of an image and puts them in one 1D array.
    image_id: An int ID of the image. Useful for debugging.
    original_image_shape: [H, W, C] before resizing or padding.
    image_shape: [H, W, C] after resizing and padding
    window: (y1, x1, y2, x2) in pixels. The area of the image where the real
            image is (excluding the padding)
    scale: The scaling factor applied to the original image (float32)
    active_class_ids: List of class_ids available in the dataset from which
        the image came. Useful if training on images from multiple datasets
        where not all classes are present in all datasets.
    &amp;quot;&amp;quot;&amp;quot;
    meta = np.array(
        [image_id] +                  # size=1
        list(original_image_shape) +  # size=3
        list(image_shape) +           # size=3
        list(window) +                # size=4 (y1, x1, y2, x2) in image cooredinates
        [scale] +                     # size=1
        list(active_class_ids)        # size=num_classes
    )
    return meta


def load_image_gt(dataset, config, image_id, augment=False, augmentation=None,
                  use_mini_mask=False):
    &amp;quot;&amp;quot;&amp;quot;Load and return ground truth data for an image (image, mask, bounding boxes).
    augment: (deprecated. Use augmentation instead). If true, apply random
        image augmentation. Currently, only horizontal flipping is offered.
    augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
        For example, passing imgaug.augmenters.Fliplr(0.5) flips images
        right/left 50% of the time.
    use_mini_mask: If False, returns full-size masks that are the same height
        and width as the original image. These can be big, for example
        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,
        224x224 and are generated by extracting the bounding box of the
        object and resizing it to MINI_MASK_SHAPE.
    Returns:
    image: [height, width, 3]
    shape: the original shape of the image before resizing and cropping.
    class_ids: [instance_count] Integer class IDs
    bbox: [instance_count, (y1, x1, y2, x2)]
    mask: [height, width, instance_count]. The height and width are those
        of the image unless use_mini_mask is True, in which case they are
        defined in MINI_MASK_SHAPE.
    &amp;quot;&amp;quot;&amp;quot;
    # Load image and mask
    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    original_shape = image.shape
    image, window, scale, padding, crop = utils.resize_image(
        image,
        min_dim=config.IMAGE_MIN_DIM,
        min_scale=config.IMAGE_MIN_SCALE,
        max_dim=config.IMAGE_MAX_DIM,
        mode=config.IMAGE_RESIZE_MODE)
    mask = utils.resize_mask(mask, scale, padding, crop)

    # Random horizontal flips.
    # TODO: will be removed in a future update in favor of augmentation
    if augment:
        logging.warning(&amp;quot;&#39;augment&#39; is deprecated. Use &#39;augmentation&#39; instead.&amp;quot;)
        if random.randint(0, 1):
            image = np.fliplr(image)
            mask = np.fliplr(mask)

    # Augmentation
    # This requires the imgaug lib (https://github.com/aleju/imgaug)
    if augmentation:
        import imgaug

        # Augmenters that are safe to apply to masks
        # Some, such as Affine, have settings that make them unsafe, so always
        # test your augmentation on masks
        MASK_AUGMENTERS = [&amp;quot;Sequential&amp;quot;, &amp;quot;SomeOf&amp;quot;, &amp;quot;OneOf&amp;quot;, &amp;quot;Sometimes&amp;quot;,
                           &amp;quot;Fliplr&amp;quot;, &amp;quot;Flipud&amp;quot;, &amp;quot;CropAndPad&amp;quot;,
                           &amp;quot;Affine&amp;quot;, &amp;quot;PiecewiseAffine&amp;quot;]

        def hook(images, augmenter, parents, default):
            &amp;quot;&amp;quot;&amp;quot;Determines which augmenters to apply to masks.&amp;quot;&amp;quot;&amp;quot;
            return augmenter.__class__.__name__ in MASK_AUGMENTERS

        # Store shapes before augmentation to compare
        image_shape = image.shape
        mask_shape = mask.shape
        # Make augmenters deterministic to apply similarly to images and masks
        det = augmentation.to_deterministic()
        image = det.augment_image(image)
        # Change mask to np.uint8 because imgaug doesn&#39;t support np.bool
        mask = det.augment_image(mask.astype(np.uint8),
                                 hooks=imgaug.HooksImages(activator=hook))
        # Verify that shapes didn&#39;t change
        assert image.shape == image_shape, &amp;quot;Augmentation shouldn&#39;t change image size&amp;quot;
        assert mask.shape == mask_shape, &amp;quot;Augmentation shouldn&#39;t change mask size&amp;quot;
        # Change mask back to bool
        mask = mask.astype(np.bool)

    # Note that some boxes might be all zeros if the corresponding mask got cropped out.
    # and here is to filter them out
    _idx = np.sum(mask, axis=(0, 1)) &amp;gt; 0
    mask = mask[:, :, _idx]
    class_ids = class_ids[_idx]
    # Bounding boxes. Note that some boxes might be all zeros
    # if the corresponding mask got cropped out.
    # bbox: [num_instances, (y1, x1, y2, x2)]
    bbox = utils.extract_bboxes(mask)

    # Active classes
    # Different datasets have different classes, so track the
    # classes supported in the dataset of this image.
    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)
    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][&amp;quot;source&amp;quot;]]
    active_class_ids[source_class_ids] = 1

    # Resize masks to smaller size to reduce memory usage
    if use_mini_mask:
        if USE_MINI_MASK_SHAPE:
            mask = utils.minimize_mask(bbox, mask, MINI_MASK_SHAPE)
        else:
            mask = utils.minimize_mask(bbox, mask, mask.shape[:2])

    # Image meta data
    image_meta = compose_image_meta(image_id, original_shape, image.shape,
                                    window, scale, active_class_ids)

    return image, image_meta, class_ids, bbox, mask


image_id = np.random.choice(dataset.image_ids, 1)[0]
image, image_meta, class_ids, bbox, mask = load_image_gt(
    dataset, config, image_id, use_mini_mask=False)


visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 5))])

image, image_meta, class_ids, bbox, mask = load_image_gt(
    dataset, config, image_id, use_mini_mask=True)


visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 5))])

USE_MINI_MASK_SHAPE = True

image, image_meta, class_ids, bbox, mask = load_image_gt(
    dataset, config, image_id, use_mini_mask=True)


visualize.display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 5))])

mask = utils.expand_mask(bbox, mask, image.shape)
visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-image6.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Với ảnh ở line 1 là ảnh gốc ban đầu và các full mask của bức ảnh, ảnh ở line 2 là chỉ lấy mask của bounding box, ảnh ở line 3 là lấy mask ở bounding box và scale ảnh (do scale ảnh nên ở line 3 các bạn sẽ thấy mask có hình răng cưa, khác với các mask line 2). Line 4 là ảnh ở line 3 được revert back lại hình gốc ban đầu. Các bạn có để ý thấy rằng nó sẽ bị răng cưa ở biên cạnh chứ không được smooth như ảnh gốc. Nếu chúng ta không làm object annotations kỹ, thì object cũng sẽ bị răng cưa như trên.&lt;/p&gt;

&lt;h2 id=&#34;anchors&#34;&gt;Anchors&lt;/h2&gt;

&lt;p&gt;Thứ tự của các anchor thật sự rất quan trọng. Trong quá trình train, thứ tự của các anchor như thế nào thì trong quá trình test, validation, prediction phải dùng y hệt vậy.&lt;/p&gt;

&lt;p&gt;Trong mạng FPN, các anchor phải được xắp xếp theo cách mà chúng ta có thể dễ dàng liên kết với giá trị output&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Xắp xếp các anchor theo thứ tự các lớp của pyramid. Tất cả các anchor của level đầu tiên, tiếp theo là các anchor của các lớp thứ hai, lớp thư ba&amp;hellip; Việc xắp xếp theo cách này sẽ giúp chúng ta dễ dàng phân tách các lớp anchor và dễ hiểu theo lẽ tự nhiên.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Trong mỗi level, xắp xếp các anchor trong mỗi level bằng thứ tự xử lý của các feature map. Thông thường, một convolution layer sẽ dịch chuyển trên feature map bắt đầu từ vị trí trái - trên (top - left) đi xuống phải dưới (từ trái qua phải, xuống hàng rồi lại từ trái qua phải).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Trên mỗi cell của feature map, chúng ta sẽ xắp xếp các anchor theo các ratios.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anchor Stride:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
backbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)
anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, 
                                          config.RPN_ANCHOR_RATIOS,
                                          backbone_shapes,
                                          config.BACKBONE_STRIDES, 
                                          config.RPN_ANCHOR_STRIDE)

# Print summary of anchors
num_levels = len(backbone_shapes)
anchors_per_cell = len(config.RPN_ANCHOR_RATIOS)
print(&amp;quot;Total anchors: &amp;quot;, anchors.shape[0])
print(&amp;quot;ANCHOR Scales: &amp;quot;, config.RPN_ANCHOR_SCALES)
print(&amp;quot;BACKBONE STRIDE: &amp;quot;, config.BACKBONE_STRIDES)
print(&amp;quot;ratios: &amp;quot;, config.RPN_ANCHOR_RATIOS)
print(&amp;quot;Anchors per Cell: &amp;quot;, anchors_per_cell)
# print(&amp;quot;Anchors stride: &amp;quot;, config.RPN_ANCHOR_STRIDE)
print(&amp;quot;Levels: &amp;quot;, num_levels)
anchors_per_level = []
for l in range(num_levels):
    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]
    print(&amp;quot;backbone_shapes in level &amp;quot;,l,&#39; &#39;,backbone_shapes[l][0],&#39;x&#39;,backbone_shapes[l][1])
    print(&amp;quot;num_cells in level &amp;quot;,l,&#39; &#39;,num_cells)
    anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2)
    print(&amp;quot;Anchors in Level {}: {}&amp;quot;.format(l, anchors_per_level[l]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Total anchors:  261888
ANCHOR Scales:  (32, 64, 128, 256, 512)
BACKBONE STRIDE:  [4, 8, 16, 32, 64]
ratios:  [0.5, 1, 2]
Anchors per Cell:  3
Levels:  5
backbone_shapes in level  0   256 x 256
num_cells in level  0   65536
Anchors in Level 0: 196608
backbone_shapes in level  1   128 x 128
num_cells in level  1   16384
Anchors in Level 1: 49152
backbone_shapes in level  2   64 x 64
num_cells in level  2   4096
Anchors in Level 2: 12288
backbone_shapes in level  3   32 x 32
num_cells in level  3   1024
Anchors in Level 3: 3072
backbone_shapes in level  4   16 x 16
num_cells in level  4   256
Anchors in Level 4: 768
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong kiến trức FPN, feature map tại một số layer đầu tiên là những feature map có độ phân giải lớn. Ví dụ, nếu bức ảnh đầu vào có kích thước là 1024x1024 pixel, và kích thước của mỗi anchor lớp đầu tiên là 32x32 pixel (giá trị đầu tiên của RPN_ANCHOR_SCALES (32, 64, 128, 256, 512)) và bước nhảy (STRIDE) của lớp đầu tiên là 4 (giá trị đầu tiên của BACKBONE_STRIDES ([4, 8, 16, 32, 64])). Từ những dữ kiện này, ta có thể suy ra được là sẽ sinh ra backbone cell có kích thước 256x256 pixel =&amp;gt; 256x256 = 65536 anchor. Với mỗi backbone cell, chúng ta thực hiện phép scale với 3 tỷ lệ khác nhau là [0.5, 1, 2], vậy chúng ta có tổng cộng là 65536x3 = 196608 anchor (xấp xỉ 200k anchor). Để ý một điều là kích thước của một anchor là 32x32 pixel, và bước nhảy là 4, cho nên chúng ta sẽ bị chống lấn (overlap) 28 pixel của anchor 1 và anchor 2 ngay sau nó.&lt;/p&gt;

&lt;p&gt;Một điều thú vị là, nếu ta tăng bước nhảy lên gấp 2 lần, ví dụ từ 4 pixel lấy một anchor lên 8 pixel lấy một anchor, thì số lượng anchor giảm đi đến 4 lần (196608 anchor ở level 0 so với 49152 anchor ở level 1).&lt;/p&gt;

&lt;p&gt;Thử vẽ tất cả các anchor của tất cả các level ở điểm giữa một bức ảnh bức kỳ lên, mỗi một level sẽ dùng một màu khác nhau, chúng ta được một hình như bên dưới.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Visualize anchors of one cell at the center of the feature map of a specific level

# Load and draw random image
image_id = np.random.choice(dataset.image_ids, 1)[0]
image, image_meta, _, _, _ = modellib.load_image_gt(dataset, config, image_id)
fig, ax = plt.subplots(1, figsize=(10, 10))
ax.imshow(image)
levels = len(backbone_shapes)

kn_color =np.array( [(255,0,0),(0,255,0),(0,0,255),(128,0,0),(0,128,0),(0,0,128)])/255.

for level in range(levels):
    # colors = visualize.random_colors(levels)
    colors = kn_color
    # Compute the index of the anchors at the center of the image
    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels
    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]
    print(&amp;quot;Level {}. Anchors: {:6}  Feature map Shape: {} &amp;quot;.format(level, level_anchors.shape[0], 
                                                                  backbone_shapes[level]))
    center_cell = backbone_shapes[level] // 2
    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])
    level_center = center_cell_index * anchors_per_cell 
    center_anchor = anchors_per_cell * (
        (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2) \
        + center_cell[1] / config.RPN_ANCHOR_STRIDE)
    level_center = int(center_anchor)

    # Draw anchors. Brightness show the order in the array, dark to bright.
    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):
        y1, x1, y2, x2 = rect
        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor=&#39;none&#39;,
                              edgecolor=np.array(colors[level]) / anchors_per_cell)
        print(i)
        ax.add_patch(p)


plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-image7.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn ảnh trên,các bạn phần nào đó mường tượng ra các anchor sẽ như thế nào rồi phải không.&lt;/p&gt;

&lt;h2 id=&#34;prediction&#34;&gt;Prediction&lt;/h2&gt;

&lt;p&gt;Để tiến hành detect vị trí quả bóng và mask của quả bóng, chúng ta download một ảnh small party nhỏ trên internet về và kiểm chứng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import os

import tensorflow as tf

import cv2

DEVICE = &amp;quot;/cpu:0&amp;quot; 
ROOT_DIR = os.path.abspath(&amp;quot;../../&amp;quot;)
MODEL_DIR = os.path.join(ROOT_DIR, &amp;quot;logs&amp;quot;)
# Create model in inference mode

class InferenceConfig(config.__class__):
    # Run detection on one image at a time
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

config = InferenceConfig()
config.display()

with tf.device(DEVICE):
    model = modellib.MaskRCNN(mode=&amp;quot;inference&amp;quot;, model_dir=MODEL_DIR,
                              config=config)


weights_path = &amp;quot;mask_rcnn_balloon.h5&amp;quot;

# Load weights
print(&amp;quot;Loading weights &amp;quot;, weights_path)
# model.load_weights(weights_path, by_name=True)

imgpath = &amp;quot;datasets\\balloon\\test\\t1.png&amp;quot;
# imgpath = &amp;quot;datasets/balloon/val/14898532020_ba6199dd22_k.jpg&amp;quot;

image = cv2.imread(imgpath)

image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)



ds_name = [&#39;BG&#39;, &#39;balloon&#39;]


results = model.detect([image], verbose=1)

def get_ax(rows=1, cols=1, size=16):
    &amp;quot;&amp;quot;&amp;quot;Return a Matplotlib Axes array to be used in
    all visualizations in the notebook. Provide a
    central point to control graph sizes.
    
    Adjust the size attribute to control how big to render images
    &amp;quot;&amp;quot;&amp;quot;
    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))
    return ax
# Display results
ax = get_ax(1)
r = results[0]
visualize.display_instances(image, r[&#39;rois&#39;], r[&#39;masks&#39;], r[&#39;class_ids&#39;], 
                            dataset.class_names, r[&#39;scores&#39;], ax=ax,
                            title=&amp;quot;Predictions&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-image8.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả nhận dạng khá chính xác phải không các bạn.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thêm dấu tiếng việt cho câu không dấu</title>
      <link>/blog/2019-03-16-vietnamese-accent/</link>
      <pubDate>Sat, 16 Mar 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-03-16-vietnamese-accent/</guid>
      <description>&lt;p&gt;Thêm dấu tiếng việt là một trong những bài toán khá hay trong xử lý  ngôn ngữ tự nhiên. Ở đây, mình đã tiến hành thu thập dữ liệu bài báo của nhiều nguồn khác nhau như zing.vn, vnexpress, kenh14.vn &amp;hellip; làm kho ngữ liệu và xây dựng mô hình.&lt;/p&gt;

&lt;p&gt;Để tiến hành thực nghiệm, mình sẽ lấy một số đoạn văn mẫu ở trang tin tức của thế giới di động (https.www.thegioididong.com) (mình không crawl nội dung tin tức ở trang này làm dữ liệu học).&lt;/p&gt;

&lt;p&gt;Ở bài viết link &lt;a href=&#34;https://www.thegioididong.com/tin-tuc/3-ngay-cuoi-tuan-mua-laptop-online-tang-them-pmh-den-400k-tra-gop-0--1151334&#34;&gt;https://www.thegioididong.com/tin-tuc/3-ngay-cuoi-tuan-mua-laptop-online-tang-them-pmh-den-400k-tra-gop-0--1151334&lt;/a&gt;, mình lấy đoạn mở đầu &amp;ldquo;Từ ngày &lt;sup&gt;15&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; đến &lt;sup&gt;17&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;, nhiều mẫu laptop tại Thế Giới Di Động sẽ được ưu đãi mạnh, tặng phiếu mua hàng đến 400 ngàn đồng, trả góp 0% và nhiều quà tặng hấp dẫn khác khi mua theo hình thức ONLINE. Nếu đang có nhu cầu mua laptop, bạn hãy nhanh chóng xem qua danh sách sản phẩm dưới đây nhé.&amp;rdquo;, bỏ dấu của câu đi, thì mình được câu&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Tu ngay &lt;sup&gt;15&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; den &lt;sup&gt;17&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;, nhieu mau laptop tai The Gioi Di Dong se duoc uu dai manh, tang phieu mua hang den 400 ngan dong, tra gop 0% va nhieu qua tang hap dan khac khi mua theo hinh thuc ONLINE. Neu dang co nhu cau mua laptop, ban hay nhanh chong xem qua danh sach san pham duoi day nhe.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Sử dụng mô hình mình đã huấn luyện, thu được kết quả như sau:&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Từ ngày &lt;sup&gt;15&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; đến &lt;sup&gt;17&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; m t m, nhiều mẫu laptoP tạI thế giỚi di động sẽ được ưu đãi mạnh, tang phiếu mua hàng đến 400 ngàn đồng, trả góp 0 r% và nhiều quà tặng hấp dẫn khác khi mua theo hìNH THỨc Onfine. nếu đang có nhu cầu mua laptop, bạn hãy nhanh chóng xem qua danh sách sản phẩm dưới&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Kết quả khá khả quan phải không các bạn, còn một số lỗi nhỏ ở phần nhận dạng ký tự hoa nữa. Mình sẽ fix lại ở các bài viết sau.&lt;/p&gt;

&lt;p&gt;Mình thí nghiệm tiếp với phần đầu bài viết &lt;a href=&#34;https://www.thegioididong.com/tin-tuc/apple-ban-ra-thi-truong-35-trieu-cap-tai-nghe-airpods-nam-2018-1155181&#34;&gt;https://www.thegioididong.com/tin-tuc/apple-ban-ra-thi-truong-35-trieu-cap-tai-nghe-airpods-nam-2018-1155181&lt;/a&gt;.
Đoạn &amp;ldquo;Hôm nay, báo cáo của Counterpoint Research cho thấy, trong năm 2018 Apple đã bán được khoảng 35 triệu cặp tai nghe không dây AirPods. Theo hãng phân tích này, AirPods hiện là tai nghe không dây phổ biến nhất.&amp;rdquo;, bỏ dấu tiếng việt là thu được &amp;ldquo;Hom nay, bao cao cua Counterpoint Research cho thay, trong nam 2018 Apple da ban duoc khoang 35 trieu cap tai nghe khong day AirPods. Theo hang phan tich nay, AirPods hien la tai nghe khong day pho bien nhat.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Kết quả của mô hình: &amp;ldquo;Hôm nay, bạo cáo của Coorteenria eEeeroa c ttt, trong năm 2018 apple đã bán được khoảng 35 triệu cặp tại nghe không đầy aitcoDs. theo Hàng phân tích này, airxoDs Hiện là tai nghe không dạy phổ biến nhất.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Mô hình của mình cho lặp 50 lần. Mình tiến hành thí nghiệm và publish mô hình ở lần lặp thứ 10.&lt;/p&gt;

&lt;p&gt;Mã nguồn file predict&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.models import load_model
model = load_model(&#39;a_best_weight.h5&#39;)

from collections import Counter

import numpy as np

import utils
import string
import re

alphabet = set(&#39;\x00 _&#39; + string.ascii_lowercase + string.digits + &#39;&#39;.join(utils.ACCENTED_TO_BASE_CHAR_MAP.keys()))

print(&amp;quot;alphabet&amp;quot;,alphabet)
codec = utils.CharacterCodec(alphabet, utils.MAXLEN)

def guess(ngram):
    text = &#39; &#39;.join(ngram)
    text += &#39;\x00&#39; * (utils.MAXLEN - len(text))
    if utils.INVERT:
        text = text[::-1]
    preds = model.predict_classes(np.array([codec.encode(text)]), verbose=0)
    rtext = codec.decode(preds[0], calc_argmax=False).strip(&#39;\x00&#39;)
    if len(rtext)&amp;gt;0:
        index = rtext.find(&#39;\x00&#39;)
        if index&amp;gt;-1:
            rtext = rtext[:index]
    return rtext


def add_accent(text):
    # lowercase the input text as we train the model on lowercase text only
    # but we keep the map of uppercase characters to restore cases in output
    is_uppercase_map = [c.isupper() for c in text]
    text = utils.remove_accent(text.lower())

    outputs = []
    words_or_symbols_list = re.findall(&#39;\w[\w ]*|\W+&#39;, text)

    # print(words_or_symbols_list)

    for words_or_symbols in words_or_symbols_list:
        if utils.is_words(words_or_symbols):
            outputs.append(_add_accent(words_or_symbols))
        else:
            outputs.append(words_or_symbols)
        # print(outputs)
    output_text = &#39;&#39;.join(outputs)

    # restore uppercase characters
    output_text = &#39;&#39;.join(c.upper() if is_upper else c
                            for c, is_upper in zip(output_text, is_uppercase_map))
    return output_text

def _add_accent(phrase):
    grams = list(utils.gen_ngram(phrase.lower(), n=utils.NGRAM, pad_words=utils.PAD_WORDS_INPUT))
    
    guessed_grams = list(guess(gram) for gram in grams)
    # print(&amp;quot;phrase&amp;quot;,phrase,&#39;grams&#39;,grams,&#39;guessed_grams&#39;,guessed_grams)
    candidates = [Counter() for _ in range(len(guessed_grams) + utils.NGRAM - 1)]
    for idx, gram in enumerate(guessed_grams):
        for wid, word in enumerate(re.split(&#39; +&#39;, gram)):
            candidates[idx + wid].update([word])
    output = &#39; &#39;.join(c.most_common(1)[0][0] for c in candidates if c)
    return output.strip(&#39;\x00 &#39;)



# print(add_accent(&#39;do,&#39;))
# print(add_accent(&#39;7.3 inch,&#39;))
# print(add_accent(&#39;Truoc do, tren san khau su kien SDC 2018, giam doc cao cap mang marketing san pham di dong cua Samsung, ong Justin Denison da cam tren tay nguyen mau cua thiet bi nay. Ve co ban, no chang khac gi mot chiec may tinh bang 7.3 inch, duoc cau thanh tu nhieu lop phu khac nhau nhu polyme, lop man chong soc, lop phan cuc voi do mong gan mot nua so voi the he truoc, lop kinh linh hoat va mot tam lung da nang co the bien thanh man hinh. Tat ca se duoc ket dinh bang mot loai keo cuc ben, cho phep chiec may nay co the gap lai hang tram ngan lan ma khong bi hu hong.&#39;))
# print(add_accent(&#39;man hinh. Tat ca se duoc ket dinh bang mot loai keo cuc ben, cho phep chiec may nay co the gap lai hang tram ngan lan ma khong bi hu hong.&#39;))
print(add_accent(&#39;Hom nay, bao cao cua Counterpoint Research cho thay, trong nam 2018 Apple da ban duoc khoang 35 trieu cap tai nghe khong day AirPods. Theo hang phan tich nay, AirPods hien la tai nghe khong day pho bien nhat.&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mã nguồn file utils&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re
import string
import time
from contextlib import contextmanager
import numpy as np



# maximum string length to train and predict
# this is set based on our ngram length break down below
MAXLEN = 32

# minimum string length to consider
MINLEN = 3

# how many words per ngram to consider in our model
NGRAM = 5

# inverting the input generally help with accuracy
INVERT = True

# mini batch size
BATCH_SIZE = 128

# number of phrases set apart from training set to validate our model
VALIDATION_SIZE = 100000

# using g2.2xl GPU is ~5x faster than a Macbook Pro Core i5 CPU
HAS_GPU = True

PAD_WORDS_INPUT  = True

### Ánh xạ từ không dấu sang có dấu

ACCENTED_CHARS = {
    &#39;a&#39;: u&#39;a á à ả ã ạ â ấ ầ ẩ ẫ ậ ă ắ ằ ẳ ẵ ặ&#39;,
    &#39;o&#39;: u&#39;o ó ò ỏ õ ọ ô ố ồ ổ ỗ ộ ơ ớ ờ ở ỡ ợ&#39;,
    &#39;e&#39;: u&#39;e é è ẻ ẽ ẹ ê ế ề ể ễ ệ&#39;,
    &#39;u&#39;: u&#39;u ú ù ủ ũ ụ ư ứ ừ ử ữ ự&#39;,
    &#39;i&#39;: u&#39;i í ì ỉ ĩ ị&#39;,
    &#39;y&#39;: u&#39;y ý ỳ ỷ ỹ ỵ&#39;,
    &#39;d&#39;: u&#39;d đ&#39;,
}

### Ánh xạ từ có dấu sang không dấu
ACCENTED_TO_BASE_CHAR_MAP = {}
for c, variants in ACCENTED_CHARS.items():
    for v in variants.split(&#39; &#39;):
        ACCENTED_TO_BASE_CHAR_MAP[v] = c

# \x00 ký tự padding

### Những ký tự cơ bản, bao gồm ký tự padding, các chữ cái và các chữ số
BASE_ALPHABET = set(&#39;\x00 _&#39; + string.ascii_lowercase + string.digits)

### Bộ ký tự bao gồm những ký tự cơ bản và những ký tự có dấu
ALPHABET = BASE_ALPHABET.union(set(&#39;&#39;.join(ACCENTED_TO_BASE_CHAR_MAP.keys())))


def is_words(text):
    return re.fullmatch(&#39;\w[\w ]*&#39;, text)

# Hàm bỏ dấu khỏi một câu
def remove_accent(text):
    &amp;quot;&amp;quot;&amp;quot; remove accent from text &amp;quot;&amp;quot;&amp;quot;
    return u&#39;&#39;.join(ACCENTED_TO_BASE_CHAR_MAP.get(char, char) for char in text)

#hàm thêm padding vào một câu
def pad(phrase, maxlen):
    &amp;quot;&amp;quot;&amp;quot; right pad given string with \x00 to exact &amp;quot;maxlen&amp;quot; length &amp;quot;&amp;quot;&amp;quot;
    return phrase + u&#39;\x00&#39; * (maxlen - len(phrase))


def gen_ngram(words, n=3, pad_words=True):
    &amp;quot;&amp;quot;&amp;quot; gen n-grams from given phrase or list of words &amp;quot;&amp;quot;&amp;quot;
    if isinstance(words, str):
        words = re.split(&#39;\s+&#39;, words.strip())

    if len(words) &amp;lt; n:
        if pad_words:
            words += [&#39;\x00&#39;] * (n - len(words))
        yield tuple(words)
    else:
        for i in range(len(words) - n + 1):
            yield tuple(words[i: i + n])

def extract_phrases(text):
    &amp;quot;&amp;quot;&amp;quot; extract phrases, i.e. group of continuous words, from text &amp;quot;&amp;quot;&amp;quot;
    return re.findall(r&#39;\w[\w ]+&#39;, text, re.UNICODE)


@contextmanager
def timing(label):
    begin = time.monotonic()
    print(label, end=&#39;&#39;, flush=True)
    try:
        yield
    finally:
        duration = time.monotonic() - begin
    print(&#39;: took {:.2f}s&#39;.format(duration))

class CharacterCodec(object):
    def __init__(self, alphabet, maxlen):
        self.alphabet = list(sorted(set(alphabet)))
        self.index_alphabet = dict((c, i) for i, c in enumerate(self.alphabet))
        self.maxlen = maxlen

    def encode(self, C, maxlen=None):
        maxlen = maxlen if maxlen else self.maxlen
        X = np.zeros((maxlen, len(self.alphabet)))
        for i, c in enumerate(C[:maxlen]):
            X[i, self.index_alphabet[c]] = 1
        return X

    def try_encode(self, C, maxlen=None):
        try:
            return self.encode(C, maxlen)
        except KeyError:
            return None

    def decode(self, X, calc_argmax=True):
        if calc_argmax:
            X = X.argmax(axis=-1)
        return &#39;&#39;.join(self.alphabet[x] for x in X)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;link donwnload mô hình ở lần lặp thứ 10 ở &lt;a href=&#34;https://github.com/AlexBlack2202/alexmodel/blob/master/a_best_weight.h5?raw=true&#34;&gt;https://github.com/AlexBlack2202/alexmodel/blob/master/a_best_weight.h5?raw=true&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;À, kết quả của câu nói phần mở đầu là &amp;ldquo;mẹ nói rằng em rất đậm đang&amp;rdquo;. Hi hi, may quá.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tối ưu hoá ngẫu nhiên - Bài toán người giao hàng</title>
      <link>/blog/2019-02-08-randomized-optimization-in-python-v1/</link>
      <pubDate>Fri, 08 Feb 2019 00:12:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-08-randomized-optimization-in-python-v1/</guid>
      <description>

&lt;h2 id=&#34;bài-toán-người-giao-hàng-là-gì&#34;&gt;Bài toán người giao hàng là gì&lt;/h2&gt;

&lt;p&gt;Người giao hàng là bài toán cơ bản trong nhóm bài toán tối ưu. Bài toán được phát biểu như sau: Có một người giao hàng cần đi giao hàng tại n thành phố. Xuất phát từ một thành phố nào đó, đi qua các thành phố khác để giao hàng và trở về thành phố ban đầu. Mỗi thành phố chỉ đến một lần, khoảng cách từ một thành phố đến các thành phố khác là xác định được. Hãy tìm một chu trình (một đường đi khép kín thỏa mãn điều kiện trên) sao cho tổng độ dài các cạnh là nhỏ nhất.&lt;/p&gt;

&lt;p&gt;Có rất nhiều cách để giải bài toán này, các bạn đọc có thể search google để tìm thêm cách giải khác, ở đây, mình sẽ trình bày cách sử dụng thư viện mlrose của python để giải quyết bài toán trên.&lt;/p&gt;

&lt;h2 id=&#34;cài-đặt-chương-trình-và-thực-thi&#34;&gt;Cài đặt chương trình và thực thi&lt;/h2&gt;

&lt;p&gt;Chúng ta giả định rằng người giao hàng sẽ đi qua 5 thành phố, và mỗi thành phố sẽ có 2 giá trị x và y tương ứng với toạ độ của các thành phố đó trên bản đồ.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input = [
[9, 12],
[24, 15],
[12 ,30],
[4 ,3],
[13, 27],
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Theo phần trước, chúng ta sẽ xây dựng 4 phần&lt;/p&gt;

&lt;h3 id=&#34;xây-dựng-vector-state&#34;&gt;Xây dựng vector state&lt;/h3&gt;

&lt;p&gt;Đơn giản là một vector x có số lượng phần tử bằng số lượng thành phố  mà người giao hàng sẽ viết thăm&lt;/p&gt;

&lt;p&gt;x = [x0,x1,2,x3,x4], trong đó, giá trị x1 là chỉ số của thành phố người giao hàng sẽ ghé đầu tiên, x0 là toạ độ thành phố bắt đầu&lt;/p&gt;

&lt;h3 id=&#34;xây-dựng-hàm-fitness-function&#34;&gt;Xây dựng hàm fitness function&lt;/h3&gt;

&lt;p&gt;Mục tiêu của bài toán là tìm đường đi ngăn nhất, nên chúng ta có thể dễ dàng xây dựng hàn fitness bằng cách tính khoảng cách euclide giữa các thành phố.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def fitness_fun(state):
    distance = 0

    for index in range(1, len(state)):
        dist = np.linalg.norm(input[state[index-1]]-input[state[index]])

        distance = distance + dist

    dist = np.linalg.norm(input[state[0]]-input[state[len(state)-1]])
    distance = distance + dist

    return distance
    
fitness_cust = mlrose.CustomFitness(fitness_fun,&#39;tsp&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;xác-định-loại-bài-toán&#34;&gt;Xác định loại bài toán&lt;/h3&gt;

&lt;p&gt;Đây là bài toán rời rạc không lặp, nên ta sẽ sử dụng hàm TSPOpt, length = 5 do số lượng phần tử của state là 5, maximize=False do bài toán tìm đường đi ngắn nhất .&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;problem_fit = mlrose.TSPOpt(length = 5, fitness_fn = fitness_cust,
                            maximize=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;xác-định-thuật-toán-tối-ưu&#34;&gt;Xác định thuật toán tối ưu&lt;/h3&gt;

&lt;p&gt;Chúng ta vẫn tiếp tục sử dụng thuật toán simulated_annealing như trước xem kết quả như thế nào&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Define decay schedule
schedule = mlrose.ExpDecay()

# Define initial state
init_state = np.array([0, 1, 2, 3, 4])

# Set random seed
np.random.seed(1)

# Solve problem using simulated annealing
best_state, best_fitness = mlrose.simulated_annealing(problem, schedule = schedule,
                                                      max_attempts = 10, max_iters = 500,
                                                      init_state = init_state)

print(&#39;The best state found is: &#39;, best_state)
print(&#39;The fitness at the best state is: &#39;, best_fitness)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;The best state found is:  [1 4 2 0 3]
The fitness at the best state is:  71.30882356753094
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đây là kết quả tối ưu của bài toán.&lt;/p&gt;

&lt;p&gt;Thử thay bằng giải thuật di truyền GA, với tỷ lệ đột biến là 0.2&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;best_state, best_fitness = mlrose.genetic_alg(problem,mutation_prob = 0.2)

print(&#39;The best state found is: &#39;, best_state)
print(&#39;The fitness at the best state is: &#39;, best_fitness)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
The best state found is:  [0 2 4 1 3]
The fitness at the best state is:  71.30882356753094

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử thay đổi tập dữ liệu input có nhiều số phần tử hơn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input =[(1, 1), (4, 2), (5, 2), (6, 4), (4, 4), (3, 6), (1, 5), (2, 3)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;The best state found is:  [3 4 5 6 7 0 1 2]
The fitness at the best state is:  17.34261754766733
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tối ưu hoá ngẫu nhiên</title>
      <link>/blog/2019-02-08-getting-started-with-randomized-optimization-in-python/</link>
      <pubDate>Fri, 08 Feb 2019 00:09:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-08-getting-started-with-randomized-optimization-in-python/</guid>
      <description>

&lt;h2 id=&#34;bài-toán-tối-ưu-hoá-là-gì&#34;&gt;Bài toán tối ưu hoá là gì&lt;/h2&gt;

&lt;p&gt;Theo  Russell and Norvig  bài toán tối ưu hoá là bài toán mà &amp;ldquo;the aim is to find the best state according to an objective function&amp;rdquo; (mình xin phép để nguyên câu tiếng anh).&lt;/p&gt;

&lt;p&gt;Trong đó, state trong từ best state phụ thuộc vào ngữ cảnh của bài toán. Ví dục&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trong ngữ cảnh là mạng neural network, state chính là các trọng số (weight), best state là tìm các trọng số tối ưu&lt;/li&gt;
&lt;li&gt;Trong bài toán 8 hậu, state là vị trí của các con hậu, best state là vị trí tốt nhất thoả yêu cầu, cũng chính là lời giải.&lt;/li&gt;
&lt;li&gt;Trong bài toán người giao hàng, state là các thành phố người giao hàng đi qua.&lt;/li&gt;
&lt;li&gt;Trong bài toán tô màu cho mỗi quốc gia trên bản đồ, state là màu được tô cho mỗi quốc gia&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nói đến đây, các bạn chắc cũng đã hiểu được khái niệm state là gì rồi. Điều quan trọng ở đây là chúng ta có thể biểu diễn state dưới dạng một con số, hoặc một mảng các giá trị số. (nghĩa là chúng ta phải chuyển đổi màu, thành phố, &amp;hellip; dưới dạng số) thì mới có thể tính toán được.&lt;/p&gt;

&lt;p&gt;Từ best trong chữ best state được biểu diễn bởi một hàm toán học (mà chúng ta quen thuộc với các từ như là objective funtion, fitness funtion, cost funtion, loss function , v.v). Cái mà chúng ta muốn là cực đại hoặc cực tiểu  hoá nó (để có được kết quả tốt nhất). Hàm này nhận đầu vào là state array và trả về &amp;ldquo;fitness&amp;rdquo; value.&lt;/p&gt;

&lt;p&gt;Cho nên, chúng ta có thể định nghĩa đơn giản bài toán tối ưu là việc tìm các giá trị tối ưu để cực đại/ cực tiểu hoá một hàm toán học.&lt;/p&gt;

&lt;h2 id=&#34;ví-dụ&#34;&gt;Ví dụ&lt;/h2&gt;

&lt;p&gt;Một ví dụ xàm xàm như sau&lt;/p&gt;

&lt;p&gt;Ta có một (state) vector x = [x0,x1,x2,x3,x4] thuộc đoạn [0,1]
một hàm f(x) = x0 + x1 + x2 + x3 + x4, tìm các giá trị x để f đạt cực đại.&lt;/p&gt;

&lt;p&gt;Rõ ràng, bằng việc tính nhẩm, chúng ta biết được rằng giá trị cực đại của hàm trên là 5, và lời giải cho bài toán trên là x = [1,1,1,1,1].&lt;/p&gt;

&lt;p&gt;Còn theo toán học cấp 3, ta sẽ tính đạo hàm riêng phần của từng phần tử (cái này đơn giản, mình không nhắc lại), và cũng đạt được x = [1,1,1,1,1]&lt;/p&gt;

&lt;h2 id=&#34;tại-sao-lại-dùng-randomized-optimization&#34;&gt;Tại sao lại dùng Randomized Optimization?&lt;/h2&gt;

&lt;p&gt;Trong bài toán ở trên, chúng ta có thể dễ dàng nhẩm được giá trị tối ưu một cách nhanh chóng. Tuy nhiên, trong thực tế, bài toán sẽ khó hơn một chút, và có nhiều hàm chúng ta không thể dễ dàng tìm được giá trị đạo hàm một cách nhanh chóng được (tốn thời gian rất lâu để giải bài toán ). Lúc này, chúng ta sẽ dùng Randomized optimization.&lt;/p&gt;

&lt;p&gt;Randomized optimization sẽ bắt đầu tại một điểm ngẫu nhiên &amp;ldquo;best&amp;rdquo; state nào đó, sau đó sẽ sinh ngẫu nhiên một state khác (thường là láng giềng của &amp;ldquo;best&amp;rdquo; state hiện tại). Nếu state mới đạt giá trị finest tốt hơn &amp;ldquo;best&amp;rdquo; state hiện tại thì gán &amp;ldquo;best&amp;rdquo; state bằng state mới. Quá trình này lặp đi lặp lại cho đến khi không thể tìm được state mới này tốt hơn &amp;ldquo;best&amp;rdquo; state hiện tại.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/global_maximin.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Không có gì bảo đảm rằng randomized optimization sẽ tìm được lời giải tối ưu. Ví dụ như hình trên, thuật toán chỉ có thể dừng ở local maximin, rồi đứng yên ở đó. Tuy nhiên, nếu chúng ta thiết lập số lần lặp đủ lớn, thuật toán thông thường sẽ trả về kết quả tốt hơn.&lt;/p&gt;

&lt;p&gt;Ở đây, chúng ta có một sự đánh đổi trade-off giữa thời gian tìm ra lời giải tối ưu và chất lượng của lời giải.&lt;/p&gt;

&lt;h2 id=&#34;giải-bài-toán-tối-ưu-bằng-thư-viện-mlrose&#34;&gt;Giải bài toán tối ưu bằng thư viện mlrose&lt;/h2&gt;

&lt;p&gt;Để giải bài toán tối ưu bằng thư viện mlrose, chúng ta sẽ phải định nghĩa 4 thứ:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Định nghĩa state vector&lt;/li&gt;
&lt;li&gt;Định nghĩa hàm fitness function&lt;/li&gt;
&lt;li&gt;Xác định loại bài toán&lt;/li&gt;
&lt;li&gt;Chọn  một thuật toán tối ưu hoá ngẫu nhiên để chạy.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Để đơn giản, chúng ta sẽ giải quyết bài toán 8 hậu bằng thư viện mlrose.&lt;/p&gt;

&lt;h3 id=&#34;bài-toán-8-hậu&#34;&gt;Bài toán 8 hậu&lt;/h3&gt;

&lt;p&gt;Nhắc lại một chút về bài toán 8 hậu. Trong bàn cờ vua có kích thước 8x8, chúng ta phải chọn vị trí đặt 8 con hậu sao cho trên mỗi dòng, cột và đường chéo của một con hậu bất kỳ đang đứng không giáp mặt với con hậu khác.&lt;/p&gt;

&lt;h4 id=&#34;định-nghĩa-state&#34;&gt;Định nghĩa state&lt;/h4&gt;

&lt;p&gt;Đây rõ ràng là bài toán tối ưu, và bước đầu tiên ta sẽ định nghĩa một vector trạng thái x = [x0, x1, x2, x3, x4, x5, x6, x7], quy ước toạ độ 0,0 là vị trí trái dưới. Giá trị của xi là vị trị cột của con hậu dòng i đang đứng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/8_queen.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ví dụ, ở hình trên, ta có x = [6, 1, 7, 5, 0, 2, 3, 4], với x0 = 6 nghĩa là con hậu đang ở cột 0 dòng 6 (góc toạ độ chúng ta khảo sát là trái dưới)&lt;/p&gt;

&lt;p&gt;Hình trên không phải là lời giải tối ưu cho bài toán, vì con hậu ở cột 5, cột 6 và cột 7 giáp mặt nhau theo đường chéo.&lt;/p&gt;

&lt;h4 id=&#34;định-nghĩa-fitness-funtion&#34;&gt;Định nghĩa fitness funtion&lt;/h4&gt;

&lt;p&gt;Trong thư viện mlrose đã định nghĩa sẵn hàm fitness function cho một số bài toán đơn giản, ví dụ như trong bài toán 8 hậu vừa rồi. Tuy nhiên, chúng ta sẽ không sử dụng hàm có sẵn đó, mà sẽ tự viết một hàm fitness riêng. Có nhiều cách để định nghĩa hàm fitness khác nhau cho bài toán này. Ở đay, chúng ta sẽ xây dựng một hàm có input là vị trí của các con hậu output là một con số thông báo số lượng con hậu không giáp nhau. Nếu số lượng là 8 thì input chính là lời giải của bài toán.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define alternative N-Queens fitness function for maximization problem
def queens_max(state):
    
    # Initialize counter
    fitness = 0
    
    # For all pairs of queens
    for i in range(len(state) - 1):
        for j in range(i + 1, len(state)):
            
            # Check for horizontal, diagonal-up and diagonal-down attacks
            if (state[j] == state[i]) \
                or (state[j] == state[i] + (j - i)) \
                or (state[j] == state[i] - (j - i)):
                
                # If no attacks, then increment counter
                fitness += 1
                break


    return fitness

fitness_cust = mlrose.CustomFitness(queens_max)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;xác-định-loại-bài-toán&#34;&gt;Xác định loại bài toán&lt;/h4&gt;

&lt;p&gt;Thư viện mlrose cung cấp cho chúng ta các lớp để định nghĩa 3 loại bài toán tối ưu:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;DiscreteOpt: Lớp này được sử dụng để giải các bài toán có giá trị trạng thái là rời rạc. Và tập các trạng thái sẽ được cung cấp trước. Mỗi phần tử trong state chỉ nhận một giá trị trong tập trạng thái. và mỗi phần tử trong tập trạng thái chỉ thuộc về một phần tử trong state.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ContinuousOpt: Lớp này được sử dụng để giải các bài toán có giá trị trạng thái là liên tục.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TSPOpt: Lớp này được dùng để giải các bài toán về travelling. Ví dụ bài toán người giao hàng. Bài toán này khác bài toán Discrete ở chỗ chúng ta sẽ phải tìm ra thứ tự tối ưu của các con số.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bài toán 8 hậu được xếp vào dạng bài toán tối ưu rời rạc. Trong đó, mỗi phần tử trong state vector chỉ mang một con số từ 0 đến 7.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
problem = mlrose.DiscreteOpt(length = 8, fitness_fn = fitness,
                             maximize = False, max_val = 8)
                             
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;length chính là số lượng phần tử trong state vector ( chúng ta có 8 cột nên length = 8), max_val = 8 (đã nói ở trên, giá trị tối ưu là khi 8 con hậu không giáp mặt nhau). Do bài toán của mình là cực tiểu (lý do là fitness  = 0 thì không có con hậu nào giáp mặt nhau, nên chúng ta set maximize = False)&lt;/p&gt;

&lt;h4 id=&#34;xác-định-thuật-toán-tối-ưu&#34;&gt;Xác định thuật toán tối ưu&lt;/h4&gt;

&lt;p&gt;Thư viện mlrose cung cấp cho chúng ta các thuật toán như leo đồi (hill climbing), leo đồi ngẫu nhiên (stochastic hill climbing),simulated annealing, thuật giải di truyền (genetic algorithm), MIMIC (Mutual-Information-Maximizing Input Clustering). Với dạng bài toán rời rạc và travelling, chúng ta có thể chọn bất kỳ thuật toán tối ưu nào. Với bài toán liên tục, thì thuật toán MIMIC không hỗ trợ.&lt;/p&gt;

&lt;p&gt;Ví dụ, chúng ta sẽ sử dụng simulated annealing để mô phỏng hàm tối ưu, với trạng thái init là x = [1,2,3,4,5,6,7], lặp 1000 lần để tìm trạng thái tốt nhất. Có 10 lần thử. để tìm hàng xóm tốt nhất trong mỗi lần lặp.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Define decay schedule
schedule = mlrose.ExpDecay()

# Define initial state
init_state = np.array([0, 1, 2, 3, 4, 5, 6, 7])

# Set random seed
np.random.seed(1)

# Solve problem using simulated annealing
best_state, best_fitness = mlrose.simulated_annealing(problem, schedule = schedule,
                                                      max_attempts = 10, max_iters = 1000,
                                                      init_state = init_state)

print(&#39;The best state found is: &#39;, best_state)
print(&#39;The fitness at the best state is: &#39;, best_fitness)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;The best state found is:  [0 7 6 4 7 1 3 5]
The fitness at the best state is:  1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Do best state =1 , nên có 2 con hậu có thể nhìn thấy và tấn công nhau, Chúng ta sẽ thử thay dổi số max_attempts =10 thành max_attempts = 50 xem sao.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
The best state found is:  [2 0 6 4 7 1 3 5]
The fitness at the best state is:  0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/8_queen_result.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thử thay bằng bài toán 12 hậu&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mlrose

import numpy as np

# Define alternative N-Queens fitness function for maximization problem
def queens_max(state):
    
    # Initialize counter
    fitness = 0
    
    # For all pairs of queens
    for i in range(len(state) - 1):
        for j in range(i + 1, len(state)):
            
            # Check for horizontal, diagonal-up and diagonal-down attacks
            if (state[j] == state[i]) \
                or (state[j] == state[i] + (j - i)) \
                or (state[j] == state[i] - (j - i)):
                
                # If no attacks, then increment counter
                fitness += 1
                break


    return fitness

fitness_cust = mlrose.CustomFitness(queens_max)

problem = mlrose.DiscreteOpt(length = 12, fitness_fn = fitness_cust, maximize = False, max_val = 12)


# Define decay schedule
schedule = mlrose.ExpDecay()

# Define initial state
init_state = np.array([0, 1, 2, 3, 4, 5, 6, 7,8,9,10,11])

# Set random seed
np.random.seed(1)

# Solve problem using simulated annealing
best_state, best_fitness = mlrose.simulated_annealing(problem, schedule = schedule,
                                                      max_attempts = 100, max_iters = 5000,
                                                      init_state = init_state)

print(&#39;The best state found is: &#39;, best_state)
print(&#39;The fitness at the best state is: &#39;, best_fitness)
``

Kết quả

```python
The best state found is:  [ 8 10  3  6  0  9  1  5  2 11  7  4]
The fitness at the best state is:  0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/12_queen_result.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Tất nhiên, ở trên chỉ là 1 trong số các lời giải của bài toán trên, chúng ta còn có nhiều lời giải khác, do bài toán có nhiều nghiệm.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hệ thống tín dụng xã hội của Trung Quốc - Những ảnh hưởng khi bạn có điểm xã hội thấp</title>
      <link>/blog/2019-02-07-china-social-creadit-system/</link>
      <pubDate>Thu, 07 Feb 2019 00:09:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-07-china-social-creadit-system/</guid>
      <description>

&lt;p&gt;Chính quyền Trung Quốc đang xây dựng một hệ thống xếp hạng có tên là &amp;ldquo; Hệ thống tín dụng xã hội - social credit system&amp;rdquo;. Hệ thống được xây dựng nhằm mục đích theo dõi hành vi của công dân và xếp hạng tất cả các hành vi trên.&lt;/p&gt;

&lt;p&gt;Theo một tài liệu cho biết,&amp;ldquo;Hệ thống tín dụng xã hội&amp;rdquo;, lần đầu tiên được công bố vào năm 2014, nhằm mục đích củng cố ý tưởng rằng &amp;ldquo;giữ niềm tin là vinh quang và phá vỡ niềm tin là ô nhục&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Hệ thống sẽ được vận hành hoàn toàn trên toàn quốc vào năm 2020, nhưng đã được thí điểm ở một số vùng trên đất nước, và mang lại kết quả khá khả quan.&lt;/p&gt;

&lt;p&gt;Tại thời điểm hiện tại, hệ thống đang được điều hành bởi chính phủ, một số công ty tư nhân cũng được cấp phép tham gia xây dựng và phát triển hệ thống, như alibaba, tencent.&lt;/p&gt;

&lt;p&gt;Giống như điểm tín dụng tư nhân, điểm xã hội của một người có thể đi lên xuống tùy theo hành vi của họ. Cách thức tính điểm và các hành vi được cho là tốt/xấu hiện thời vẫn chưa được công bố. Nhưng các ví dụ về vi phạm đã bị trừ điểm bao gồm lái xe ẩu, hút thuốc trong khu vực cấm hút thuốc, mua quá nhiều trò chơi video và đăng tin tức giả lên mạng.&lt;/p&gt;

&lt;h4 id=&#34;1-cấm-bay-máy-bay-hoặc-đi-tàu-điện-ngầm&#34;&gt;1. Cấm bay máy bay hoặc đi tàu điện ngầm&lt;/h4&gt;

&lt;p&gt;Chính phủ Trung Quốc đã bắt đầu trừng phạt người dân bằng cách hạn chế việc đi lại của họ.&lt;/p&gt;

&lt;p&gt;Chín triệu người có điểm thấp đã bị chặn mua vé cho các chuyến bay nội địa, Channel News Asia đưa tin vào 16/Mar/2018 nguồn &lt;a href=&#34;https://www.channelnewsasia.com/news/asia/china-bad-social-credit-barred-from-buying-train-plane-tickets-10050390&#34;&gt;https://www.channelnewsasia.com/news/asia/china-bad-social-credit-barred-from-buying-train-plane-tickets-10050390&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Người dân cũng có thể bị giới hạn sử dụng các dịch vụ nâng cao, ví dụ ba triệu người không được mua vé hạng thương gia (trích cùng nguồn trên).&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Here&amp;#39;s a dystopian vision of the future: A real announcement I recorded on the Beijing-Shanghai bullet train. (I&amp;#39;ve subtitled it so you can watch in silence.) &lt;a href=&#34;https://t.co/ZoRWtdcSMy&#34;&gt;pic.twitter.com/ZoRWtdcSMy&lt;/a&gt;&lt;/p&gt;&amp;mdash; James O&amp;#39;Malley (@Psythor) &lt;a href=&#34;https://twitter.com/Psythor/status/1056811593177227264?ref_src=twsrc%5Etfw&#34;&gt;October 29, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;video trên, được đăng bởi nhà báo James O&amp;rsquo;Malley, cho thấy một thông báo trên một chuyến tàu cao tốc từ Bắc Kinh đến Thượng Hải cảnh báo mọi người không nên có những hành vi sai trái - nếu không thì &amp;ldquo;hành vi của họ sẽ được ghi lại trong hệ thống thông tin tín dụng cá nhân&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;2-điều-chỉnh-tốc-độ-internet&#34;&gt;2. Điều chỉnh tốc độ internet&lt;/h4&gt;

&lt;p&gt;Theo nghiên cứu của  Rachel Botsman (nguồn &lt;a href=&#34;https://www.wired.co.uk/article/chinese-government-social-credit-score-privacy-invasion&#34;&gt;https://www.wired.co.uk/article/chinese-government-social-credit-score-privacy-invasion&lt;/a&gt;) chính quyền sẽ giới hạn tốc độ, băng thông của các dịch vụ internet, 3G, 4G, &amp;hellip; của những công dân có điểm tính dụng xã hội thấp.&lt;/p&gt;

&lt;p&gt;Trong nghiên cứu của tác giả, một số hành vi sẽ  bị trừng phạt, bao gồm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Công dân có thanh toán hóa đơn đúng hạn hay không.&lt;/li&gt;
&lt;li&gt;Dành quá nhiều thời gian để chơi trò chơi video&lt;/li&gt;
&lt;li&gt;Lãng phí tiền mua hàng tào lao và đăng lên phương tiện truyền thông xã hội (dạng như tự sướng ở Việt Nam mình á).&lt;/li&gt;
&lt;li&gt;Truyền bá tin tức giả mạo, cụ thể là về các cuộc tấn công khủng bố hoặc an ninh sân bay.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;3-cấm-bạn-hoặc-con-cái-của-bạn-được-học-ở-những-trường-tốt&#34;&gt;3. Cấm bạn, hoặc con cái của bạn được học ở những trường tốt&lt;/h4&gt;

&lt;p&gt;Theo Beijing News reported(nguồn &lt;a href=&#34;http://www.bjnews.com.cn/news/2018/03/19/479533.html&#34;&gt;http://www.bjnews.com.cn/news/2018/03/19/479533.html&lt;/a&gt;), 17 người đã từ chối thực hiện nghĩa vụ quân sự vào năm ngoái (2017) đã bị cấm đăng ký vào giáo dục đại học, nộp đơn vào trường trung học hoặc tiếp tục việc học tập của họ.&lt;/p&gt;

&lt;p&gt;Theo nguồn &lt;a href=&#34;https://www.businessinsider.com/china-social-credit-affects-childs-university-enrolment-2018-7?r=UK&#34;&gt;https://www.businessinsider.com/china-social-credit-affects-childs-university-enrolment-2018-7?r=UK&lt;/a&gt;, vào tháng &lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2018&lt;/sub&gt;, một trường đại học ở Trung Quốc, đã cấm một sinh viên nhập học (dù anh ấy đã thi đậu), vì lý do là điểm tín dụng xã hội của bố anh ấy &amp;ldquo;xấu&amp;rdquo;.&lt;/p&gt;

&lt;h4 id=&#34;4-không-cho-bạn-có-một-công-việc-tốt&#34;&gt;4. Không cho bạn có một công việc tốt&lt;/h4&gt;

&lt;p&gt;Theo nguồn của Botsman, các cá nhân có điểm tín nhiệm thấp sẽ bị cấm làm quản lý ở các công ty nhà nước, các ngân hàng lớn.&lt;/p&gt;

&lt;p&gt;Các hành vi như gian lận thuế, tham ô, &amp;hellip; cũng ảnh hưởng đến điểm xã hội.&lt;/p&gt;

&lt;h4 id=&#34;5-không-được-thuê-những-khách-sạn-tốt&#34;&gt;5. Không được thuê những khách sạn tốt&lt;/h4&gt;

&lt;p&gt;Theo Botsman, những người gian lận nghĩa vụ quân sự sẽ bị cấm thuê khách sạn tốt khi đi du lịch.&lt;/p&gt;

&lt;p&gt;Những công dân có điểm tín dụng tốt sẽ được thuê khách sạn mà không cần phải đặt cọc, có thể kéo dài thời gian du lịch hơn.&lt;/p&gt;

&lt;h4 id=&#34;6-cấm-nuôi-chó&#34;&gt;6. Cấm nuôi chó&lt;/h4&gt;

&lt;p&gt;Thành phố Tế Nam đã bắt đầu thực thi một hệ thống tín dụng xã hội cho các chủ sở hữu chó vào năm 2017. Theo đó, chủ vật nuôi sẽ bị trừ điểm nếu nuôi chó mà không xích, không rọ mõm, hoặc để cho chó đi bậy nơi công cộng.&lt;/p&gt;

&lt;p&gt;Những người bị zero điểm sẽ bị cấm nuôi chó, con vật sẽ bị tịch thu, người sở hữu phải làm bài kiểm tra. Nguồn &lt;a href=&#34;http://uk.businessinsider.com/china-dog-owners-social-credit-score-2018-10&#34;&gt;http://uk.businessinsider.com/china-dog-owners-social-credit-score-2018-10&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;7-bị-bêu-tên-trước-công-chúng&#34;&gt;7. Bị bêu tên trước công chúng&lt;/h4&gt;

&lt;p&gt;Chính phủ đã và đang xây dựng một danh sách các cá nhân có điểm tín nhiệm xấu và sẵn sàng đăng tên kèm hình ảnh của họ trên các phương tiện thông tin đại chúng. Các công ty cũng được khuyến khích tham khảo các thông tin của công dân trong hệ thống trước khi thuê họ.&lt;/p&gt;

&lt;p&gt;Được biết, toà án sẽ thông báo cho công dân về hành vi của họ trước khi tên của họ được đưa vào danh sách đen. Công dân có 10 ngày kháng cáo kể từ khi nhận được thông báo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/7TkE4ojDQ0WIqEkiYt/giphy.gif&#34; alt=&#34;Hình ảnh danh sách&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nguồn &lt;a href=&#34;https://www.hrw.org/news/2017/12/12/chinas-chilling-social-credit-blacklist&#34;&gt;https://www.hrw.org/news/2017/12/12/chinas-chilling-social-credit-blacklist&lt;/a&gt;, &lt;a href=&#34;http://zxgk.court.gov.cn/&#34;&gt;http://zxgk.court.gov.cn/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lựa chọn siêu tham số cho mô hình LSTM đơn giản sử dụng Keras</title>
      <link>/blog/2019-02-06-choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras/</link>
      <pubDate>Wed, 06 Feb 2019 00:20:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-06-choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras/</guid>
      <description>

&lt;h2 id=&#34;mở-đầu&#34;&gt;Mở đầu&lt;/h2&gt;

&lt;p&gt;Việc xây dựng một mô hình machine learning chưa bao giờ thật sự dễ dàng. Rất nhiều bài báo chỉ &amp;ldquo;show hàng&amp;rdquo; những thứ cao siêu, những thứ chỉ nằm trong sự tưởng tượng của chính các nhà báo. Còn khi đọc các bài báo khoa học về machine learning, tác giả công bố cho chúng ta những mô hình rất tốt, giải quyết một domain nhỏ vấn đề của họ. Tuy nhiên, có một thứ họ không/ chưa công bố. Đó là cách thức họ lựa chọn số lượng note ẩn, số lượng layer trong mô hình neural network. Trong bài viết này, chúng ta sẽ xây dựng mô hình LSTM đơn giản để dự đoán giới tính khi biết tên một người, và thử tìm xem công thức để chọn ra tham số &amp;ldquo;đủ tốt&amp;rdquo; là như thế nào.&lt;/p&gt;

&lt;h2 id=&#34;chẩn-bị-dữ-liệu&#34;&gt;Chẩn bị dữ liệu&lt;/h2&gt;

&lt;p&gt;Tập dữ liệu ở đây có khoảng 500000 tên kèm giới tính. Đầu tiên mình sẽ làm sạch dữ liệu bằng cách chỉ lấy giới tính là &amp;rsquo;m&amp;rsquo; và &amp;lsquo;f&amp;rsquo;, loại bỏ những tên quá ngắn (có ít hơn 3 ký tự)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filepath = &#39;firstnames.csv&#39;
max_rows = 500000 # Reduction due to memory limitations

df = (pd.read_csv(filepath, usecols=[&#39;name&#39;, &#39;gender&#39;],sep=&amp;quot;;&amp;quot;)
        .dropna(subset=[&#39;name&#39;, &#39;gender&#39;])
        .assign(name = lambda x: x.name.str.strip())
        .assign(gender = lambda x: x.gender.str.lower())
        .head(max_rows))

df= df[df.gender.isin([&#39;m&#39;,&#39;f&#39;])]

# In the case of a middle name, we will simply use the first name only
df[&#39;name&#39;] = df[&#39;name&#39;].apply(lambda x: str(x).split(&#39; &#39;, 1)[0])

# Sometimes people only but the first letter of their name into the field, so we drop all name where len &amp;lt;3
df.drop(df[df[&#39;name&#39;].str.len() &amp;lt; 3].index, inplace=True)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sử dụng một kỹ thuật khá cũ trong NLP là one-hot encoding. Mỗi ký tự được biểu diễn bởi một vector nhị phân. Ví dụ có 26 ký tự trong bảng chữ cái tiếng anh, vector đại diện cho chữ a là [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], ký tự b được biểu diễn là [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], &amp;hellip; tương tự cho đến z.&lt;/p&gt;

&lt;p&gt;Một từ được encode là một tập các vector. Ví dụ chữ hello được biểu diễn là&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #h,
 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #e,
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #l,
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #l,
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] #o]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đọc đến đây, chắc các bạn đã mườn tượng ra rằng một từ sẽ được encode như thế nào rồi phải không. Tiếp theo, chúng ta sẽ xây dựng hàm encode cho tập dữ liệu&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; # Define a mapping of chars to integers
char_to_int = dict((c, i) for i, c in enumerate(accepted_chars))
int_to_char = dict((i, c) for i, c in enumerate(accepted_chars))

# Removes all non accepted characters
def normalize(line):
    return [c.lower() for c in line if c.lower() in accepted_chars]

# Returns a list of n lists with n = word_vec_length
def name_encoding(name):

    # Encode input data to int, e.g. a-&amp;gt;1, z-&amp;gt;26
    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i &amp;lt; word_vec_length]
    
    # Start one-hot-encoding
    onehot_encoded = list()
    
    for value in integer_encoded:
        # create a list of n zeros, where n is equal to the number of accepted characters
        letter = [0 for _ in range(char_vec_length)]
        letter[value] = 1
        onehot_encoded.append(letter)
        
    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array
    for _ in range(word_vec_length - len(name)):
        onehot_encoded.append([0 for _ in range(char_vec_length)])
        
    return onehot_encoded

# Encode the output labels
def lable_encoding(gender_series):
    labels = np.empty((0, 2))
    for i in gender_series:
        if i == &#39;m&#39;:
            labels = np.append(labels, [[1,0]], axis=0)
        else:
            labels = np.append(labels, [[0,1]], axis=0)
    return labels
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và tiến hành chia tập dữ liệu thành train, val, và test set&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
# Split dataset in 60% train, 20% test and 20% validation
train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])

# Convert both the input names as well as the output lables into the discussed machine readable vector format
train_x =  np.asarray([np.asarray(name_encoding(normalize(name))) for name in train[predictor_col]])
train_y = lable_encoding(train.gender)

validate_x = np.asarray([name_encoding(normalize(name)) for name in validate[predictor_col]])
validate_y = lable_encoding(validate.gender)

test_x = np.asarray([name_encoding(normalize(name)) for name in test[predictor_col]])
test_y = lable_encoding(test.gender)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vậy là chúng ta đã có chuẩn bị xong dữ liệu đầy đủ rồi đó. Bây giờ chúng ta xây dựng mô hình thôi.&lt;/p&gt;

&lt;h2 id=&#34;xây-dựng-mô-hình&#34;&gt;Xây dựng mô hình&lt;/h2&gt;

&lt;p&gt;Có rất nhiều cách để chọn tham số cho mô hình, ví dụ như ở &lt;a href=&#34;https://stats.stackexchange.com/questions/95495/guideline-to-select-the-hyperparameters-in-deep-learning&#34;&gt;https://stats.stackexchange.com/questions/95495/guideline-to-select-the-hyperparameters-in-deep-learning&lt;/a&gt;
 liệt kê ra 4 cách là Manual Search, Grid Search, Random Search, Bayesian Optimization. Tuy nhiên,  những cách trên đều khá tốn thời gian và đòi hỏi người kỹ sư phải có am hiểu nhất định.&lt;/p&gt;

&lt;p&gt;Ở đây, chúng ta sử dụng một công thức được đưa ra trong link &lt;a href=&#34;https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw/136542#136542&#34;&gt;https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw/136542#136542&lt;/a&gt;, cụ thể&lt;/p&gt;

&lt;p&gt;$$ N_h = \frac{N_s}{(\alpha * (N_i + N_o))}$$&lt;/p&gt;

&lt;p&gt;Trong đó Ni là số lượng input neural, No là số lượng output neural, Ns là số lượng element trong tập dữ liệu train. alpha là một con số trade-off đại diện cho tỷ lệ thuộc đoạn [2-10].&lt;/p&gt;

&lt;p&gt;Một lưu ý ở đây là bạn có thể dựa vào công thức và số alpha mà ước lượng xem rằng bạn đã có đủ dữ liệu mẫu hay chưa. Một ví dụ đơn giản là giả sử bạn có 10,000 mẫu dữ liệu, input số từ 0 đến 9, output là 64, chọn alpha ở mức nhỏ nhất là 2, vậy theo công thức số neural ẩn là 10000/(2*64*10) = 7.8 ~ 8. Nếu bạn tăng số alpha lên thì số hidden layer còn ít nữa. Điều trên chứng tỏ rằng số lượng mẫu của bạn chưa đủ, còn thiếu quá nhiều.  Nếu bạn tăng gấp 100 lần số dữ liệu mẫu, thì con số có vẻ hợp lý hơn.&lt;/p&gt;

&lt;p&gt;Trong tập dữ liệu, mình có:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;The input vector will have the shape  {17} x {82}
Train len:  (21883, 17, 82) 36473

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tổng cộng N_s là 21883, Ni là 17, No là 82, chọn alpha là 2 thì mình có 21883/(2*17*82) = 7.8 ~ 8. Một con số khá nhỏ, chứng tỏ dữ liệu của mình còn quá ít.&lt;/p&gt;

&lt;p&gt;Đối với tập dữ liệu nhỏ như thế này, mình thường sẽ áp dụng công thức sau:&lt;/p&gt;

&lt;p&gt;$$ N_h= \beta* (N_i + N_o) $$&lt;/p&gt;

&lt;p&gt;Với beta là một con số thực thuộc nửa đoạn (0,1]. Thông thường sẽ là &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;. Kết quả là số lượng neural của mình khoảng 929.333 node. Thông thường, mình sẽ chọn số neural là một con số là bội số của 2, ở đây 929 gần với 2^10 nhất, nên mình chọn số neural là 2^10.&lt;/p&gt;

&lt;p&gt;Tóm lại, mình sẽ theo quy tắc&lt;/p&gt;

&lt;p&gt;Nếu dữ liệu nhiều:&lt;/p&gt;

&lt;p&gt;$$ N_h = \frac{N_s}{(\alpha * (N_i + N_o))}$$&lt;/p&gt;

&lt;p&gt;Nếu dữ liệu ít&lt;/p&gt;

&lt;p&gt;$$ N_h= \frac{2}{3}* (N_i + N_o) $$&lt;/p&gt;

&lt;p&gt;Làm tròn lên bằng với bội số của 2 mũ gần nhất.&lt;/p&gt;

&lt;p&gt;Một lưu ý nhỏ là số lượng node càng nhiều thì tỷ lệ overfit càng cao, và thời gian huấn luyện càng lâu. Do đó, bạn nên trang bị máy có cấu hình kha khá một chút, tốt hơn hết là nên có GPU đi kèm. Ngoài ra, bạn nên chuẩn bị càng nhiều dữ liệu càng tốt. Một kinh nghiệm của mình rút ra trong quá trình làm Machine Learning là nếu không có nhiều dữ liệu, thì đừng cố thử áp dụng các phương pháp ML trên nó.&lt;/p&gt;

&lt;p&gt;Mô hình mình xây dựng như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
hidden_nodes = 1024


# Build the model
print(&#39;Build model...&#39;)
model = Sequential()
model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))
model.add(Dropout(0.2))
model.add(Dense(units=output_labels))
model.add(Activation(&#39;softmax&#39;))
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;acc&#39;])

batch_size=1000
model.fit(train_x, train_y, batch_size=batch_size, epochs=50, validation_data=(validate_x, validate_y))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Do bài viết chỉ tập trung vào vấn đề lựa chọn số lượng node, nên mình sẽ bỏ qua những phần phụ như là early stoping, save each epochs &amp;hellip;, Các vấn đề trên ít nhiều mình đã đề cập ở các bài viết trước.&lt;/p&gt;

&lt;p&gt;Kết quả của việc huấn luyện mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 21883/21883 [==============================] - 34s 2ms/step - loss: 0.6602 - acc: 0.6171 - val_loss: 0.6276 - val_acc: 0.7199
Epoch 2/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5836 - acc: 0.7056 - val_loss: 0.5625 - val_acc: 0.7193
Epoch 3/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5531 - acc: 0.7353 - val_loss: 0.5506 - val_acc: 0.7389
Epoch 4/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.5480 - acc: 0.7446 - val_loss: 0.5664 - val_acc: 0.7313
Epoch 5/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5406 - acc: 0.7420 - val_loss: 0.5247 - val_acc: 0.7613
Epoch 6/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.5077 - acc: 0.7686 - val_loss: 0.4918 - val_acc: 0.7790
Epoch 7/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4825 - acc: 0.7837 - val_loss: 0.4939 - val_acc: 0.7740
Epoch 8/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.4611 - acc: 0.7887 - val_loss: 0.4407 - val_acc: 0.8037
Epoch 9/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4421 - acc: 0.7987 - val_loss: 0.4657 - val_acc: 0.8005
Epoch 10/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4293 - acc: 0.8055 - val_loss: 0.4183 - val_acc: 0.8141
Epoch 11/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.4129 - acc: 0.8128 - val_loss: 0.4171 - val_acc: 0.8212
Epoch 12/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.4153 - acc: 0.8141 - val_loss: 0.4031 - val_acc: 0.8188
Epoch 13/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3978 - acc: 0.8191 - val_loss: 0.3918 - val_acc: 0.8280
Epoch 14/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3910 - acc: 0.8268 - val_loss: 0.3831 - val_acc: 0.8276
Epoch 15/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3848 - acc: 0.8272 - val_loss: 0.3772 - val_acc: 0.8314
Epoch 16/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3751 - acc: 0.8354 - val_loss: 0.3737 - val_acc: 0.8363
Epoch 17/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3708 - acc: 0.8345 - val_loss: 0.3717 - val_acc: 0.8374
Epoch 18/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3688 - acc: 0.8375 - val_loss: 0.3768 - val_acc: 0.8330
Epoch 19/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3704 - acc: 0.8375 - val_loss: 0.3621 - val_acc: 0.8392
Epoch 20/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3608 - acc: 0.8444 - val_loss: 0.3656 - val_acc: 0.8422
Epoch 21/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3548 - acc: 0.8459 - val_loss: 0.3670 - val_acc: 0.8417
Epoch 22/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3521 - acc: 0.8452 - val_loss: 0.3555 - val_acc: 0.8462
Epoch 23/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3432 - acc: 0.8504 - val_loss: 0.3591 - val_acc: 0.8402
Epoch 24/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3415 - acc: 0.8524 - val_loss: 0.3471 - val_acc: 0.8470
Epoch 25/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3355 - acc: 0.8555 - val_loss: 0.3577 - val_acc: 0.8436
Epoch 26/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3320 - acc: 0.8552 - val_loss: 0.3602 - val_acc: 0.8430
Epoch 27/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3294 - acc: 0.8578 - val_loss: 0.3565 - val_acc: 0.8485
Epoch 28/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3235 - acc: 0.8602 - val_loss: 0.3427 - val_acc: 0.8514
Epoch 29/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3138 - acc: 0.8651 - val_loss: 0.3523 - val_acc: 0.8470
Epoch 30/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.3095 - acc: 0.8683 - val_loss: 0.3457 - val_acc: 0.8487
Epoch 31/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.3064 - acc: 0.8701 - val_loss: 0.3538 - val_acc: 0.8531
Epoch 32/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2985 - acc: 0.8717 - val_loss: 0.3555 - val_acc: 0.8455
Epoch 33/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2930 - acc: 0.8741 - val_loss: 0.3430 - val_acc: 0.8525
Epoch 34/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2901 - acc: 0.8786 - val_loss: 0.3457 - val_acc: 0.8503
Epoch 35/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2852 - acc: 0.8776 - val_loss: 0.3458 - val_acc: 0.8510
Epoch 36/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2817 - acc: 0.8811 - val_loss: 0.3445 - val_acc: 0.8568
Epoch 37/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2780 - acc: 0.8816 - val_loss: 0.3356 - val_acc: 0.8540
Epoch 38/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2734 - acc: 0.8852 - val_loss: 0.3442 - val_acc: 0.8559
Epoch 39/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2579 - acc: 0.8904 - val_loss: 0.3552 - val_acc: 0.8540
Epoch 40/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2551 - acc: 0.8927 - val_loss: 0.3677 - val_acc: 0.8532
Epoch 41/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2558 - acc: 0.8921 - val_loss: 0.3496 - val_acc: 0.8588
Epoch 42/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2472 - acc: 0.8963 - val_loss: 0.3534 - val_acc: 0.8587
Epoch 43/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2486 - acc: 0.8948 - val_loss: 0.3490 - val_acc: 0.8537
Epoch 44/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2503 - acc: 0.8965 - val_loss: 0.3594 - val_acc: 0.8552
Epoch 45/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2391 - acc: 0.8993 - val_loss: 0.3793 - val_acc: 0.8566
Epoch 46/50
21883/21883 [==============================] - 31s 1ms/step - loss: 0.2244 - acc: 0.9048 - val_loss: 0.3815 - val_acc: 0.8543
Epoch 47/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2203 - acc: 0.9095 - val_loss: 0.3848 - val_acc: 0.8554
Epoch 48/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2221 - acc: 0.9051 - val_loss: 0.3892 - val_acc: 0.8558
Epoch 49/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2117 - acc: 0.9124 - val_loss: 0.3654 - val_acc: 0.8544
Epoch 50/50
21883/21883 [==============================] - 30s 1ms/step - loss: 0.2141 - acc: 0.9118 - val_loss: 0.3726 - val_acc: 0.8547


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Độ chính xác trên tập train là hơn 90%, trên tập val là hơn 85%. Nhìn kỹ hơn vào những từ sai ta thấy rằng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;             name gender predicted_gender
6750       Chiaki      f                m
28599      Naheed      f                m
11448  Espiridión      m                f
895       Akmaral      f                m
33778         Ros      f                m

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Có một sự nhập nhằng ở ngôn ngữ giữa tên nam và tên nữ ở những từ này. Có lẽ một tập dữ liệu với đầy đủ họ và tên sẽ cho ra một kết quả có độ chính xác cao hơn. Ví dụ, ở Việt Nam, tên Ngọc thì có thể đặt được cho cả Nam lẫn Nữ.&lt;/p&gt;

&lt;p&gt;Mình sẽ cố gắng kiếm một bộ dataset tên tiếng việt và thực hiện việc xây dựng mô hình xác định giới tính thông qua tên người dựa vào mô hình LSTM.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Giảm bộ nhớ sử dụng trong python</title>
      <link>/blog/2019-02-06-how-to-reduce-memory-consumption-by-half-by-adding-just-one-line-of-code/</link>
      <pubDate>Wed, 06 Feb 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-06-how-to-reduce-memory-consumption-by-half-by-adding-just-one-line-of-code/</guid>
      <description>

&lt;h2 id=&#34;mở-đầu&#34;&gt;Mở đầu&lt;/h2&gt;

&lt;p&gt;Bắt đầu bằng một class đơn giản như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DataItem(object):
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bạn nghĩ một đối tượng của class trên sẽ chiếm bao nhiêu bộ nhớ. Chúng ta cùng tiến hành một vài thí nghiệm nho nhỏ bên dưới.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dx = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;-&amp;quot;)
print (&amp;quot;sys.getsizeof(dx):&amp;quot;, sys.getsizeof(dx))
&amp;gt;&amp;gt; sys.getsizeof(dx): 56
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả ra là &lt;em&gt;56 bytes&lt;/em&gt;, khá hợp lý phải không các bạn. Thử với một ví dụ khác xem sao nhỉ.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dy = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;I am working at MWG&amp;quot;)
print (&amp;quot;sys.getsizeof(dy):&amp;quot;, sys.getsizeof(dy))
&amp;gt;&amp;gt; sys.getsizeof(dy): 56
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả vẫn là &lt;em&gt;56 bytes&lt;/em&gt;. Có cái gì đó sai sai ở đây không nhỉ?&lt;/p&gt;

&lt;p&gt;Chúng ta thực nghiệm một vài thí nghiệm khác để chứng thực.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print (sys.getsizeof(&amp;quot;&amp;quot;))
&amp;gt;&amp;gt; 49
print (sys.getsizeof(&amp;quot;1&amp;quot;))
&amp;gt;&amp;gt; 50
print (sys.getsizeof(1))
&amp;gt;&amp;gt; 28
print (sys.getsizeof(dict()))
&amp;gt;&amp;gt; 240
print (sys.getsizeof({}))
&amp;gt;&amp;gt; 240
print (sys.getsizeof(list()))
&amp;gt;&amp;gt; 64
print (sys.getsizeof([]))
&amp;gt;&amp;gt; 64
print (sys.getsizeof(()))
&amp;gt;&amp;gt; 48
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một điều cực kỳ bất ngờ đã xuất hiện ở đây. Một chuỗi rỗng chiếm đến tận &lt;em&gt;49 bytes&lt;/em&gt;, một dictionary rỗng, không chứa phần tử nào chiếm đến &lt;em&gt;240 bytes&lt;/em&gt;, và một list rỗng chiếm tới &lt;em&gt;64 bytes&lt;/em&gt;. Rõ ràng, python đã lưu một số thứ gì đó ngoài dữ liệu của mình.&lt;/p&gt;

&lt;p&gt;Đi sâu vào thử tìm hiểu những thứ &amp;lsquo;linh kiện&amp;rsquo; linh tinh mà python đã kèm theo cho chúng ta là gì nhé.&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ cần một hàm in ra những thứ mà python đã &amp;lsquo;nhúng&amp;rsquo; thêm vào class DataItem chúng ta khai báo ở trên.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def dump(obj):
  for attr in dir(obj):
    print(&amp;quot;  obj.%s = %r&amp;quot; % (attr, getattr(obj, attr)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;và dump biến dy ra thôi&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dump(dy)

obj.__class__ = &amp;lt;class &#39;__main__.DataItem&#39;&amp;gt;
  obj.__delattr__ = &amp;lt;method-wrapper &#39;__delattr__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__dict__ = {&#39;name&#39;: &#39;Alex Black&#39;, &#39;age&#39;: 42, &#39;address&#39;: &#39;i am working at MWG&#39;}
  obj.__dir__ = &amp;lt;built-in method __dir__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__doc__ = None
  obj.__eq__ = &amp;lt;method-wrapper &#39;__eq__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__format__ = &amp;lt;built-in method __format__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__ge__ = &amp;lt;method-wrapper &#39;__ge__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__getattribute__ = &amp;lt;method-wrapper &#39;__getattribute__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__gt__ = &amp;lt;method-wrapper &#39;__gt__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__hash__ = &amp;lt;method-wrapper &#39;__hash__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__init__ = &amp;lt;bound method DataItem.__init__ of &amp;lt;__main__.DataItem object at 0x000001A64A6DD0F0&amp;gt;&amp;gt;
  obj.__init_subclass__ = &amp;lt;built-in method __init_subclass__ of type object at 0x000001A64A5DE738&amp;gt;
  obj.__le__ = &amp;lt;method-wrapper &#39;__le__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__lt__ = &amp;lt;method-wrapper &#39;__lt__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__module__ = &#39;__main__&#39;
  obj.__ne__ = &amp;lt;method-wrapper &#39;__ne__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__new__ = &amp;lt;built-in method __new__ of type object at 0x000000005C2DC580&amp;gt;
  obj.__reduce__ = &amp;lt;built-in method __reduce__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__reduce_ex__ = &amp;lt;built-in method __reduce_ex__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__repr__ = &amp;lt;method-wrapper &#39;__repr__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__setattr__ = &amp;lt;method-wrapper &#39;__setattr__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__sizeof__ = &amp;lt;built-in method __sizeof__ of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__str__ = &amp;lt;method-wrapper &#39;__str__&#39; of DataItem object at 0x000001A64A6DD0F0&amp;gt;
  obj.__subclasshook__ = &amp;lt;built-in method __subclasshook__ of type object at 0x000001A64A5DE738&amp;gt;
  obj.__weakref__ = None
  obj.address = &#39;i am working at MWG&#39;
  obj.age = 42
  obj.name = &#39;Alex Black&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow, có vẻ khá là đồ sộ nhỉ.&lt;/p&gt;

&lt;p&gt;Trên github, có một hàm có sẵn tính toán số lượng bộ nhớ mà object chiếm được dựa vào cách truy xuất trực tiếp từng trường dữ liệu của đối tượng và tính toán kích thước&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys

def get_size(obj, seen=None):
    &amp;quot;&amp;quot;&amp;quot;Recursively finds size of objects&amp;quot;&amp;quot;&amp;quot;
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    # Important mark as seen *before* entering recursion to gracefully handle
    # self-referential objects
    seen.add(obj_id)
    if isinstance(obj, dict):
        size += sum([get_size(v, seen) for v in obj.values()])
        size += sum([get_size(k, seen) for k in obj.keys()])
    elif hasattr(obj, &#39;__dict__&#39;):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, &#39;__iter__&#39;) and not isinstance(obj, (str, bytes, bytearray)):
        size += sum([get_size(i, seen) for i in obj])
    return size
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;thử với 2 biến dx và dy của chúng ta xem sao&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; print (&amp;quot;get_size(d1):&amp;quot;, get_size(dx))
get_size(d1): 466
&amp;gt;&amp;gt;&amp;gt; print (&amp;quot;get_size(d1):&amp;quot;, get_size(dy))
get_size(d1): 484
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng tốn lần lượt là 466 và 484 bytes. Có vẻ đúng đó nhỉ.&lt;/p&gt;

&lt;p&gt;Điều chúng ta quan tâm lúc này là có cách nào để giảm bộ nhớ tiêu thụ của một object hay không?&lt;/p&gt;

&lt;h2 id=&#34;giảm-bộ-nhớ-tiêu-thụ-của-một-đối-tượng-trong-python&#34;&gt;Giảm bộ nhớ tiêu thụ của một đối tượng trong python&lt;/h2&gt;

&lt;p&gt;Tất nhiên là sẽ có cách giảm. Python là một ngôn ngữ thông dịch, và nó cho phép chúng ta mở rộng lớp bất kể lúc nào bằng cách thêm một/ nhiều trường dữ liệu.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dz = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;-&amp;quot;)
dz.height = 1.80
print ( get_size(dz))
&amp;gt;&amp;gt; 484
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chính vì lý do này, trình biên dịch sẽ tốn thêm một đống bộ nhớ tạm để chúng ta có thể dễ dàng mở rộng một lớp trong tương lai. Nếu chúng ta &amp;ldquo;ép buộc&amp;rdquo; trình biên dịch, nói rằng chúng ta chỉ có nhiêu đó trường, và bỏ phần dư thừa đi.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DataItem(object):
    __slots__ = [&#39;name&#39;, &#39;age&#39;, &#39;address&#39;]
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và thử lại&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
dz = DataItem(&amp;quot;Alex Black&amp;quot;, 42, &amp;quot;i am working at MWG&amp;quot;)
print (&amp;quot;sys.getsizeof(dz):&amp;quot;, get_size(dz))

&amp;gt;&amp;gt;sys.getsizeof(dz): 64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Các bạn thấy gì không, bộ nhớ tiêu thụ chỉ là &amp;ldquo;64 bytes&amp;rdquo;. Dung lượng đã giảm đi hơn &amp;ldquo;7 lần&amp;rdquo; so với model class ban đầu. Tuy nhiên, chúng ta sẽ không thể mở rộng class dễ dàng như xưa nữa.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; dz.height = 1.80
Traceback (most recent call last):
  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;
AttributeError: &#39;DataItem&#39; object has no attribute &#39;height&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử tạo một đối tượng có 1000 phần tử và kiểm tra thử.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DataItem(object):
    __slots__ = [&#39;name&#39;, &#39;age&#39;, &#39;address&#39;]
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address


data = []

tracemalloc.start()
start =datetime.datetime.now()
for p in range(100000):
    data.append(DataItem(&amp;quot;Alex&amp;quot;, 42, &amp;quot;middle of nowhere&amp;quot;))
    
end =datetime.datetime.now()
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics(&#39;lineno&#39;)
total = sum(stat.size for stat in top_stats)
print(&amp;quot;Total allocated size: %.1f MB&amp;quot; % (total / (1024*1024)))
print(&amp;quot;Total execute time:&amp;quot;,(end-start).microseconds)

&amp;gt;&amp;gt; Total allocated size: 6.9 MB
&amp;gt;&amp;gt; Total execute time: 232565
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bỏ dòng &lt;strong&gt;slots&lt;/strong&gt; = [&amp;lsquo;name&amp;rsquo;, &amp;lsquo;age&amp;rsquo;, &amp;lsquo;address&amp;rsquo;] đi thử&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
class DataItem(object):
    def __init__(self, name, age, address):
        self.name = name
        self.age = age
        self.address = address


data = []

tracemalloc.start()
start =datetime.datetime.now()
for p in range(100000):
    data.append(DataItem(&amp;quot;Alex&amp;quot;, 42, &amp;quot;middle of nowhere&amp;quot;))
end =datetime.datetime.now()
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics(&#39;lineno&#39;)
total = sum(stat.size for stat in top_stats)
print(&amp;quot;Total allocated size: %.1f MB&amp;quot; % (total / (1024*1024)))
print(&amp;quot;Total execute time:&amp;quot;,(end-start).microseconds)

&amp;gt;&amp;gt; Total allocated size: 16.8 MB
&amp;gt;&amp;gt; Total execute time: 240772
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So sánh thử, chúng ta thấy rằng số lượng RAM giảm đi khá nhiều, thời gian thực thi khá tương đương nhau (có giảm một chút).&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 mẹo hay sử dụng python</title>
      <link>/blog/2019-02-05-5-python-tricks-you-need-to-know-today/</link>
      <pubDate>Tue, 05 Feb 2019 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-02-05-5-python-tricks-you-need-to-know-today/</guid>
      <description>

&lt;h2 id=&#34;mở-đầu&#34;&gt;Mở đầu&lt;/h2&gt;

&lt;p&gt;Hiện nay, có rất nhiều thư viện do cộng đồng đóng góp và xây dựng. Ví dụ như biopython trong tin sinh học, pandas (data science), keras/tensorflow (machine learning), astropy ( cho thiên văn học - astronomy). Trước khi bắt đầu đọc bài viết này, bạn đên đọc &amp;ldquo;Python Tricks Book&amp;rdquo; của Dan Bader trước (&lt;a href=&#34;https://dbader.org/products/python-tricks-book/&#34;&gt;https://dbader.org/products/python-tricks-book/&lt;/a&gt;). Trong sách, anh ấy đã chia sẻ một số lời khuyên và mẹo về các code python hiệu quả hơn.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-số-1-sức-mạnh-của-một-dòng&#34;&gt;Mẹo số 1: Sức mạnh của một dòng&lt;/h2&gt;

&lt;p&gt;Khi bạn đọc một đoạn giải thuật với nhiều dòng code, có thể bạn sẽ bị quên thông tin những dòng trước đó đã viết gì, đặc biệt là trong những câu lệnh điều kiện. Ví dụ:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
if alpha &amp;gt; 7:
     beta = 999
elif alpha == 7:
    beta = 99
else:
   beta =0

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chóng ta có thể viết đơn giản hơn chỉ với một dòng code như sau.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;beta = 999 if alpha &amp;gt; 7 else 99 if alpha == 7 else 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;thật đơn giản phải không. Bạn chỉ cần nhìn đúng một dòng là nằm được nội dung ý nghĩa của đoạn code bạn cần. Một ví dụ khác về vòng lặp for.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lst = [1, 2, 3, 4] 
lst_double = []

for num in lst:
    lst_double.append(num * 2)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đoạn code trên có thể viết lại dưới dạng 1 dòng như sau.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lst_double = [num * 2 for num in lst]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tất nhiên, bạn không nên &amp;ldquo;lạm dụng&amp;rdquo; one line một cách thái quá, ví dụ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pprint; pprint.pprint(zip((&#39;Byte&#39;, &#39;KByte&#39;, &#39;MByte&#39;, &#39;GByte&#39;, &#39;TByte&#39;), (1 &amp;lt;&amp;lt; 10*i for i in xrange(5))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trông nó có vẻ hơi &amp;ldquo;lố bịch&amp;rdquo; phải không.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-2-các-thao-tác-nhanh-trên-chuỗi&#34;&gt;Mẹo 2: Các thao tác nhanh trên chuỗi&lt;/h2&gt;

&lt;p&gt;Python cung cấp cho chúng ta một số cách viết ngắn gọn giúp chúng ta có thể dể dàng thao tác trên chuỗi. Để reverse một chuỗi, chúng ta sử dụng toán tử ::-1&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
str = &#39;i am alex&#39;
print(str[::-1])
&amp;gt;&amp;gt; xela ma i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mẹo trên cũng có thể sử dụng đối với list số nguyên.&lt;/p&gt;

&lt;p&gt;Để nối các phần tử trong một list thành một chuỗi, chúng ta có thể dùng hàm join()&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
str1 = [&amp;quot;pig&amp;quot;, &amp;quot;year&amp;quot; , &amp;quot;2019&amp;quot;]
str2 = &amp;quot;happy &amp;quot;
str3 = &amp;quot;new &amp;quot;


print( &#39; &#39;.join(str1))
&amp;gt;&amp;gt; pig year 2019

print(str2+str3+&#39; &#39;.join(str1))
&amp;gt;&amp;gt; happy new year 2019
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thật tuyệt vời phải không các bạn.&lt;/p&gt;

&lt;p&gt;Ngoài ra các bạn có thể sử dụng biếu thức chính quy để tìm kiếm chuỗi và pattern. Về biểu thức chính quy trong python, các bạn có thể tìm hiểu ở &lt;a href=&#34;https://docs.python.org/3/library/re.html&#34;&gt;https://docs.python.org/3/library/re.html&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-số-3-chuỗi-lồng-nhau&#34;&gt;Mẹo số 3: Chuỗi lồng nhau&lt;/h2&gt;

&lt;p&gt;Thử tưởng tượng rằng bạn có hàng tá các list, và sau một mớ các thao tác, kết quả của bạn là một list các list. Chúng ta sẽ sử dụng itertools - một thư viện được cung cấp sẵn trong python để giải quyết vấn đề này giúp chúng ta.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import itertools
flatten = lambda x: list(itertools.chain.from_iterable(x))
s =[[&amp;quot;this&amp;quot;,&amp;quot;is&amp;quot;],[&amp;quot;the&amp;quot;,&amp;quot;year&amp;quot;], [&amp;quot;of&amp;quot;, &amp;quot;pig&amp;quot;], [&amp;quot;in&amp;quot;], [&amp;quot;Việt&amp;quot;, &amp;quot;Nam&amp;quot;]]

print(&#39; &#39;,join(flatten(s)))
&amp;gt;&amp;gt; this is the year of pig in Việt Nam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nếu bạn chạy dòng code trên bị lỗi, rất có thể là do terminal của bạn không hỗ trợ tiếng việt font unicode. Hãy chuyển qua font unicode trên terminal hoặc dùng terminal của ubuntu, bash (trên window 10).&lt;/p&gt;

&lt;p&gt;Ngoài ra, itertools còn hỗ trợ rất nhiều hàm khác để giúp chúng ta thao tác trên chuỗi lồng dễ dàng hơn. Các bạn có thể tham khảo thêm ở &lt;a href=&#34;https://docs.python.org/2/library/itertools.html&#34;&gt;https://docs.python.org/2/library/itertools.html&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;mẹo-4-cấu-trúc-dữ-liệu-đơn-giản&#34;&gt;Mẹo 4: Cấu trúc dữ liệu đơn giản.&lt;/h2&gt;

&lt;p&gt;Chúng ta có thể xây dựng một cây đơn giản chỉ với một dòng mã lệnh:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tree(): return defaultdict(tree)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một ví dụ đơn giản khác là hàm tạo số nguyên chỉ với 1 dòng code ngắn gọn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reduce( (lambda r,x: r-set(range(x**2,N,x)) if (x in r) else r), 
        range(2,N), set(range(2,N)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Python có hỗ trợ nhiều thư viện rất mạnh trong việc giải quyết các vấn đề trong thế giới thực. Ví dụ thư viện Collections&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter
myList = [1,1,2,3,4,5,3,2,3,4,2,1,2,3]
print(Counter(myList))
Counter({2: 4, 3: 4, 1: 3, 4: 2, 5: 1})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một lưu ý nhỏ là các thư viện này chỉ nên sử dụng khi tập dữ liệu của bạn nhỏ, nếu tập dữ liệu lớn, ví dụ bạn cần đếm số lần xuất hiện của các từ trong tập văn bản với 100GB dữ liệu. Bạn hãy dùng cách khác, ví dụ hadoop, hoặc tăng bộ nhớ ram của bạn lên, ví dụ 1 Tb chẳng hạn :)&lt;/p&gt;

&lt;h2 id=&#34;mẹo-5-xuất-dữ-liệu-ra-command-line-dễ-dàng&#34;&gt;Mẹo 5: Xuất dữ liệu ra command line dễ dàng&lt;/h2&gt;

&lt;p&gt;Để xuất dữ liệu của một list int ra command line, theo như mẹo ở trên, ta sẽ dùng hàm .join() và vòng lặp.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python`&#34;&gt;lst_row = [1,2,3,4,5]
print(&#39;,&#39;.join([str(x) for x in lst_row])
1,2,3,4,5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cách đơn giản hơn chỉ với một dòng code (Ước gì mình biết cách này sớm hơn, hix).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(*lst_row, sep=&#39;,&#39;)
1,2,3,4,5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một mẹo khác là trong một số trường hợp duyệt mảng, bạn cần lấy giá trị và chỉ số của mảng đó để làm một số thao tác khác&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
lst_arr = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]

int_index = 0

for item in lst_arr:
    print(int_index, item)
    int_index = int_index + 1
    
&amp;gt;&amp;gt; 0 a
1 b
2 c
3 d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hoặc cách viết giống c/c++&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
for int_index in len(lst_arr):
    print(int_index, lst_arr[int_index])
    
&amp;gt;&amp;gt; 0 a
1 b
2 c
3 d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một cách khác là sử dụng hàm có sẵn enumerate của python&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for int_index, item in enumerate(lst_arr):
    print(int_index, item)
    
&amp;gt;&amp;gt; 0 a
1 b
2 c
3 d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Có rất nhiều mẹo hay để đơn giản hoá việc xuất dữ liệu ra terminal. Hãy thông tin cho mình biết nếu bạn có nhiều mẹo hay khác cần chia sẻ nhé.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hệ thống gợi ý khoá học cho website DonorChoose.org</title>
      <link>/blog/2019-01-03-donor-project-matching-with-recommender-systems/</link>
      <pubDate>Tue, 11 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2019-01-03-donor-project-matching-with-recommender-systems/</guid>
      <description>

&lt;h2 id=&#34;đặt-vấn-đề&#34;&gt;Đặt vấn đề&lt;/h2&gt;

&lt;p&gt;DonorsChoose.org được thành lập vào năm 2000 bởi một giáo viên lịch sử tại Mỹ tên là Bronx và đã huy động được 685 triệu đô la cho các lớp học. &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt; các giáo viên ở các trường công lập ở Hoa Kỳ đã sử dụng Donor để gửi các yêu cầu bài tập cho học sinh. Từ đó, Donor trở thành nền tảng giáo dục hàng đầu hỗ trợ cho các vấn đề giáo dục công cộng.&lt;/p&gt;

&lt;p&gt;Đến nay, hơn 3 triệu người dùng và đối tác đã đóng góp hơn 1,1 triệu dự án cho Donor. Nhưng các giáo viên vẫn phải tốn hàng tỷ đô tiền túi để chuẩn bị các dụng cụ học tập trên lớp (để truyền tải kiến thức cho học sinh).&lt;/p&gt;

&lt;p&gt;Giải pháp được đưa ra ở đây là xây dựng một chiến dịch gợi ý cho các nhà tại trợ.&lt;/p&gt;

&lt;h3 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h3&gt;

&lt;p&gt;Chúng ta có các file sau:&lt;/p&gt;

&lt;p&gt;File Donations.csv.  Với mỗi dự án (Project ID), sẽ có 1 hoặc nhiều nhà quyên góp (Donor ID) mỗi cặp (dự án - nhà quyên góp sẽ định dang bằng 1 mã chung (Donation ID) và có các cột thông tin liên quan đến việc quyên góp đó). File có xấp xỉ 4.67 triệu dòng (chính xác là 4687844 dòng) và 7 cột. (Project ID - Định danh dự án, Donation ID - Định danh khoảng đóng góp (tưởng tượng như khoá tự tăng của bảng này đó các bạn), Donor ID - Mã định danh người đóng góp, Donation Included Option - hỗ trợ website donoschoose 15% giá trị quyên góp, Donation Amount - Số tiền quyên góp, Donor Cart Sequence - Thứ tự của dự án trọng bảng danh sách quyên góp,Donation Received Date - Ngày giờ quyên góp).&lt;/p&gt;

&lt;p&gt;File Donors.csv. File định danh người quyên góp. Chứa tổng cộng hơn 2 triệu dòng( chính xác là 2122640 dòng)
File có kích thước 2122640 x 5 với các thông tin cột là Donor ID (khoá chính, không trùng), Donor City (tên thành phố nhà đầu tư đang sinh sống), Donor State (tiểu bang mà người quyên góp đang sống), Donor is teacher, Donor Zip (3 ký tự đầu của mã bưu điện nhà từ thiện).&lt;/p&gt;

&lt;p&gt;File Teacher.csv. File có tổng cộng 402900 dòng với các cột TeachId, Teacher Prefix (Mr, Mrs, Ms), Teacher First Project Posted Date.&lt;/p&gt;

&lt;p&gt;File Schools.csv. File có tổng cộng 72994 dòng với các cột là SchoolID, SchoolName (tên trường có thể trùng nhau), School Metro Type ( phân loại trường thuộc 1 trong 5 nhóm : suburnban - ngoại ô, rural - nông thôn, uban - thành thị, town - thị trấn, unknow), School Percentage Free Lunch ( Số nguyên, mô tả tỷ lệ phần trăm số học sinh đủ điều kiện ăn trưa miễn phí hoặc ăn trưa giảm phí. Dữ liệu thu được cung cấp bởi một đối tác thống kê độc lập là NCES. Nếu trường nào không có giá trị do NCES cung cấp, chúng ta sẽ lấy số phần trăm này là trung bình phần trăm của các trường cùng huyện), School State (Trường đang toạ lạc ở bang nào (vd cali, Florida, Virginia, &amp;hellip;)), School Zip (mã bưu chính), School City, School County&lt;/p&gt;

&lt;p&gt;File Resources.csv. Với mỗi dự án, chúng ta cần các loại tài nguyên khác nhau. Các cột là Project ID (mã dự án), Resource Item Name (tên tài nguyên cần cho dự án đó vd project 000009891526c0ade7180f8423792063 cần &amp;lsquo;chair move and store cart&amp;rsquo;), Resource Quantity (số lượng tài nguyên cần, vd cần 1 cái ghế, 2 cái bảng v.v),
Resource Unit Price (đơn giá cho 1 đơn vị tài nguyên, vd cái ghế giá 7 ngàn, cái bảng giá 10 ngàn, nếu 1 unit là ghế + bảng thì là 17 ngàn), Resource Vendor Name(nhà cung cấp, vd: Amazon Business, Woodwind and Brasswind).&lt;/p&gt;

&lt;p&gt;File Projects.csv&lt;/p&gt;

&lt;h3 id=&#34;xây-dựng-chiến-lược-tiếp-cận-bài-toán&#34;&gt;Xây dựng chiến lược tiếp cận bài toán&lt;/h3&gt;

&lt;p&gt;Hãy xem đây như là bài toán gợi ý. Và Donors chính là hệ thống cung cấp các sản phẩm. Ví dụ đơn giản là bạn có website nghe nhạc mp3.zing.vn, alice vào nghe một hoặc một vài bài nhạc. Chúng ta sẽ xây dựng một hệ gợi ý những bài nhạc tiếp theo alice nên nghe dựa vào những bài nhạc đã nghe trước đó của alice. Tương tự vậy, hệ thống Donor như là website mp3.zing, bài nhạc tương tự như các project đang có, người dùng tương tự như các nhà tự thiện. Một khi một nhà từ thiện đã quyên góp cho 1 hoặc 1 nhón các dự án, chúng ta sẽ lên kế hoạch và gợi ý cho khác hàng dự án tiếp theo khách hàng nên tìm hiểu kỹ để xét xem có nên donate hay không.&lt;/p&gt;

&lt;p&gt;Dựa vào các chiến lược trên, chúng ta có 3 cách có thể tiếp cận vấn đề:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Content-based filltering.&lt;/li&gt;
&lt;li&gt;Collaborative Filtering&lt;/li&gt;
&lt;li&gt;Hybrid methods&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;1-tiền-xử-lý-dữ-liệu&#34;&gt;1. Tiền xử lý dữ liệu&lt;/h4&gt;

&lt;p&gt;Trước khi bắt đầu xây dựng chương trình gợi ý, chúng ta cần phải load dữ liệu lên bộ nhớ chính và làm sạch dữ liệu.&lt;/p&gt;

&lt;p&gt;Trước tiên, chúng ta sẽ import các thư viện cần thiết. Nếu thiếu các thư viện nào, các bạn cứ pip install tên thư viện trong cmd/terminal là được&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import scipy
import pandas as pd
import math
import random
import sklearn
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.linalg import svds
import matplotlib.pyplot as plt
import os
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load 3 file Projects.csv, Donations.csv, Donors.csv lên và merge donations với donors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set up test mode to save some time
test_mode = True

# Read datasets
projects = pd.read_csv(&#39;../input/Projects.csv&#39;)
donations = pd.read_csv(&#39;../input/Donations.csv&#39;)
donors = pd.read_csv(&#39;../input/Donors.csv&#39;)

#this piece of code converts Project_ID which is a 32-bit Hex int digits 10-1010
# create column &amp;quot;project_id&amp;quot; with sequential integers
f=len(projects)
projects[&#39;project_id&#39;] = np.nan
g = list(range(10,f+10))
g = pd.Series(g)
projects[&#39;project_id&#39;] = g.values

# Merge datasets
donations = donations.merge(donors, on=&amp;quot;Donor ID&amp;quot;, how=&amp;quot;left&amp;quot;)
df = donations.merge(projects,on=&amp;quot;Project ID&amp;quot;, how=&amp;quot;left&amp;quot;)

# only load a few lines in test mode
if test_mode:
    df = df.head(10000)

donations_df = df
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở giai đoạn xây dựng code và debug, mình chỉ load 10000 dữ liệu lên để test thử  (để đảm bảo rằng mình code đúng - bằng cách set test_mode = True). Khi chạy thật mình sẽ set lại test_mode = False.&lt;/p&gt;

&lt;p&gt;Thực hiện một vài bước phân tích kỹ thuật đơn giản để nắm rõ hơn về dữ liệu.&lt;/p&gt;

&lt;p&gt;Thử đo mối quan hệ giữa các dự án và các &amp;ldquo;mạnh thường quân&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Deal with missing values
donations[&amp;quot;Donation Amount&amp;quot;] = donations[&amp;quot;Donation Amount&amp;quot;].fillna(0)

# Define event strength as the donated amount to a certain project
donations_df[&#39;eventStrength&#39;] = donations_df[&#39;Donation Amount&#39;]

def smooth_donor_preference(x):
    return math.log(1+x, 2)
    
donations_full_df = donations_df \
                    .groupby([&#39;Donor ID&#39;, &#39;Project ID&#39;])[&#39;eventStrength&#39;].sum() \
                    .apply(smooth_donor_preference).reset_index()
        
# Update projects dataset
project_cols = projects.columns
projects = df[project_cols].drop_duplicates()

print(&#39;# of projects: %d&#39; % len(projects))
print(&#39;# of unique user/project donations: %d&#39; % len(donations_full_df))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# of projects: 1889
# of unique user/project donations: 8648
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dựa vào kết quả trên tập test, chúng ta có thể đưa ra một vài nhận xét như sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hầu hết các mạnh thường quân chỉ donate cho 1 project (tỷ lệ 86,48%)&lt;/li&gt;
&lt;li&gt;Sẽ có trường hợp 1 mạnh thường quân sẽ donate cho nhiều dự án, và cũng có trường hợp 1 mạnh thường quân donate nhiều lần cho 1 dự án. Trường hợp này chiếm phần ít.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Để đánh giá mô hình, chúng ta sẽ chia tập dữ liệu thành 2 phần là train và test. Ở đây, chúng ta sẽ set tỷ lệ train/test là 20%.&lt;/p&gt;

&lt;h4 id=&#34;2-xây-dựng-mô-hình-content-based-filtering&#34;&gt;2. Xây dựng mô hình Content-Based Filtering&lt;/h4&gt;

&lt;p&gt;Cách tiếp cận đầu tiên, chúng ta sẽ tìm những project gần giống với những project mà donor đã donated. Đơn giản nhất là với mỗi project, chúng ta sẽ định nghĩa các vector đặc trưng của chúng và đo độ giống nhau giữa hai vector đó. Vector đặc trưng chúng ta có thể xây dựng trên các thuộc tính như project type, project catefory, grade level, resource category, cost, school zip code, &amp;hellip; hoặc các bạn có thể từ các vector cơ bản do tập dữ liệu cung cấp bổ sung thêm các vector cấp cao hơn, ví dụ như là rút trích các feature từ tên project hoặc mô tả của project, loại bỏ stopwords &amp;hellip;&lt;/p&gt;

&lt;p&gt;Ở đây, chúng ta sẽ sử dụng kỹ thuật TF-IDF để rút trích thông tin đặc trưng của dự án dựa trên project tittle và description. Về TF-IDF, các bạn có thể đọc ở một bài viết nào đó của google, mình không tiện nhắc đến nó chi tiết ở bài viết này.&lt;/p&gt;

&lt;h5 id=&#34;a-xây-dựng-tập-đặc-trưng&#34;&gt;a. Xây dựng tập đặc trưng&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Preprocessing of text data
textfeats = [&amp;quot;Project Title&amp;quot;,&amp;quot;Project Essay&amp;quot;]
for cols in textfeats:
    projects[cols] = projects[cols].astype(str) 
    projects[cols] = projects[cols].astype(str).fillna(&#39;&#39;) # FILL NA
    projects[cols] = projects[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently
 
text = projects[&amp;quot;Project Title&amp;quot;] + &#39; &#39; + projects[&amp;quot;Project Essay&amp;quot;]
vectorizer = TfidfVectorizer(strip_accents=&#39;unicode&#39;,
                             analyzer=&#39;word&#39;,
                             lowercase=True, # Convert all uppercase to lowercase
                             stop_words=&#39;english&#39;, # Remove commonly found english words (&#39;it&#39;, &#39;a&#39;, &#39;the&#39;) which do not typically contain much signal
                             max_df = 0.9, # Only consider words that appear in fewer than max_df percent of all documents
                             # max_features=5000 # Maximum features to be extracted                    
                            )                        
project_ids = projects[&#39;Project ID&#39;].tolist()
tfidf_matrix = vectorizer.fit_transform(text)
tfidf_feature_names = vectorizer.get_feature_names()


## build profile

def get_project_profile(project_id):
    idx = project_ids.index(project_id)
    project_profile = tfidf_matrix[idx:idx+1]
    return project_profile

def get_project_profiles(ids):
    project_profiles_list = [get_project_profile(x) for x in np.ravel([ids])]
    project_profiles = scipy.sparse.vstack(project_profiles_list)
    return project_profiles

def build_donors_profile(donor_id, donations_indexed_df):
    donations_donor_df = donations_indexed_df.loc[donor_id]
    donor_project_profiles = get_project_profiles(donations_donor_df[&#39;Project ID&#39;])
    donor_project_strengths = np.array(donations_donor_df[&#39;eventStrength&#39;]).reshape(-1,1)
    #Weighted average of project profiles by the donations strength
    donor_project_strengths_weighted_avg = np.sum(donor_project_profiles.multiply(donor_project_strengths), axis=0) / (np.sum(donor_project_strengths)+1)
    donor_profile_norm = sklearn.preprocessing.normalize(donor_project_strengths_weighted_avg)
    return donor_profile_norm

from tqdm import tqdm

def build_donors_profiles(): 
    donations_indexed_df = donations_full_df[donations_full_df[&#39;Project ID&#39;].isin(projects[&#39;Project ID&#39;])].set_index(&#39;Donor ID&#39;)
    donor_profiles = {}
    for donor_id in tqdm(donations_indexed_df.index.unique()):
        donor_profiles[donor_id] = build_donors_profile(donor_id, donations_indexed_df)
    return donor_profiles

donor_profiles = build_donors_profiles()
print(&amp;quot;# of donors with profiles: %d&amp;quot; % len(donor_profiles))

mydonor1 = &amp;quot;6d5b22d39e68c656071a842732c63a0c&amp;quot;
mydonor2 = &amp;quot;0016b23800f7ea46424b3254f016007a&amp;quot;
mydonor1_profile = pd.DataFrame(sorted(zip(tfidf_feature_names, 
                        donor_profiles[mydonor1].flatten().tolist()), 
                        key=lambda x: -x[1])[:10],
                        columns=[&#39;token&#39;, &#39;relevance&#39;])
mydonor2_profile = pd.DataFrame(sorted(zip(tfidf_feature_names, 
                        donor_profiles[mydonor2].flatten().tolist()), 
                        key=lambda x: -x[1])[:10],
                        columns=[&#39;token&#39;, &#39;relevance&#39;])

print(&#39;feature of user &#39; + str(mydonor1))
print(mydonor1_profile)

print(&#39;feature of user &#39; + str(mydonor2))
print(mydonor2_profile)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mã nguồn ở trên cũng có chú thích đầy đủ, và đọc cũng dễ hiểu, nên mình không nói thêm gì nhiều. Mình tóm gọn một chút là chúng ta sẽ convert toàn bộ project tittle và description về dạng chữ thường, tách từ dựa vào khoảng trắng, loại bỏ những english stopwords. Sau đó xây dựng profile cho từng donor.&lt;/p&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature of  user 6d5b22d39e68c656071a842732c63a0c
        token  relevance
0       music   0.450057
1  auditorium   0.355256
2        cart   0.272809
3       chair   0.223861
4   equipment   0.211338
5   musicians   0.179244
6        time   0.172908
7      moving   0.137749
8        ohms   0.134065
9     prepare   0.131274
feature of  user 0016b23800f7ea46424b3254f016007a
         token  relevance
0  pollinators   0.670222
1       plants   0.305398
2       module   0.223407
3  pollination   0.211870
4        seeds   0.180609
5      writing   0.166816
6        books   0.137455
7      reading   0.115003
8       weaved   0.111704
9         bees   0.101842
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nhìn kết quả trên, ta thấy rằng donor 1 có vẻ thích những thứ liên quan đến âm nhạc (music, auditorim), trong khi đó donor 2 thích những thứ liên quan đến trồng trọt (pollinators - thụ phấn, plants - cây cối)&lt;/p&gt;

&lt;h5 id=&#34;b-xây-dựng-mô-hình&#34;&gt;b. Xây dựng mô hình&lt;/h5&gt;

&lt;p&gt;Việc xây dựng mô hình đến đây là khá đơn giản. Chúng ta chỉ việc tính khoảng cách cosin giữa vector cần dự đoán và toàn bộ vector có trong tập train rồi show top K prject có liên quan cao nhất&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

class ContentBasedRecommender:
    
    MODEL_NAME = &#39;Content-Based&#39;
    
    def __init__(self, projects_df=None):
        self.project_ids = project_ids
        self.projects_df = projects_df
        
    def get_model_name(self):
        return self.MODEL_NAME
        
    def _get_similar_projects_to_donor_profile(self, donor_id, topn=1000):
        #Computes the cosine similarity between the donor profile and all project profiles
        cosine_similarities = cosine_similarity(donor_profiles[donor_id], tfidf_matrix)
        #Gets the top similar projects
        similar_indices = cosine_similarities.argsort().flatten()[-topn:]
        #Sort the similar projects by similarity
        similar_projects = sorted([(project_ids[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1])
        return similar_projects
        
    def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10, verbose=False):
        similar_projects = self._get_similar_projects_to_donor_profile(donor_id)
        #Ignores projects the donor has already donated
        similar_projects_filtered = list(filter(lambda x: x[0] not in projects_to_ignore, similar_projects))
        
        recommendations_df = pd.DataFrame(similar_projects_filtered, columns=[&#39;Project ID&#39;, &#39;recStrength&#39;]).head(topn)

        recommendations_df = recommendations_df.merge(self.projects_df, how = &#39;left&#39;, 
                                                    left_on = &#39;Project ID&#39;, 
                                                    right_on = &#39;Project ID&#39;)[[&#39;recStrength&#39;, &#39;Project ID&#39;, &#39;Project Title&#39;, &#39;Project Essay&#39;]]


        return recommendations_df


cbr_model = ContentBasedRecommender(projects)


print(&#39;recommend for user &#39; + str(mydonor1))
print(cbr_model.recommend_projects(mydonor1))

print(&#39;recommend for user &#39; + str(mydonor2))
print(cbr_model.recommend_projects(mydonor2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recommend for user 6d5b22d39e68c656071a842732c63a0c
   recStrength                        ...                                                              Project Essay
0     1.000000                        ...                          the music students in our classes perform freq...
1     0.390997                        ...                          i have spent 12 years as an educator rebuildin...
2     0.338676                        ...                          &amp;quot;music is what feelings sound like.&amp;quot; -g. cates...
3     0.331034                        ...                          true music is created not by the teacher but b...
4     0.324355                        ...                          every morning my first grade students come to ...
5     0.322923                        ...                          in today&#39;s fast paced environment, students ne...
6     0.315910                        ...                          &amp;quot;music is a moral law.  it gives soul to the u...
7     0.314845                        ...                          i walk in the door so excited to get the stude...
8     0.310103                        ...                          some students have never put their hands on a ...
9     0.297516                        ...                          my students do not have money, but they do hav...

[10 rows x 4 columns]
recommend for user 0016b23800f7ea46424b3254f016007a
   recStrength                        ...                                                              Project Essay
0     1.000000                        ...                          my students are creative, curious, and excited...
1     0.211962                        ...                          our school is a title 1 school.  100% of stude...
2     0.189111                        ...                          my students are active and eager learners who ...
3     0.188095                        ...                          being a small rural school we do a lot of trad...
4     0.173520                        ...                          &amp;quot;science is a way of life...science is the pro...
5     0.159015                        ...                          my second grade students love to come to schoo...
6     0.158071                        ...                          i teach 28 fourth graders in a neighborhood sc...
7     0.150389                        ...                          in my classroom we are working hard to become ...
8     0.144724                        ...                          as a teacher in a diverse, low-income, high-po...
9     0.139937                        ...                          have you ever been told you need to read, but ...

[10 rows x 4 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mình dùng cmd nên bị giới hạn kết quả, các bạn có thể write log vào file hoặc dùng jupiter để show kết quả rõ hơn.&lt;/p&gt;

&lt;p&gt;Ở đây, chúng ta nhận thấy rằng các recommend cho donor 1 thường là những project liên quan tới âm nhạc (nhìn tập feature ta cũng có thể đoán được). Và recommend cho donor 2 là những thứ liên quan đến chủ đề làm vườn và reading.&lt;/p&gt;

&lt;h4 id=&#34;3-collaborative-filtering-model&#34;&gt;3. Collaborative Filtering Model&lt;/h4&gt;

&lt;p&gt;Lý thuyết về Collaborative Filtering Model các bạn có thể xem ở các bài viết khác của mình hoặc tham khảo thêm trên mạng. Ở đây, mình sẽ sử dụng Singular Value Decomposition (SVD) để xây dựng ma trận đặc trưng.&lt;/p&gt;

&lt;h5 id=&#34;a-xây-dựng-ma-trận-donor-project&#34;&gt;a. Xây dựng ma trận donor - project&lt;/h5&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ xây dựng ma trận mối quan hệ giữa donor và project. Nếu donor i có donated cho 1 project j thì dòng i cột j của ma trận sẽ được đánh dấu là 1, ngược lại là 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#### create matrix
#Creating a sparse pivot table with donors in rows and projects in columns
donors_projects_pivot_matrix_df = donations_full_df.pivot(index=&#39;Donor ID&#39;, 
                                                          columns=&#39;Project ID&#39;, 
                                                          values=&#39;eventStrength&#39;).fillna(0)

# Transform the donor-project dataframe into a matrix
donors_projects_pivot_matrix = donors_projects_pivot_matrix_df.as_matrix()

# Get donor ids
donors_ids = list(donors_projects_pivot_matrix_df.index)

print(donors_projects_pivot_matrix[:5]) # print first 5 row
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;b-singular-value-decomposition&#34;&gt;b. Singular Value Decomposition&lt;/h5&gt;

&lt;p&gt;Sau khi có ma trận trên, ta có một nhận xét rằng nó rất thưa, số lượng 0 thì nhiều mà 1 thì ít. Sau khi áp dụng SVD, ma trận kết quả sẽ ít thưa hơn (có thể đạt được đến mức không còn thưa nữa).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Performs matrix factorization of the original donor-project matrix
# Here we set k = 20, which is the number of factors we are going to get
# In the definition of SVD, an original matrix A is approxmated as a product A ≈ UΣV 
# where U and V have orthonormal columns, and Σ is non-negative diagonal.
U, sigma, Vt = svds(donors_projects_pivot_matrix, k = 20)
sigma = np.diag(sigma)

# Reconstruct the matrix by multiplying its factors
all_donor_predicted_ratings = np.dot(np.dot(U, sigma), Vt) 

#Converting the reconstructed matrix back to a Pandas dataframe
cf_preds_df = pd.DataFrame(all_donor_predicted_ratings, 
                           columns = donors_projects_pivot_matrix_df.columns, 
                           index=donors_ids).transpose()
                           
print(cf_preds_df.head())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;                                  0003aba06ccf49f8c44fc2dd3b582411                ...                 ffff088c35d3455779a30898d1327b76
Project ID                                                                        ...

000009891526c0ade7180f8423792063                     -3.423182e-34                ...-4.577244e-34
00000ce845c00cbf0686c992fc369df4                     -3.061322e-36                ...-6.492305e-36
00002d44003ed46b066607c5455a999a                      1.368936e-33                ...-2.239156e-32
00002eb25d60a09c318efbd0797bffb5                      1.784576e-33                ...1.163684e-32
0000300773fe015f870914b42528541b                      4.314216e-34                ...-4.666110e-34

[5 rows x 8015 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;c-xây-dựng-collaborative-filtering-model&#34;&gt;c. Xây dựng Collaborative Filtering Model&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

class CFRecommender:
    
    MODEL_NAME = &#39;Collaborative Filtering&#39;
    
    def __init__(self, cf_predictions_df, projects_df=None):
        self.cf_predictions_df = cf_predictions_df
        self.projects_df = projects_df
        
    def get_model_name(self):
        return self.MODEL_NAME
        
    def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10):
        # Get and sort the donor&#39;s predictions
        sorted_donor_predictions = self.cf_predictions_df[donor_id].sort_values(ascending=False) \
                                    .reset_index().rename(columns={donor_id: &#39;recStrength&#39;})

        # Recommend the highest predicted projects that the donor hasn&#39;t donated to
        recommendations_df = sorted_donor_predictions[~sorted_donor_predictions[&#39;Project ID&#39;].isin(projects_to_ignore)] \
                               .sort_values(&#39;recStrength&#39;, ascending = False) \
                               .head(topn)

 
        recommendations_df = recommendations_df.merge(self.projects_df, how = &#39;left&#39;, 
                                                          left_on = &#39;Project ID&#39;, 
                                                          right_on = &#39;Project ID&#39;)[[&#39;recStrength&#39;, &#39;Project ID&#39;, &#39;Project Title&#39;, &#39;Project Essay&#39;]]


        return recommendations_df

cfr_model = CFRecommender(cf_preds_df, projects)
print(cfr_model.recommend_projects(mydonor1))

print(cfr_model.recommend_projects(mydonor2))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[5 rows x 8015 columns]
    recStrength                        ...                                                              Project Essay
0  3.015461e-17                        ...                          Our students are some of the hardest working k...
1  2.237275e-17                        ...                          As Service Learning Coordinators at our elemen...
2  2.188501e-17                        ...                          We are trying to engage more students in scien...
3  1.768711e-17                        ...                          We are a brand new charter school that has onl...
4  1.344489e-17                        ...                          Sitting at a desk for a sustained period of ti...
5  9.957278e-18                        ...                          Our students come from a Title I school in Jer...
6  6.932330e-18                        ...                          In my school 50% of the students are socioecon...
7  8.589640e-19                        ...                          Have you ever been told you need to read, but ...
8  6.698040e-19                        ...                          &amp;quot;I cannot say good-bye to those whom I have gr...
9  5.733941e-19                        ...                          I have students in class who are squinting and...

[10 rows x 4 columns]
    recStrength                        ...                                                              Project Essay
0  3.015461e-17                        ...                          Our students are some of the hardest working k...
1  2.237275e-17                        ...                          As Service Learning Coordinators at our elemen...
2  2.188501e-17                        ...                          We are trying to engage more students in scien...
3  1.768711e-17                        ...                          We are a brand new charter school that has onl...
4  1.344489e-17                        ...                          Sitting at a desk for a sustained period of ti...
5  9.957278e-18                        ...                          Our students come from a Title I school in Jer...
6  6.932330e-18                        ...                          In my school 50% of the students are socioecon...
7  8.589640e-19                        ...                          Have you ever been told you need to read, but ...
8  6.698040e-19                        ...                          &amp;quot;I cannot say good-bye to those whom I have gr...
9  5.733941e-19                        ...                          I have students in class who are squinting and...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả trả về có vẻ không được đẹp như ở phương pháp trên. Ở đây, thuật toán dựa vào hành vi donated của những người khác có điểm tương đồng với user donor 1 và 2. Bởi vậy gợi ý những project sẽ khác những gợi ý ở phương pháp 1.&lt;/p&gt;

&lt;h4 id=&#34;4-hybrid-method&#34;&gt;4. Hybrid Method&lt;/h4&gt;

&lt;p&gt;Phương pháp lai này kết hợp cả 2 hướng tiếp cận của hai phương pháp ở trên. Ở đây, chúng ta sẽ xây dựng một mô hình nhỏ, nhân điểm của content based và collaborative filtering lại với nhau, sau đó xếp hạng để được điểm hybrid. Đây là 1 cách đơn giản, các bạn có thể tìm đọc nhiều cách tiếp cận khác và ứng dụng vào bài toán.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HybridRecommender:
    
    MODEL_NAME = &#39;Hybrid&#39;
    
    def __init__(self, cb_rec_model, cf_rec_model, projects_df):
        self.cb_rec_model = cb_rec_model
        self.cf_rec_model = cf_rec_model
        self.projects_df = projects_df
        
    def get_model_name(self):
        return self.MODEL_NAME
        
    def recommend_projects(self, donor_id, projects_to_ignore=[], topn=10):
        #Getting the top-1000 Content-based filtering recommendations
        cb_recs_df = self.cb_rec_model.recommend_projects(donor_id, projects_to_ignore=projects_to_ignore, 
                                                           topn=1000).rename(columns={&#39;recStrength&#39;: &#39;recStrengthCB&#39;})
        
        #Getting the top-1000 Collaborative filtering recommendations
        cf_recs_df = self.cf_rec_model.recommend_projects(donor_id, projects_to_ignore=projects_to_ignore,  
                                                           topn=1000).rename(columns={&#39;recStrength&#39;: &#39;recStrengthCF&#39;})
        
        #Combining the results by Project ID
        recs_df = cb_recs_df.merge(cf_recs_df,
                                   how = &#39;inner&#39;, 
                                   left_on = &#39;Project ID&#39;, 
                                   right_on = &#39;Project ID&#39;)
        
        #Computing a hybrid recommendation score based on CF and CB scores
        recs_df[&#39;recStrengthHybrid&#39;] = recs_df[&#39;recStrengthCB&#39;] * recs_df[&#39;recStrengthCF&#39;]
        
        #Sorting recommendations by hybrid score
        recommendations_df = recs_df.sort_values(&#39;recStrengthHybrid&#39;, ascending=False).head(topn)

        recommendations_df = recommendations_df.merge(self.projects_df, how = &#39;left&#39;, 
                                                    left_on = &#39;Project ID&#39;, 
                                                    right_on = &#39;Project ID&#39;)[[&#39;recStrengthHybrid&#39;, 
                                                                              &#39;Project ID&#39;, &#39;Project Title&#39;, 
                                                                              &#39;Project Essay&#39;]]


        return recommendations_df
    
hybrid_model = HybridRecommender(cbr_model, cfr_model, projects)


print(hybrid_model.recommend_projects(mydonor1))

print(hybrid_model.recommend_projects(mydonor2))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   recStrengthHybrid                        ...                                                              Project Essay
0       1.574375e-18                        ...                          we are trying to engage more students in scien...
1       1.221807e-18                        ...                          in my school 50% of the students are socioecon...
2       1.214293e-18                        ...                          our students are some of the hardest working k...
3       4.037232e-19                        ...                          sitting at a desk for a sustained period of ti...
4       6.661794e-20                        ...                          “music expresses that which cannot be put into...
5       4.872264e-20                        ...                          i walk in the door so excited to get the stude...
6       4.410098e-20                        ...                          i have spent 12 years as an educator rebuildin...
7       2.907349e-20                        ...                          &amp;quot;music is what feelings sound like.&amp;quot; -g. cates...
8       2.121616e-20                        ...                          &amp;quot;i cannot say good-bye to those whom i have gr...
9       1.353927e-20                        ...                          our band program is one of the largest in our ...

[10 rows x 4 columns]
   recStrengthHybrid                        ...                                                              Project Essay
0       2.811124e-18                        ...                          in this modern, digital age, i would like to u...
1       1.249967e-18                        ...                          we are a brand new charter school that has onl...
2       6.055628e-19                        ...                          my students are african american and hispanic....
3       5.912367e-19                        ...                          the a. community and its students are a very s...
4       2.541749e-19                        ...                          do you want to go on an adventure and learn ab...
5       2.494812e-19                        ...                          the average day in my class involves students ...
6       2.323313e-19                        ...                          i teach ela (reading component) to self-contai...
7       1.271629e-19                        ...                          hi there! do you want to help to instill a lif...
8       1.044990e-19                        ...                          having writing utensils is essential for stude...
9       1.004780e-19                        ...                          there&#39;s no such thing as a kid who hates readi...

[10 rows x 4 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả trả ra tốt hơn nhiều so với cách 2, donor1 có music, donor2 có cây trồng và sách.&lt;/p&gt;

&lt;h4 id=&#34;5-đánh-giá-mô-hình&#34;&gt;5. Đánh giá mô hình&lt;/h4&gt;

&lt;p&gt;Có rất nhiều cách khác nhau để đánh giá mô hình recommend system. Một trong các cách mình sử dụng ở đây là sử dụng độ đo top K accuracy. Độ đo này được tính như sau:&lt;/p&gt;

&lt;p&gt;Với mỗi user:
    Với mỗi item user đã pick trong test set
        Lấy mẫu 1000 item khác mà người dùng chưa bao giờ pick&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo. Cố lên.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Một số mẹo để lựa chọn mô hình object detection</title>
      <link>/blog/2018-12-11-a-bunch-of-tips-and-tricks-for-training-deep-neural-networks/</link>
      <pubDate>Tue, 11 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-11-a-bunch-of-tips-and-tricks-for-training-deep-neural-networks/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Việc huấn luyện một mô hình neural network khá đơn giản, chỉ việc download code mẫu về, quăng tập data của mình vào, rồi cho chạy, xong. Nhưng khó khăn ở đây là làm cách nào để nâng độ chính xác của mô hình lên. Ở bài viết này, chúng ta sẽ tìm hiểu một số cách giúp tăng độ chính xác của mô hình.&lt;/p&gt;

&lt;h3 id=&#34;kiểm-tra-dữ-liệu&#34;&gt;Kiểm tra dữ liệu&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Thực chất, chúng ta phải hiểu rõ kỹ chúng ta đang có những gì trong tay, thì chúng ta mới dạy cho máy học đủ và đúng được. Các bạn hãy kiểm tra thật kỹ để đảm bảo rằng tập nhãn được gán chính xác, bouding box của đối tượng được vẽ không quá dư thừa, không có missing value, v.v. Một ví dụ nhỏ là tập MNIST, có nhiều hình bị nhập nhằng giữa những con số, chúng ta không thể phân biệt được chính xác hình đó là con số nào bằng mắt thường.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tiếp theo, các bạn hãy quyết định xem rằng mình có nên sử dụng các pre-train model hay không.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Nếu tập dữ liệu của bạn gần giống với tập dữ liệu ImageNet, hãy dùng pre-train model. Có các mô hình đã được huấn luyện sẵn là VGG net, ResNet, DenseNet, Xception. Với các kiến trúc khác nhau như VGG(16 và 19 layer), ResNet (50, 101, 152 layer), DenseNet(201,169,121 layer). Ban đầu, đừng sử dụng các kiến trúc có số lượng nhiều (ResNet152, DenseNet201) bởi vì nó rất tốn chi phí tính toán. Chúng ta nên bắt đầu bởi các mô hình nhỏ như VGG16, ResNet50. Hãy chọn một mô hình mà bạn nghĩ là sẽ có kết quả tốt. Sau khi huấn luyện, nếu kết quả không được như ý muốn, hãy tăng số lớp lên (ví dụ ban đầu chọn Resnet50, sau đó nâng lên Resnet101, &amp;hellip;).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu bạn có ít dữ liệu, bạn nãy &amp;ldquo;đóng băng&amp;rdquo; lại trọng số của pre-train model, chỉ huấn luyện phần phân lớp. Bạn cũng có thể thêm phần Dropout để tránh overfit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu tập dữ liệu của bạn không giống một tí nào so với taapk ImageNet, không nên dùng pre-train model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Luôn luôn sử dụng lớp chuẩn hoá trong mô hình. Nếu bạn huấn luyện mô hình với batch-size lớn ( ví dụ lớn hơn 10), hãy sử dụng BatchNormalization Layer trong keras. Nếu bạn sử dụng batch-size nhỏ (ví dụ 1), thì hãy sử dụng InstanceNormalization. Hai layer này đã có sẵn trong Keras, trong các framework khác thì mình không rõ lắm. Có nhiều tác giả đã chỉ ra rằng sử dụng BatchNormalization  sẽ cho kết quả tốt hơn nếu tăng batch-size và hiệu năng sẽ giảm khi batch-size nhỏ, và trong trường hợp batch-size nhỏ thì kết quả sẽ tốt hơn một tí khi sử dụng InstanceNormalization thay cho BatchNormalization. Ngoài ra, các bạn cũng có thể sử dụng GroupNormalization (mình chưa kiểm chứng GroupNormalization có làm tăng độ chính xác hay không).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu bạn sử dụng concatenation layer để kết hợp các feature từ nhiều convolution layers (Li), và những Li  trên rút trích thông tin từ cùng một input (F), thì bạn jay sử dụng SpatialDropout ngay sau concatenation layer trên (Xem hình bên dưới). Khi các convolution layer rút trích thông tin từ cùng một nguồn, các đặc trưng của chúng thường sẽ có mức tương quan với nhau rất lớn. SpatialDropout sẽ loại bỏ những đặc trưng có mức độ liên quan cao này và giúp bạn chống lại hiện tượng overfiting. Thông thường người ta chỉ sử dụng SpatialDropout ở các lớp gần input layer, và không sử dụng chúng ở các lớp cao bên trên.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/spatialdropoutusecase.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Theo andrej Karpathy, để xác định khả năng lưu trữ thông tin của mô hình, hãy rút một phần nhỏ dữ liệu trong tập train của bạn đem đi huấn luyện. Nếu mô hình không overfit, chúng ta tăng số lượng node/layer lên. Nếu mô hình bị overfit, sử dụng các kỹ thuật như L1, L2, Dropout hoăc các kỹ thuật khác để chống lại việc overfit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Các kỹ thuật chuẩn hoá thường sẽ ràng buộc hoặc tinh gọn các trọng số của mô hình. Nó cũng đồng thời giúp chúng ta chống lại việc gradient explosion (gradient mang giá trị lớn khi tính backpropagation) (lý do là các trọng số sẽ bị giới hạn trong đoạn nào đó, ví dụ L2 giới hạn căn bậc 2 tổng bình phương các trọng số =1 chẳng hạn). Ví dụ dưới sử dụng kares và giới hạn max của L2 là 2.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.constraints import max_norm
# add to Dense layers
model.add(Dense(64, kernel_constraint=max_norm(2.)))
# or add to Conv layers
model.add(Conv2D(64, kernel_constraint=max_norm(2.)))
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Việc sử dụng mean subtraction đôi khi cho kết quả khá tệ, đặc biệt là khi sử dụng trong ảnh xám (grayscale image), hoặc các bài toán phân đoạn ảnh.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Luôn nhớ đến việc xáo trộn dữ liệu (nếu bạn có thể). Nếu được, hãy thực hiện xáo trộn dữ liệu trong quá trình huấn luyện. Việc xáo trộn ảnh sẽ giúp bạn cải thiện độ chính xác.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu bài toán của bạn thuộc nhóm dense prediction (ví dụ phân đoạn ngữ nghĩa - semantic segmentation). Hãy sử dụng pre-train model là Dilated Residual Networks. Mô hình trên cực kỳ hiệu quả cho bài toán này.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Để xác định thông tin ngữ cảnh xung quanh các đối tượng, hãy sử dụng module multi-scale feature pooling. Module này sẽ giúp bạn tăng độ chính xác và thường được sử dụng trong bài toán phân đoạn ngữ nghĩa (semantic segmentation) hoặc bài toán phân đoạn nền (foreground segmentation).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Khi bạn tính độ lỗi hoặc độ chính xác, nếu có vùng nào không trả về nhãn, hoặc nhãn trả về không chắc chắn, hãy bỏ qua việc tính toán chúng đi. Hành động này sẽ giúp mô hình của bạn chắc chắn hơn khi đưa ra quyết định.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng trọng số cho từng class trong quá trình training nếu dữ liệu của bạn có tính bất cân bằng cao. Hãy đặt trọng số lớn cho những lớp có ít dữ liệu, và trọng số nhỏ cho những lớp có nhiều dữ liệu. Trọng số của các lớp có thể được tính toán một cách dễ dàng bằng các sử dụng thư viện skearn trong python. Ngoài ra, bạn có thể sử dụng các kỹ thuật như OverSampling hoặc UnderSampling đối với tập dữ liệu nhỏ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Chọn đúng hàm tối ưu. Có rất nhiều hàm tối ưu như Adam, Adagrad, Adadellta, RMSprop, &amp;hellip; Trong các paper người ta thường sử dụng tổ hợp SGD + momentun. Có hai vấn đề cần được xem xét ở đây: Một là nếu bạn muốn mô hình có độ hội tụ nhanh, hãy dùng Adam ( và có khả năng cao là mô hình sẽ bị kẹt ở điểm cực tiểu cục bộ -&amp;gt; không có tính tổng quát hoá cao). Hai là sử dujg SGD + momentun để tìm cực tiểu toàn cục, mô hình này phụ thuộc rất nhiều vào giá trị khởi tạo ban đầu và mô hình thường sẽ hội tụ rất chậm. (Xem hình bên dưới)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/optimal.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Thông thường, chúng ta sẽ chọn learning-rate là (1e-1, 1e-3, 1e-6). Nếu bạn sử dụng pre-train model, hãy sử dụng learning rate nhỏ hơn 1e-3 (ví dụ 1e-4). Nếu bạn không sử dụng pre-train model, hãy sử dụng learning-rate lớn hơn 1e-3. Bạn có thể grid search giá trị learning-rate và chọn ra mô hình cho kết quả tốt nhất. Bạn có thể sử dụng Learing Rate Schedulers giảm giá trịn learning rate trong quá trình huấn luyện mô hình.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bên cạnh việc sử dụng Learing Rate Schedulers để giảm giá trị learning rate, bạn có thể sử dụng một kỹ thuật khác để giảm giá trị learning-rate. Ví dụ sau 5 epochs, độ lỗi trên tập validation không thay đổi, bạn giảm learning-rate đi 10 lần (vd từ 1e-3 thành 1e-4). Trong keras, bạn có thể dễ dàng implement công thức trên bằng việc sử dụng callbacs ReduceLROnPlateau.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reduce = keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.1, patience=5, mode=&#39;auto&#39;)
early = keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=1e-4, patience=10, mode=&#39;auto&#39;)
model.fit(X, Y, callbacks=[reduce, early])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ví dụ trên, chúng ta sẽ giảm learning-rate đi 10 lần khi độ lỗi trên tập validation không thay đổi qua 5 lần lặp liên tiếp, và sẽ dừng việc huấn luyện khi độ lỗi không giảm qua 10 lần lặp liên tiếp.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Nếu bài toán của bạn thuộc nhóm dense prediction như phân đoạn ảnh, phân đoạn ngữ nghĩa, bạn nên sử dụng skip connection để chống lại việc các biên của đối tượng hoặc các thông tin đặc trưng hữu ích của đối tượng bị mất trong max-pooling hoặc strided convolution. Skip connection cũng giúp mô hình học features map từ feature space và image space dễ dàng hơn, và nó cũng giúp cho bạn giảm bị vanish gradient ( giá trị gradient nhỏ dần và gần xấp xỉ bằng 0, nên trọng số không thay đổi nhiều, dẫn đến không hội tụ).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nên sử dụng data augmentation, như là horizontally flipping, rotating, zoom-croping&amp;hellip; để tăng dữ liệu của bạn lên. Việc có nhiều dữ liệu sẽ giúp mô hình có mức tổng quát hoá cao hơn.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng Max-pooling trước Relu để giảm thiểu mức độ tính toán thay vì làm ngược lại. chúng ta biết rằng ReLU trả ra giá trị có ngưỡng cực tiểu là 0 do f(x)=max(0,x), và max-pooling tính max cho các đặc trưng f(x) = max(x1,x2,&amp;hellip;,xi). Nếu ta sử dụng &lt;em&gt;Conv &amp;gt; ReLU &amp;gt; Max-pooling&lt;/em&gt;, ta sẽ tốn i lần tính ReLu, và 1 lần tính max. Nếu ta sử dụng &lt;em&gt;Conv -&amp;gt; max-pooling &amp;gt; ReLU&lt;/em&gt;, ta tốn 1 lần tính max, 1 lần tính ReLU.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nếu có thể, hãy thử sử dụng Depthwise Separable Convolution. Nó giúp mô hình giảm số lượng tham số so với các convolution khác, ngoài ra nó giúp mô hình chạy nhanh hơn.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Điều cuối cùng là đừng bao giờ từ bỏ. Hãy tin tưởng rằng bạn có thể làm được. Nếu bạn vẫn không thể đạt được độ chính xác như mong đợi, hãy điều chỉnh lại các tham số, kiến trúc mô hình, tập dữ liệu huấn luyện đến khi bạn đạt được mô hình với độ chính xác như bạn đề ra.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo. Cố lên.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lựa chọn mô hình object detectors</title>
      <link>/blog/2018-12-10-design-choices-lessons-learned-and-trends-for-object-detections/</link>
      <pubDate>Mon, 10 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-10-design-choices-lessons-learned-and-trends-for-object-detections/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Các thuật toán phát hiện đối tượng, như các thuật toán thuộc nhóm region proposal hoặc single shot đầu bắt đầu bởi những ý tưởng khác nhau, nhưng sau qua một vài quá trình cập nhật và nâng cấp cho đến thời điểm hiện tại, mô hình chung của chúng đã gần gần giống nhau hơn. Và hai thuật toán trên là hai thuật toán tiêu biểu cạnh tranh nhau danh hiệu thuật toán phát hiện đối tượng nhanh nhất và thuật toán nhận diện chính xác nhất.
Trong bài viết này, chúng ta sẽ đề cập đến một số chiến lược lựa chọn mô hình cho bài toán object detector và một số benchmarks do team Google Research thực hiện.&lt;/p&gt;

&lt;h2 id=&#34;box-encoding-và-loss-function&#34;&gt;Box encoding và loss function&lt;/h2&gt;

&lt;p&gt;Có rất nhiều hàm lỗi và box encoding được sử dụng trong các thuật toán phát hiện đối tượng. Ví dụ, SSD trả ra căn bậc hai của Width và height để giảm tỷ lệ độ lỗi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/box_encoding_architerch.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Các bạn có thể để ý kỹ hơn SSD phiên bản custom không sử dụng cặp toạ độ trái trên - phải dưới mà là cặp tâm - căn bậc hai của with, căn bậc hai của height. Một số thuật toán lại dùng log width, log height, một số lại dùng tâm là Wc/Wa, Wy/ha, với Wc và Wy là toạ độ tâm của đối tượng, wa và ha là chiều dài và rộng của anchor khớp nhất (matching anchor). Các bạn có thể tham khảo thêm ở &lt;a href=&#34;https://arxiv.org/pdf/1611.10012.pdf&#34;&gt;https://arxiv.org/pdf/1611.10012.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Để huấn luyện mô hình tốt hơn, Các nhà nghiên cứu sử dụng các trọng số khác nhau cho các hàm lỗi, YOLO và một ví dụ minh hoạ.&lt;/p&gt;

&lt;h2 id=&#34;feature-extraction&#34;&gt;Feature extraction&lt;/h2&gt;

&lt;p&gt;Trong thực tế, Feature extraction ảnh hưởng lớn trên 2 phần tradeoff là độ chính xác và tốc độ. Nhóm thuật toán ResNet và Inception đi theo tiêu chí là độ chính xác quan trọng hơn tốc độ (và quả thật nhóm thuật toán thuộc họ này có độ chính xác khá cao). MobileNet cung cấp cho chúng ta một mô hình khá nhỏ gọn, sử dụng SSD, mục tiêu của nhóm này là có thể xử lý được trên các thiết bị di động và thời gian xử lý là realtime.&lt;/p&gt;

&lt;h2 id=&#34;feature-extractor-accuracy&#34;&gt;Feature extractor accuracy&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/feature_extraction_accuracy.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn vào hình trên, chúng ta có thể thấy rõ ràng rằng Faster R-CNN và R-FCN đều cho độ chính xác khá tốt trên feature extraction. Ngược lại SSD có kết quả khá tệ.&lt;/p&gt;

&lt;h2 id=&#34;non-max-suppression-nms&#34;&gt;Non-max suppression (nms)&lt;/h2&gt;

&lt;p&gt;Sau khi thu được vị trí của các đối tượng, chúng ta sẽ merge lại các vị trí bị phát hiện trùng lắp. Các thuật toán thuộc nhóm single shot thường cho ra output overlap khá nhiều.&lt;/p&gt;

&lt;h2 id=&#34;data-augmentation&#34;&gt;Data augmentation&lt;/h2&gt;

&lt;p&gt;Ngày nay, hầu hết các thuật toán đều sử dụng Data augmentation. Việc augment data bằng cách cắt xét ảnh, quay ảnh một góc ngẫu nhiên nào đó, giúp cho tránh được overfit trong quá trình huấn luyện, do đó gián tiếp tăng độ chính xác của mô hình.&lt;/p&gt;

&lt;h2 id=&#34;feature-map-strides&#34;&gt;Feature map strides&lt;/h2&gt;

&lt;p&gt;Thuật toán thuộc nhóm single shot thường có tuỳ chọn layter feature map nào được sử dụng để nhận dạng đối tượng. Feature map có stride là 2 nếu chúng ta thực hiện giảm 2 lần độ phân giải. Feature map có độ phân giải thấp thường giữ lại những thông tin đặc trưng tốt của đối tượng và giúp cho detector thực hiện tốt hơn. Tuy nhiên, những đối tượng có kính thước nhỏ sẽ bị mất thông tin trầm trọng và khó để phát hiện ra chúng.&lt;/p&gt;

&lt;h2 id=&#34;speed-v-s-accuracy&#34;&gt;Speed v.s. accuracy&lt;/h2&gt;

&lt;p&gt;Thật khó để trả lời rằng thuật toán nhận dạng đối tượng nào tốt hơn, mà câu trả lời phụ thuộc vào bài toán của bạn đang gặp. Nếu bài toán cần độ chính xác cao, hãy sử dụng ResNet hoặc Inception, nếu bạn cần chạy realtime và độ chính xác tạm chấp nhận, hãy sử dụng MobileNet hoặc YOLO. Không có (chưa có - ít nhất đến thời điểm hiện tại) có thuật toán nào đáp ứng cả 2 tiêu chí là vừa có độ chính xác cao, vừa chạy nhanh cả. Đó là một tradeoff giữa Speed và Accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/gpu-time-resnet-inception-mobilenet.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;object-size&#34;&gt;Object size&lt;/h2&gt;

&lt;p&gt;Với những hình ảnh có kích thước lớn, SSD thực hiện rút trích đặc trưng rất tốt (nên nhớ rằng mô hình rút trích đặc trưng của SSD rất đơn giản). Với những hình ảnh dạng này, SSD có thể so sánh với các thuật toán khác khác về độ chính xác.&lt;/p&gt;

&lt;p&gt;Với nhưng hình ảnh có kích thước nhỏ, chúng ta không nên/không bao giờ xài SSD.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/object_size_compatiple.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn hình ở trên, chúng ta thấy rõ độ chính xác của SSD và các thuật oán khác trên các tập dữ liệu có kích thước khác nhau. Và phụ thuộc vào kích thước dữ liệu của bạn để chọn ra mô hình tối ưu nhất.&lt;/p&gt;

&lt;h2 id=&#34;input-image-resolution&#34;&gt;Input image resolution&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/resolution_reduce.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn hình trên các bạn cũng có thể nhìn thấy rõ. Ảnh có độ phân giải lớn giúp nhận dạng đối tượng tốt hơn rất nhiều so với ảnh có độ phân giải nhỏ. Khi giảm 2 lần độ phân giải trên mỗi chiều (từ 600x600 xuống còn 300x300), trung bình độ chính xác giảm 15.88% trong quá trình huấn luyện, và trung bình giảm 27.4% trong inference.&lt;/p&gt;

&lt;h2 id=&#34;number-of-proposals&#34;&gt;Number of proposals&lt;/h2&gt;

&lt;p&gt;Số lượng proposal được sinh ra ảnh hưởng trực tiếp đến tốc độ của nhóm R-CNN. Ví dụ, Faster R-CNN có thể tăng tốc độ nhận dạng đối tượng gấp 3 lần nếu ta chỉ sử dụng 50 proposal thay vì 300 proposal. Độ chính xác chỉ giảm 4%&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/number-proposal-f-rcnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hình trên, đường nét liền mô tả độ chính xác khi tăng số lượng proposal. Đường nét đứt thể hiện thời gian xử láy tăng khi tăng số lượng proposal.&lt;/p&gt;

&lt;h2 id=&#34;điểm-danh-danh-lại-các-bước-phát-triển-của-object-detection&#34;&gt;Điểm danh danh lại các bước phát triển của object detection&lt;/h2&gt;

&lt;p&gt;Các thuật toán object detection đã phát triển trong một khoảng thời gian dài. Ý tưởng  đầu tiên, đơn giản nhất là chúng ta sẽ sử dụng cửa sổ trượt.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Sliding windows
for window in windows
    patch = get_patch(image, window)
    results = detector(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để tăng tốc, chúng ta sẽ
1. Giảm số lượng windows (R-CNN giảm còn khoảng 2000)
2. Giảm các phép tính trong việc tìm ROI (Fast R-CNN sử dụng feature map thay vì toàn bộ image patchs).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fast R-CNN
feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Việc tìm region_proposal cũng tốn khá nhiều thời gian. Faster R-CNN sử dụng một convolution network thay thế cho region proposal ở bước này (làm giảm thời gian từ 2.3s xuống còn 0.3 giây). Faster R-CNN cũng giới thiệu 1 khái nhiệm là anchor giúp cải thiện độ chính xác và việc huấn luyện trở nên dễ dàng hơn.&lt;/p&gt;

&lt;p&gt;R-FCN đưa ra một điều chỉnh nhỏ, là tiến hành tìm position và sensitive score map trên mỗi ROIS độc lập. Và tính trung bình xác suất xuất hiện đối tượng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# R-FCN
feature_maps = process(image)
ROIs = region_proposal(feature_maps)         
score_maps = compute_score_map(feature_maps)
for ROI in ROIs
    V = pool(score_maps, ROI)     
    class_scores = average(V)         
    class_probabilities = softmax(class_scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R-FCN chạy khá nhanh, nhưng độ chính xác thì thấp hơn một hút so với Faster R-CNN. Để ý kỹ đoạn mã giả ở trên, chúng ta phải trải qua 2 lần tính toán, một lần là tìm các ROIs, một lần là object detection. Thuật toán Single shot detector được đề xuất để sử dụng 1 lần tính toán.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
results = detector3(feature_maps) # No more separate step for ROIs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thuật toán SSD và YOLO đều thuộc nhóm single shot detectors. Cả hai đều sử dụng convolution layer để rút trích đặc trưng và một convolution filter để đưa quyết định. Cả hai đều dùng feature map có độ phân giải thấp (low resolution feature map) để dò tìm đối tượng =&amp;gt; chỉ phát hiện được các đối tượng có kích thước lớn. Một cách tiếp cận là sử dụng các feature map có độ phân giải cao (higher resolution feature map). Nhưng độ chính xác sẽ giảm do thông tin đặc trưng của đối tượng quá hỗn loạn. FPN đưa ra ý tưởng sử dụng feature map trung gian merge giữa feature map high resolution và low resolution. Việc này giúp cho chúng ta vẫn giữ được thông tin đặc trưng hữu ích của đối tượng, đồng thời cũng giữ được thông tin của các đối tượng có kích thước nhỏ. Do đó, độ chính xác cũng tăng lên và phát hiện các đối tượng có các tỷ lệ khác nhau (different scale) tốt hơn.&lt;/p&gt;

&lt;p&gt;Trong quá trình huấn luyện, chúng ta sẽ nhận ra 1 vấn đề rằng backgroup sẽ chiếm 1 phần rất lớn trong bức ảnh. Hoặc một đối tượng nào đó có số mẫu nhiều hơn so với các đối tượng khác. Thuật toán Focal loss được sinh ra để giải quyết vấn đề này.&lt;/p&gt;

&lt;h2 id=&#34;lesson-learned&#34;&gt;Lesson learned&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Feature Pyramid Networks sử dụng các feature map nhiều thông tin hơn để cải thiện độ chính xác.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng các mô hình như ResNet hoặc Inception ResNet nếu mô hình bạn cần độ chính xác và không quan tâm lắm về tốc độ.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng các thuật toán thuộc nhóm Single shot detectors như MobileNet nếu bạn cần tốc độ tính toán và có thể chạy được trên mobilenet, yêu cầu về độ chính xác tạm chấp nhận được.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng batch normaliation, nói chung là đều phải chuẩn hoá dữ liệu trước khi sử dụng.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lựa chọn anchors cẩn thận (Cái này khá khó, đòi hỏi bạn phải am hiểu khá kỹ về dữ liệu, và nếu set nhầm thì sẽ đi tong).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sử dụng data augmentation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch và tham khảo từ nguồn &lt;a href=&#34;https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff&#34;&gt;https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu single shot object detectors</title>
      <link>/blog/2018-12-06-what-do-we-learn-from-single-shot-object-detection/</link>
      <pubDate>Thu, 06 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-06-what-do-we-learn-from-single-shot-object-detection/</guid>
      <description>

&lt;h2 id=&#34;single-shot-detectors&#34;&gt;Single Shot detectors&lt;/h2&gt;

&lt;p&gt;Ở bài trước, chúng ta đã tìm hiểu về region proposal và ứng dụng của nó vào Faster R-CNN. Các thuật toán thuộc nhóm region proposal tuy cho kết quả có độ chính xác cao, nhưng chúng có một nhược điểm rất lớn là thời gian huấn luyện và đưa quyết định rất chậm. Faster R-CNN xử lý khoảng 7 &lt;em&gt;FPS&lt;/em&gt; trên tập dữ liệu PASCAL VOC 2007. Một cách để tăng tốc quá trình tính toán là giảm số lượng tính toán trên mỗi ROI.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_align(feature_maps, ROI)
    results = detector2(patch)    # Giảm khối lượng tính toán ở đây
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một ý tưởng khác, là chúng ta sẽ bỏ qua bước tìm region proposal, mà trực tiếp rút trích boundary boxes và classes trực tiếp từ feature map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
results = detector3(feature_maps) # Không cần tìm ROI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dựa trên ý tưởng sử dụng cửa sổ trượt. Chúng ta sẽ trượt trên feature máp để nhận diện các đối tượng. Với mỗi loại đối tượng khác nhau, chúng ta sửa dụng các cửa sổ trượt có kích thước khác nhau. Cách này thoạt đầu trông có vẻ khá tốt, nhưng điểm yếu của nó là đã sử dụng cửa sổ trượt làm final boundary box. Do đó, giả sử chúng ta có nhiều đối tượng, và mỗi đối tượng có kích thước khác nhau, chúng ta sẽ có rất nhiều cửa sổ trượt để bao phủ hết toàn bộ đối tượng.&lt;/p&gt;

&lt;p&gt;Một ý tưởng cải tiến là chúng ta sẽ định nghĩa trước các cửa sổ trượt, sau đó sẽ tiến hành dự đoán lớp và boundary box ( và Ý tưởng này, nhóm nghiên cứu phát triển thuật toán và đặt tên thuật toán là single shot detectors). Ý tưởng này tương tự như việc sử dụng anchors trong Faster R-CNN, nhưng single shot detectors thực hiện dự đoán boundary box và class đồng thời cùng nhau.&lt;/p&gt;

&lt;p&gt;Ví dụ, giả sử chúng ta có một feature map 8x8 và chúng ta đưa ra k = 4 dự đoán.  Vậy ta có tổng cộng 8x8x4 = 256 dự đoán.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-img-1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Xét hình bên trên, ta có 4 anchors đã được định nghĩa trước ( màu xanh lá cây), và có 4 prediction( màu xanh nước biển) tương ứng với từng anchor trên.&lt;/p&gt;

&lt;p&gt;Với thuật toán Faster R-CNN, chúng ta sử dụng một convolution filter trả ra 5 kết quả dự đoán: 4 giá trị là toạ độ của boundary box, và giá trị còn lại là xác suất xuất hiện đối tượng. Tổng quát hơn, ta có input là D feature map 8x8, output là 8x8x5, số convolution filter trong Faster R-CNN là 3x3xDx8.&lt;/p&gt;

&lt;p&gt;Với single shot detector, input của ta cũng tương tự là 8x8xD, output là 8x8x (4 + C) ( với 4 tương ứng với 4 điểm boundary box, và C là số lượng lớp đối tượng), vậy ta cần một convolution filter là 3x3xDx(4+C)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-architech.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thuật toán Single shot detect chạy khá nhanh, nhưng độ chính xác của nó không cao lắm (không bằng region proposal). Thuật toán có vấn đề về việc nhận dạng các đối tượng có kích thước nhỏ. Ví dụ như hình bên dưới, chúng ta có tổng cộng 9 ông già noel, nhưng thuật toán chỉ nhận diện được có 5 ông.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-img-2.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;ssd&#34;&gt;SSD&lt;/h2&gt;

&lt;p&gt;SSD là mô hình single shot detector sử dụng mạng VGG16 để rút trích đặc trưng. Mô hình như hình bên dưới. Trong đó, những conv có màu xanh nước biển nhạt là những custom convolution layter (ta có thể thêm bớt bao nhiêu tuỳ thích). Convolutional filter layter (là cục màu xanh lá cây) có nhiệm vụ tổng hợp các thông tin lại để đưa quyết định.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-vgg19-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Khi sử dụng mô hình như hình ở trên, chúng ta thấy rằng các custom convolution layter có nhiệm vụ làm giảm chiều và giảm độ phân giải của bức ảnh. Cho nên, mô hình chỉ có khả năng nhận ra các đối tượng có kích thước lớn. Để giải quyết vấn đề này, chúng ta sẽ sử dụng các object detector khác nhau trên mỗi feature maps (xem output của mỗi custom convolution là một feature map).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-vgg19-model1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ảnh bên dưới là sơ đồ số chiều của các feature maps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-vgg19-diagram.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SSD sử dụng các layter có kích thước giảm dần theo độ sâu để nhận dạng đối tượng. Nhìn vào hình vẽ sơ đồ bên dưới của SSD, chúng ra dễ dàng nhận thấy rằng độ phân giải giảm đáng kể qua mỗi layer và có lẽ (chắc chắn) sẽ bỏ sót những đối tượng có kích thước nhỏ ở những lớp có độ phân giải thấp. Nếu trong dự án thực tế của bạn có xảy ra vấn đề này, bạn nên tăng độ phân giải của ảnh đầu vào.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-SSD1-diagram.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;yolo&#34;&gt;YOLO&lt;/h2&gt;

&lt;p&gt;YOLO cũng là một thuật toán sử dụng single shot detector để dò tìm vị trí của các đối tượng trong ảnh. YOLO sử dụng DarkNet để tạo các feature cho bức ảnh (SSD sử dụng VGG16). Mô hình của YOLLO như ảnh ở bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-darknet-diagram.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Khác với kiến trúc mạng SSD ở trên, YOLLO không sử dụng multiple scale feature map (SSD sử dụng các custom convolution layter, qua mỗi layter thì feature maps sẽ có kích thước giảm xuống - các output của custom convolution layer chính là các feature map chúng ta thu được). Thay vào đó, YOLLO sẽ làm phẳng hoá (flatten - vd ma trận 3x3  sẽ biến thành vector 1x9, ma trận 4x5 sẽ biến thành vector 1x20 &amp;hellip;, làm phẳng nghĩa là chúng ta sẽ không dùng bộ lọc nào hết, mà sử dụng các phép biến đổi, nên không làm thay đổi giá trị, chỉ làm thay đổi hình dạng) một phần output của convolution layer và kết hợp với  convolution layer ở trong DarkNet tạo thành feature map (Xem hình ở trên sẽ rõ hơn). Ví dụ ở custom convolution layer chúng ta thu được output có kích thước 28x28x512, chúng ta sẽ flatten thành layter có kích thước 14x14x2048, kết hợp với 1 layter có kích thước 14x14x1024 ở trong darknet, chúng ta thu được feature maps có kích thước là 14x14x3072. Đem feature maps này đi đự đoán.&lt;/p&gt;

&lt;p&gt;YOLOv2 đã thêm vào rất nhiều các cải tiền để cải tăng mAP từ 63.4 trong mô hình đầu tiên (YOLOv1) lên 78.6. Các cải tiền bao gồm thêm batch norm, anchor boxes,  hi-res classifier &amp;hellip; Các bạn có thể xem ở hình bên dưới. YOLO9000 có thể nhận dạng 9000 đối tượng khác nhau.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/yollo-v2-improment.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;YOLOv2 có thể nhận diện các đối tượng với ảnh đầu vào có độ phân giải bất kỳ. Với ảnh có độ phân giải thấp thì mô hình chạy khá nhanh, có FPS cao nhưng mAP lại thấp (tradeoff giữa FPS và mAP).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/yollo-v2-acc.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;yolov3&#34;&gt;YOLOv3&lt;/h2&gt;

&lt;p&gt;YOLOv3 sử dụng darknet với kiến trúc phức hơn để rút trích đặc trưng của bức ảnh. YOLOv3 thêm vào đặc trưng Pyramid để dò tìm các đối tượng có kích thước nhỏ.&lt;/p&gt;

&lt;p&gt;Hình bên dưới so sánh tradeoff giữa thời gian thực thi và độ chính xác giữa các mô hình. Ta thấy rằng thời gian thực thi của YOLOv3 rất nhanh, cùng phân mức mAP 28.8, thời gian YOLOv3 thực thi chỉ tốn 22ms, trong khi đó SSD321 tốn đến 61ms - gấp 3 lần.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/single-shot-object-detectors-compare.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;feature-pyramid-networks-fpn&#34;&gt;Feature Pyramid Networks (FPN)&lt;/h2&gt;

&lt;p&gt;Dò tìm các đối tượng có kích thước nhỏ là một vấn đề đáng được giải quyết để nâng cao độ chính xác. Và FPN là mô hình mạng được thiết kế ra dựa trên khái niệm pyramid để giải quyết vấn đề này.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/feature-pyramid-network-model1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mô hình FPN kết hợp thông tin của mô hình theo hướng &lt;em&gt;bottom-up&lt;/em&gt; kết hợp với &lt;em&gt;top-down&lt;/em&gt; để dò tìm đối tượng (trong khi đó, các thuật toán khác chỉ thường sử dụng &lt;em&gt;bottom-up&lt;/em&gt;). Khi chúng ta ở bottom và đi lên (up), độ phân giải sẽ giảm, nhưng giá trị ngữ nghĩa sẽ tăng lên. Xem hình mô phỏng bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/feature-pyramid-network-model2.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SSD đưa ra quyết định dựa vào nhiều feature map. Nhưng layer ở bottom không được sử dụng để nhận dạng đối tượng. Vì những layter này có độ phân giải cao nhưng giá trị ngữ nghĩa của chúng lại không đủ cao (thấp) nên những nhà nghiên cứu bỏ chúng đi để tăng tốc độ xử lý. Các nhà nghiêng cứu biện minh rằng các layer ở bottom chưa đủ mức ý nghĩa cần thiết để nâng cao độ chính xác, thêm các layer đó vào sẽ không nâng độ chính xác cao thêm bao nhiêu và họ bỏ chúng đi để có tốc độ tốt hơn. Cho nên, SSD chỉ sử dụng các layer ở lớp trên , và do đó sẽ không nhận dạng được các đối tượng có kích thước nhỏ.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/ssd-model-bottom-up.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Trong khi đó, FPN xây dựng thêm mô hình top-down, nhằm mục đích xây dựng các layer có độ phân giải cao từ các layer có ngữ nghĩa cao.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-top-down-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Trong quá trình xây dựng lại các layer từ top xuống bottom, chúng ta sẽ gặp một vấn đề khá nghiêm trọng là bị mất mát thông tin của các đối tượng. Ví dụ một đối tượng nhỏ khi lên top sẽ không thấy nó, và từ top đi ngược lại sẽ không thể tái tạo lại đối tượng nhỏ đó. Để giải quyết vấn đề này, chúng ta sẽ tạo các kết nối (skip connection) giữa các reconstruction layter và các feature map để giúp quá trình detector dự đoán các vị trí của đối tượng thực hiện tốt hơn (hạn chế tốt nhất việc mất mát thông tin).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-top-down-model-with-skip-connection.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Thêm các skip connection giữa feature map và reconstruction layer&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Đồ hình bên dưới diễn ta chi tiết đường đi theo bottom-up và top-down. P2, P3, P4, P5 là các pyramid  của các feature map.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-top-down-with-bottom-up.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;so-sánh-feature-pyramid-networks-với-region-proposal-network&#34;&gt;So sánh Feature Pyramid Networks với Region Proposal Network&lt;/h2&gt;

&lt;p&gt;FPN không phải là mô hình phát hiện đối tượng. Nó là mô hình phát hiện đặc trưng và được sử dụng trong phát hiện đối tượng. Các feature map từ P2 đến P5 trong hình bên dưới độc lập với nhau và các đặc trưng được sử dụng để phát hiện đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-detail.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-feature-pyramid-networks-trong-fast-r-cnn-và-faster-r-cnn&#34;&gt;Sử dụng Feature Pyramid Networks trong Fast R-CNN và Faster R-CNN&lt;/h2&gt;

&lt;p&gt;Chúng ta hoàn toàn có thể sử dụng FPN trong Fast và Faster R-CNN. Chúng ta sẽ tạo ra các feature map sử dụng FPN, kết quả là ta thu được các puramid (feature map). Sau đó, chúng ta sẽ rút trích các ROIs trên các feature map đó. Dựa trên kích thước của các ROI, chúng ta sẽ chọn feature map nào tốt nhất để tạo các feature patches (các hình chữ nhật nhỏ). Các bạn có thể xem chi tiết ở hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fpn-in-faster-r-cnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;focal-loss-retinanet&#34;&gt;Focal loss (RetinaNet)&lt;/h2&gt;

&lt;p&gt;Trong thực tế, chúng ta sẽ gặp tình trạng tỷ lệ diện tích của các đối tượng trong ảnh nhỏ hơn nhiều so với phần background còn lại, ví dụ chúng ta cần nhận dạng một quả cam có kích thước 100x100 trong ảnh 1920x1080. Vì phần background quá lớn nên chúng sẽ là thành phần &amp;ldquo;thống trị&amp;rdquo; và làm sai lệch kết quả. SSD sử dụng phương pháp lấy mẫu tỷ lệ của object class và background class trong quá trình train (nên background sẽ không còn thống trị nữa).&lt;/p&gt;

&lt;p&gt;Ngoài ra, chúng ta sẽ còn gặp tình trạng là số lượng tỷ lệ object trong ảnh không đều nhau, ví dụ trong tập huấn luyệt có 1000 quả cam và 10 quả táo.&lt;/p&gt;

&lt;p&gt;Focal loss (FL) được sinh ra để giải quyết tình trạng này. Để đi vào chi tiết hơn, chúng ta nhắc lại hàm lỗi cross entropy.&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
  CE(p,y) =
    \begin{cases}
      -\log(p) &amp;amp; \text{if y=1} \\\&lt;br /&gt;
      -\log(1-p) &amp;amp; \text{otherwise}
    \end{cases}&lt;br /&gt;
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;Trong hàm trên thì y nhận giá trị 1 hoặc -1. Giá trị xác xuất nằm trong khoảng (0,1) là xác suất dự đoán cho lớp có y=1.&lt;/p&gt;

&lt;p&gt;Để rõ ràng hơn, ta có thể viết lại hàm trên như sau:&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
  p_t =
    \begin{cases}
      p &amp;amp; \text{if y=1} \\\&lt;br /&gt;
      1-p &amp;amp; \text{otherwise}
    \end{cases}&lt;br /&gt;
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
    CE(p,y) = CE(p_t) = -\log(p_t)
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;Ta có nhận xét rằng đối với các trường hợp được phân loại tốt (có xác suất lớn hơn 0.6) thì hàm loss nhận gái trị với độ lớn lớn hơn 0. Và trong trường hợp dữ liệu có tỷ lệ lệch cao thì tổng các giá trị này sẽ cho ra kết quả loss với một con số rất lớn so với loss của các trường hợp khó phâm loại. Và nó ảnh hưởng đến quá trình huấn luyện.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/focal-lost.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ý tưởng chính của focal-lost là đối với các trường hợp được phân loại tốt ( xác suất lớn hơn 0.5) thì focal lost sẽ làm giảm giá trị cross-entropy của nó xuống nhỏ hơn so với thông thường. Do đó, ta sẽ thêm trọng số cho hàm cross-entropy để biến thành hàm focal lost.&lt;/p&gt;

&lt;p&gt;$$
FL(p_t) = -(1-p_t)^\gamma\log(p_t)
$$&lt;/p&gt;

&lt;p&gt;Với nhân tử được thêm vào được gọi là modulating factor, gamma lớn hơn hoặc bằng 0 được gọi là tham số focusing.&lt;/p&gt;

&lt;p&gt;Nhìn hình ở trên, ta thấy rằng khi gamma = 0 thì hàm focal lost chính là cross-entropy.&lt;/p&gt;

&lt;p&gt;Đặc điểm của hàm lost trên như sau:&lt;/p&gt;

&lt;p&gt;Khi mẫu bị phân loại sai, pt nhỏ, nhân tố modulating factor gần với 1 và hàm lost ít bị ảnh hưởng. Khi pt tiến gần tới 1 (mẫu phân loại tốt), moduling factor sẽ tiến gần tới 0 và hàm loss trong trường hợp này sẽ bị giảm trọng số xuống.&lt;/p&gt;

&lt;p&gt;Tham số focusing sẽ điều chỉnh tỷ lệ các trường hợp được phân loại tốt được giảm trọng số. Khi gamma càng tăng thì ảnh hưởng của modulating factor cũng tăng. Trong các thí nghiệm cho thấy với gamma = 2 hì kết quả đạt được sẽ tốt nhất.&lt;/p&gt;

&lt;p&gt;Hình bên dưới là đồ hình của RetinaNet được xây dựng dựa trên FPN và ResNet sử dung Focal loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/retina-net-fpn-resnet.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch và tham khảo từ nguồn &lt;a href=&#34;https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&#34;&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu region based object detectors</title>
      <link>/blog/2018-12-05-what-do-we-learn-from-object-detection-p1/</link>
      <pubDate>Wed, 05 Dec 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-12-05-what-do-we-learn-from-object-detection-p1/</guid>
      <description>

&lt;h2 id=&#34;sliding-window-detectors&#34;&gt;Sliding-window detectors&lt;/h2&gt;

&lt;p&gt;Bắt đầu từ năm 2012, sau khi mạng AlexNet giành giải nhất cuộc thi 2012 ILSVRC, mọi nghiên cứu về phân lớp dữ liệu đều sử dụng mạng CNN. Kể từ đó đến đây, CNN được coi như là thuật toán thống trị trên mọi publish paper về các bài toán phân lớp đối tượng. Trong khi đó, để nhận dạng 1 đối tượng trong ảnh, các đơn giản nhất là thiết lập một cửa sổ trượt có kích thước là window size trượt từ trái qua phải, từ trên xuống dưới, quét qua toàn bộ bức ảnh. Để phát hiện các đối tượng khác nhau ở các góc nhìn khác nhau, chúng ta sẽ sử dụng cửa sổ trượt có kích thước thay đổi  và ảnh đầu vào có kích thước thay đổi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sliding-window.gif&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-base-various-windowsize.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-base-various-windowsize1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Dựa vào windowsize, chúng ta có thể cắt tấm hình bự thành các tấm hình nhỏ, sau đó sẽ rescale các phần nhỏ của bức ảnh thành các bức ảnh có kích thước cố định.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/fixed-size-image.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Các phần của bức ảnh sau đó sẽ được đem qua bộ phân lớp CNN để rút trích các đặc trưng, sau đó sử dụng một hàm phân lớp (như svm, logictic regression) để xác định lớp của bức hình và sử dụng linear regressor để tìm bao đóng của đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sliding-window-detector.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mã giả của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for window in windows
    patch = get_patch(image, window)
    results = detector(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cách dễ dàng nhất để cải tiến hiệu năng của phương pháp này là giảm số lượng tấm hình nhỏ xuống (ví dụ tăng kích thước window size). Cách này còn được giang hồ gọi là  brute force.&lt;/p&gt;

&lt;h2 id=&#34;selective-search&#34;&gt;Selective Search&lt;/h2&gt;

&lt;p&gt;Thay vì hướng tiếp cận brute force ở trên, chúng ta sử dụng phương pháp region proposal để tạo các region of interest (ROIs) để phát hiện đối tượng. Selective search là một phương pháp nằm trong nhóm region proposal. Trong phương pháp selective search(SS), chúng ta bắt đầu bằng cách xem các pixel là mỗi nhóm, các lần lặp tiếp theo, chúng ta sẽ tính khoảng cách ngữ nghĩa (ví dụ như là màu sắc, cường độ ánh sáng) giữa các nhóm và gom các nhóm có khoảng cách gần nhau về chung 1 nhóm để tìm ra phân vùng có khả năng cao nhất chứa đối tượng (ưu tiên gom những nhóm nhỏ trước).&lt;/p&gt;

&lt;p&gt;Như hình bên dưới, dòng đầu tiên, bức ảnh đâu tiên là ta có một vài nhóm nhỏ ở thời điểm X nào đó, ở hình thứ 2 là thực hiện gom nhớm theo cường độ màu sắc của hình số 1, và ở bước cuối cùng, ta thu được hình số 3. Những hình chữ nhật màu xanh ở dòng thứ 2 là những ROIS mô phỏng quá trình gom nhóm để tìm phân vùng có khả năng chứa đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/selectivesearch.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;selective search&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;mạng-r-cnn&#34;&gt;Mạng R-CNN&lt;/h2&gt;

&lt;p&gt;Mạng R-CNN sử dụng phương pháp region proposal để tạo ra khoảng 2000 ROIs. Các vùng sau đó sẽ được rescale theo một kích thước cố định nào đó và được đưa vào mô hình CNN có lớp cuối cùng kà một full conected layer để phân lớp đối tượng và để lọc ra boundary box (bao đóng) của đối tượng.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-cnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô phỏng việc sử dụng region proposal&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô phỏng việc sử dụng region proposal của RCNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mã giả của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; ROIs = region_proposal(image)
for ROI in ROIs
    patch = get_patch(image, ROI)
    results = detector(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với việc sử dụng ít tấm ảnh nhỏ hơn, và chất lượng của mỗi tấm ảnh nhỏ tốt hơn, Mạng R-CNN chạy nhanh hơn và có độ chính xác cao hơn so với mô hình sử dụng cửa sổ trượt.&lt;/p&gt;

&lt;h2 id=&#34;mạng-fast-r-cnn&#34;&gt;Mạng Fast R-CNN&lt;/h2&gt;

&lt;p&gt;Trong thực tế, các phân vùng của mạng R-CNN bị chồng lấp một phần / toàn bộ với các phân vùng khác. Do đó, việc huấn luyện và thực thi ( inference ) mạng R-CNN diễn ra khá chậm. Nếu chúng ta có 2000 proposal của mạng R-CNN, chúng ta phải thực hiện 2000 lần việc rút trích đặc trưng, một con số khác lớn.&lt;/p&gt;

&lt;p&gt;Thay vì phải rút trích đặc trưng của mỗi proposal, chúng ta có thể dùng CNN rút trích đặc trưng của toàn bộ bức ảnh trước (được feature map), đồng thời rút trích các proposal, lấy các proposal tương ứng trên feature map, rescale và cuối cùng là phân lớp và tìm vị trí của object. Với việc không phải lặp lại 2000 lần việc rút trích đặc trưng, Fast R-CNN giảm thời gian xử lý một cách đáng kể.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fast-r-cnn.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô phỏngviệc sử dụng propoxal trên feature map và các bước tiếp theo của Fast R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fast-r-cnn-network-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Đồ hình của Fast R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mã giả của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; feature_maps = process(image)
ROIs = region_proposal(image)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với việc không phải lặp đi lặp lại quá trình tìm ra các proposal, tốc độ của thuật toán tăng lên kha khá. Trong thực nghiệm, mô hình Fast R-CNN chạy nhanh hơn gấp 10 lần so với R-CNN trong quá trình huấn luyện. Và nhanh hơn 150 lần trong inferencing.&lt;/p&gt;

&lt;p&gt;Một khác biệt lớn nhất của Fast R-CNN là toàn bộ network (feature extractior, classifier, boundary box regressor) có thể huấn luyện end-to end (nghĩa là từ đầu đến cuối) với 2 hàm độ lỗi (loss funtion) khác nhau cùng lúc (classification loss và localization loss). Điều này làm tăng độ chính xác của mô hình.&lt;/p&gt;

&lt;h2 id=&#34;roi-pooling&#34;&gt;ROI Pooling&lt;/h2&gt;

&lt;p&gt;Vì Fast R-CNN sử dụng full connected layter ở lớp cuối, nên đòi hỏi input của chúng phải có kích thước cố định, nên ta phải resize lại feature về 1 kích thước cố định (do 2000 proposal có kích thước không cố định). Ở đây, các tác giả sử dụng ROI pooling để resize. Thuật toán ở đây được sử dụng như sau:&lt;/p&gt;

&lt;p&gt;Giả sử đơn giản là chúng ta có một proposal có kích thước 5x7, và chúng ta cần resize về hình dạng 2x2. Chúng ta xem kỹ hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/roi-pooling-example.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Hình ảnh mô phỏng ROI pooling&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hình ở bên trái là feature map của chúng ta.&lt;/p&gt;

&lt;p&gt;Hình số 2, vùng hình chữ nhật xanh là vùng proposal 5x7.&lt;/p&gt;

&lt;p&gt;Vì chúng ta cần resize về vùng có kích thước 2x2 (4 phần), nên ta chia vùng proposal 5x7 thành 4 phần (&lt;sup&gt;5&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; =2 dư 3, vậy có 1 phần là 2, 1 phần là 3. Tương tự &lt;sup&gt;7&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; = 3 dư 4, vậy có 1 phần 3, một phần 4. Cuối cùng ta có 4 hình chữ nhật có kích thước tương ứng là 2x3, 2x4, 3x3, 3x4) (Hình số 3).&lt;/p&gt;

&lt;p&gt;Hình số 4, từ 4 phần của vùng số 3, ta sẽ lấy giá trị lớn nhất của mỗi vùng.&lt;/p&gt;

&lt;p&gt;Vậy là ta thu được feature proposal có kích thước 2x2 rồi.&lt;/p&gt;

&lt;h2 id=&#34;faster-r-cnn&#34;&gt;Faster R-CNN&lt;/h2&gt;

&lt;p&gt;Nhìn kỹ lại vào thuật toán F-CNN, chúng ta cần phải rút rích 2000 ROIs, và nó là nguyên nhân lớn gây nên sự chậm trể của mô hình&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; feature_maps = process(image)
ROIs = region_proposal(image)         # Expensive, slow
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    results = detector2(patch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thuật toán Faster R-CNN sử dụng mô hình gần như tương tự Fast R-CNN, ngoài việc sử dụng thuật toán interal deep network thay cho selective search để tìm region proposal. Thuật toán mới chạy hiệu quả hơn khi tìm tất cả các ROIs trên mỗi bức ảnh với tốc độ 10ms/&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fater-r-cnn.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô hình của Faster R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-fater-r-cnn-model.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Đồ hình của Faster R-CNN&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;region-proposal-network&#34;&gt;Region proposal network&lt;/h2&gt;

&lt;p&gt;Mạng region proposal sử dụng feature map làm input đầu vào (như hình trên đã mô phỏng). Mạng sử dụng 1 bộ lọc 3x3, sau đó là một mô hình CNN như ZF hoặc VGG hoặc ResNet ( mô hình càng phức tạp thì độ chính xác cao, nhưng bù lại thời gian tìm kiếm sẽ lâu hơn) để dự đoán boundary box và object score (để xét xem trong bodary box trên có chứa đối tượng hay không. Trong thực tế, mạng Faster R-CNN trả về 2 lớp, lớp thứ nhất là có chứa object, lớp thứ 2 là không chứa object ( ví dụ lớp màu nền - background, lớp abc gì gì đó)) .&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Ví dụ Region proposal network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposal-network-1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;
&lt;em&gt;Mô hình Region proposal network sử dụng ZF network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Giả sử tại 1 điểm nào đó trên feature map, RPN có k dự đoán, vậy là chúng ta có tổng cộng 4xk toạ độ điểm và 2xk điểm cho điểm đó. Nhìn ví dụ ở hình bên dưới.&lt;/p&gt;

&lt;p&gt;Hình 1: ta có feature map với kích thước 8x8, vùng hình vuông được tô là filter đang xét có kích thước 3x3.
 Hình 2: Giả sử xét điểm có chấm xanh. Tại điểm đó, ta có k=3 sau khi chạy RPN, và ta được 3 hình chữ nhật như hình.&lt;/p&gt;

&lt;p&gt;Tuy nhiên, tại mỗi điểm, ta chỉ cần 1 boundary box tốt nhất. Cách đơn giản nhất là chọn ngẫu nhiên 1 cái. Nhưng như vậy thì ngay từ đầu ta chọn k=1 luôn cho khoẻ, mắc công gì phải chọn k=3. Trong thực tế, Faster R-CNN không sử dụng phương pháp random select. Thay vào đó, thuật toán một reference boxs hay còn được gọi với tên là anchors và tìm mức độ liên quan của k boundary box với k reference boxs và chọn ra boundary box có độ liên quan lớn nhất.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/anchors-box.png&#34; alt=&#34;Hình ảnh&#34; /&gt;
 &lt;em&gt;Ví dụ anchors box&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Các anchors này được lựa chọn trước đó và được xem là config của mô hình. Faster R-CNN sử dụng 9 anchor boxs (tương ứng với k =3) với 3 box đầu tiên có tỷ lệ width, height khác nhau (ví dụ 2x3, 3x3, 3x2), tiếp đó sẽ scale các box trên với các tỷ lệ khác khau (ví dụ 1.5,3,7) để đạt được 9 anchor boxs.&lt;/p&gt;

&lt;p&gt;Vì mỗi điểm sử dụng 9 anchors, nên ta có tổng cộng 2x9 score và 4x9 location (toạ độ)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/anchorsbox_feature_map.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Anchor box có thể được goijlaf priors hoặc default boundary boxes trong mỗi bài báo khác nhau.&lt;/p&gt;

&lt;h2 id=&#34;hiệu-năng-của-mô-hình-r-cnn&#34;&gt;Hiệu năng của mô hình R-CNN&lt;/h2&gt;

&lt;p&gt;Hình bên dưới mô tả benchmark của các mô hình dẫn xuất từ R-CNN, ta thấy Faster R-CNN có tốc độ tốt nhất.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-performance.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;region-based-fully-convolutional-networks&#34;&gt;Region-based Fully Convolutional Networks&lt;/h2&gt;

&lt;p&gt;Giả sử chúng ta chỉ có toạ độ của mắt phải trong khuôn mặt, chúng ta có thể nội suy ra được vị trí của khuôn mặt. Vì ta biết rằng mắt phải nằm ở vị trí trái trái trong bức hình, và ta từ đó suy ra vị trí của các phần còn lại (xem hình).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/f-rcnn-image1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nếu chúng ta có thêm thông tin khác, ví như toạ độ của mắt trái, mũi, miệng, &amp;hellip; thì chúng ta có thể kết hợp chúng để tăng độ chính xác của phân vùng khuôn mặt.&lt;/p&gt;

&lt;p&gt;Trong Faster R-CNN, chúng ta phải tìm proposal sử dụng một mô hình CNN, với khoảng 2000 ROI, chúng ta sẽ tiêu tốn một khoảng thời gian khá lớn để tìm chúng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    class_scores, box = detector(patch)         # Expensive, slow
    class_probabilities = softmax(class_scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong khi đó, với Fast R-CNN, chúng ta chỉ cần phải tính max hoặc average, nên Fast R-CNN nhanh hơn Faster R-CNN ở đây.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feature_maps = process(image)
ROIs = region_proposal(feature_maps)         
score_maps = compute_score_map(feature_maps)
for ROI in ROIs
    V = region_roi_pool(score_maps, ROI)     
    class_scores, box = average(V)                   # Much simpler, faster.
    class_probabilities = softmax(class_scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Xét feature map M có kích thước 5x5, trong đó có chứa một hình vuông màu xanh, hình vuông xanh là đối tượng thực tế ta cần tìm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example1.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ta chia hình vuông thành phân vùng có kích thước 3x3 (hình 2). Sau đó, chúng ta tạo một feature mới để từ M để tìm ra góc trái trên của hình vuông (chỉ tìm góc trái trên) (hình 3). Feature map mới giống hình thứ 3, chỉ có ô được tô màu vàng ở vị trí [2,2] được bật.&lt;/p&gt;

&lt;p&gt;Với mỗi 9 phần của hình vuông, chúng ta có 9 feature map cho mỗi phần, nhận dạng 9 vùng tương ứng cho một đối tượng. Những feature map này được gọi là position sensitive score map, bởi vì chúng detect ra điểm (score) và sub region của một đối tượng (Xem hình bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example2.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Xét ảnh bên dưới, giả sử vùng được tô gạch đỏ là proposal (hình 1). Chúng ta cũng chia nó thành những phân vùng con có kích thước 3x3 (hình 2). Và tìm xem mức độ giống nhau của mỗi vùng con của proposal và vùng con của feature map như thế nào. Kết quả sẽ được lưu vào một ma trận 3x3 như hình số 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example3.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Quá trình ánh xạ điểm từ score maps và ROIS vào mảng vote_array được gọi là position sensitive ROI pool.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example4.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Sau khi tính toán hết các giá trị của position-sensitive ROI pool, chúng ta sẽ tính trung bình của vote_array để lấy điểm của lớp (class score).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-example5.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Giả sử mô hình chúng ta phải nhận dạng k lớp, do có thêm lớp background nên chúng ta có tổng cộng k+1 lớp. Với mỗi lớp chúng ta có 3x3 score map, suy ra chúng ta có tổng cộng là (k+1)x3x3 score maps, (k+1) điểm, và dùng softmax ta sẽ thu được xác suất của mỗi lớp.&lt;/p&gt;

&lt;p&gt;Luồng dữ liệu của mô hình&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/region-proposals-r-cnn-data-flow.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết được lược dịch và tham khảo từ nguồn &lt;a href=&#34;https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9&#34;&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân tích giỏ hàng của website instacart</title>
      <link>/blog/2018-11-13-instacart-market-basket-analysis/</link>
      <pubDate>Tue, 13 Nov 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-11-13-instacart-market-basket-analysis/</guid>
      <description>

&lt;h1 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h1&gt;

&lt;p&gt;Instacart là một startup cung ứng đồ tạp hóa qua website và ứng dụng di động. Người dùng chỉ cần chọn đồ muốn mua tại các chuỗi bán lẻ và đặt đồ, Instacart sẽ đi mua và giao đến tận tay họ. Đến nay, Instacart hoạt động tại 15.000 cửa hàng tạp hoá tại 4.000 thành phố với khoảng 50.000 “trợ lý mua sắm”. Team data science của instacart đóng vai trò rất quan trọng trong việc cung cấp trải nghiệm người dùng trong việc sử dụng app để mua hàng. Hiện tại, họ đang sử dụng các dữ liệu của khách hàng để tạo nên mô hình dự đoán sản phẩm nào người dùng sẽ mua lại, sẽ mua thử lần đầu tiên, hoặc sẽ thêm vào giỏ hàng. Hiện họ đã publish khoảng 3 triệu đơn hàng của họ để các nhà khoa học dữ liệu khác sử dụng và nghiên cứu.&lt;/p&gt;

&lt;h1 id=&#34;dẫn-nhập&#34;&gt;Dẫn nhập&lt;/h1&gt;

&lt;h2 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h2&gt;

&lt;p&gt;Các bạn có thể download dữ liệu ở &lt;a href=&#34;https://www.instacart.com/datasets/grocery-shopping-2017&#34;&gt;https://www.instacart.com/datasets/grocery-shopping-2017&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Các file bao gồm:&lt;/p&gt;

&lt;p&gt;File aisles.csv (134 dòng) có 2 cột là aisle_id,aisle&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;aisle_id,aisle  
1,prepared soups salads  
2,specialty cheeses  
3,energy granola bars  
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File departments.csv (21 dòng) gồm 2 cột là department_id,department&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;department_id,department  
1,frozen  
2,other  
3,bakery   
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File order_products__(prior|train).csv (trên 30 triệu dòng)&lt;/p&gt;

&lt;p&gt;Tập này chứa danh sách sản phẩm được mua trong mỗi đơn hàng. File order_products__prior.csv chứa sản phẩm của đơn hàng trước đó của khách hàng. &amp;lsquo;reordered&amp;rsquo; nói rằng sản phẩm này trong đơn hàng hiện tại đã được mua ở đơn hàng trước đó. Vì vậy, sẽ có đơn hàng không được gán là &amp;lsquo;reordered&amp;rsquo; (chúng ta có thể gán nhãn là None hoặc cái gì đó cũng được để chỉ các sản phẩm này). &amp;lsquo;add_to_cart_order&amp;rsquo; là thứ tự của sp được thêm vào giỏ hàng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_id,product_id,add_to_cart_order,reordered  
 1,49302,1,1  
 1,11109,2,1  
 1,10246,3,0  
 ... 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File orders.csv (3.4 triệu dòng, 206k users): chứa thông tin của đơn hàng, trong đó, order_dow là ngày trong tuần, eval_set thuộc một trong 3 loại là prior, train, test.  order_number là thứ tự của đơn hàng của user này.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_id,user_id,eval_set,order_number,order_dow,order_hour_of_day,days_since_prior_order  
 2539329,1,prior,1,2,08,  
 2398795,1,prior,2,3,07,15.0  
 473747,1,prior,3,3,12,21.0  
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;File products.csv ((50k dòng) chứa thông tin sản phẩm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; product_id,product_name,aisle_id,department_id
 1,Chocolate Sandwich Cookies,61,19  
 2,All-Seasons Salt,104,13  
 3,Robust Golden Unsweetened Oolong Tea,94,7  
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với mỗi order_id trong tập test ở file orders.csv, chúng ta phải dự đoán các sản phẩm nào người dùng sẽ mua lại (&amp;ldquo;reorder&amp;rdquo;) thuộc đơn hàng đó. Nếu bạn dự đoán đó là đơn hàng không có sản phẩm nào được mua lại, thì ta sẽ điền vào giá trị &amp;lsquo;None&amp;rsquo;&lt;/p&gt;

&lt;p&gt;Ví dụ về kết quả dự đoán:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_id,products  
17,1 2  
34,None  
137,1 2 3  
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;thực-hành&#34;&gt;Thực hành&lt;/h1&gt;

&lt;p&gt;Đầu tiên, ta sẽ import một số thư viện cơ bản để sử dụng, và load tất cả các file lên. Lưu ý một chút là ở đây, mình để tất cả các file trong thư mục data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
from collections import OrderedDict

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

from sklearn import metrics, cross_validation
from sklearn.metrics import f1_score
from sklearn.preprocessing import MinMaxScaler

#Import the files
aisles_df = pd.read_csv(&#39;data/aisles.csv&#39;)
products_df = pd.read_csv(&#39;data/products.csv&#39;)
orders_df = pd.read_csv(&#39;data/orders.csv&#39;)
order_products_prior_df = pd.read_csv(&#39;data/order_products__prior.csv&#39;)
departments_df = pd.read_csv(&#39;data/departments.csv&#39;)
order_products_train_df = pd.read_csv(&#39;data/order_products__train.csv&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau đó, mình sẽ merge đơn hàng vào chi tiết đơn hàng của tập train và tập prior&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;order_products_train_df = order_products_train_df.merge(orders_df.drop(&#39;eval_set&#39;, axis=1), on=&#39;order_id&#39;)
order_products_prior_df = order_products_prior_df.merge(orders_df.drop(&#39;eval_set&#39;, axis=1), on=&#39;order_id&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;show ra 5 dòng đầu tiên của order_products_train_df&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(order_products_train_df.head())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   order_id  product_id  add_to_cart_order  reordered  user_id  order_number  order_dow  order_hour_of_day  days_since_prior_order
0         1       49302                  1          1   112108             4          4                 10                     9.0
1         1       11109                  2          1   112108             4          4                 10                     9.0
2         1       10246                  3          0   112108             4          4                 10                     9.0
3         1       49683                  4          0   112108             4          4                 10                     9.0
4         1       43633                  5          1   112108             4          4                 10                     9.0

[5 rows x 9 columns]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tổng cộng mình có 9 cột, ý nghĩa các cột mình có giải thích ở trên rồi nha.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta tạo tập tập dữ liệu đếm số lượng sản phẩm của từng người mua&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_product_df = (order_products_prior_df.groupby([&#39;product_id&#39;,&#39;user_id&#39;],as_index=False) 
                                          .agg({&#39;order_id&#39;:&#39;count&#39;}) 
                                          .rename(columns={&#39;order_id&#39;:&#39;user_product_total_orders&#39;}))

train_ids = order_products_train_df[&#39;user_id&#39;].unique() 
df_X = user_product_df[user_product_df[&#39;user_id&#39;].isin(train_ids)]
print(df_X.head())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   product_id  user_id  user_product_total_orders
0           1      138                          2
1           1      709                          1
3           1      777                          1
6           1     1052                          2
9           1     1494                          3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây, người 138 mua sản phẩm 1 2 lần, người 709 mua sản phẩm 1 1 lần, &amp;hellip; tương tự như vậy cho các user và product khác.&lt;/p&gt;

&lt;p&gt;Bước tiếp theo, chúng ta sẽ liệt kê các sản phẩm người dùng đã mua:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_carts = (order_products_train_df.groupby(&#39;user_id&#39;,as_index=False)
                                      .agg({&#39;product_id&#39;:(lambda x: set(x))})
                                      .rename(columns={&#39;product_id&#39;:&#39;latest_cart&#39;}))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;print(train_carts.head())&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  user_id                                        latest_cart
0        1  {196, 26405, 27845, 46149, 13032, 39657, 26088...
1        2  {24838, 11913, 45066, 31883, 48523, 38547, 248...
2        5  {40706, 21413, 20843, 48204, 21616, 19057, 201...
3        7  {17638, 29894, 47272, 45066, 13198, 37999, 408...
4        8  {27104, 15937, 5539, 41540, 31717, 48230, 2224...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mối tương quan giữa sản phẩm được add to card và sản phẩm được mua&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_X = df_X.merge(train_carts, on=&#39;user_id&#39;)
df_X[&#39;in_cart&#39;] = (df_X.apply(lambda row: row[&#39;product_id&#39;] in row[&#39;latest_cart&#39;], axis=1).astype(int))

print(df_X.head())

print(df_X[&#39;in_cart&#39;].value_counts())

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# df_X.head()
   product_id  user_id  user_product_total_orders latest_cart  in_cart
0           1      138                          2     {42475}        0
1         907      138                          2     {42475}        0
2        1000      138                          1     {42475}        0
3        3265      138                          1     {42475}        0
4        4913      138                          1     {42475}        0

# df_X[&#39;in_cart&#39;].value_counts()
0    7645837
1     828824
Name: in_cart, dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tỷ lệ khoảng 9.7%. Điều này nói lên rằng, người dùng trong 1 phiên mua hàng có thể add rất nhiều sản phẩm vào giỏ, nhưng chỉ khoảng 10% sản phẩm họ mua thật sự, hơn 90% sản phẩm còn lại sẽ bị remove trước khi nọ nhấn nút thanh toán.&lt;/p&gt;

&lt;h1 id=&#34;xây-dựng-tập-đặc-trưng&#34;&gt;Xây dựng tập đặc trưng&lt;/h1&gt;

&lt;h2 id=&#34;đặc-trưng-sản-phẩm&#34;&gt;Đặc trưng sản phẩm&lt;/h2&gt;

&lt;p&gt;Với đặc trưng sản phẩm, chúng ta sẽ rút trích 2 đặc trưng đơn giản là tổng số lượng đơn hàng của một sản phẩm và trung bình số lượng đơn hàng có chứa sản phẩm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prod_features = [&#39;product_total_orders&#39;,&#39;product_avg_add_to_cart_order&#39;]

prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;],as_index=False)
                                           .agg(OrderedDict(
                                                   [(&#39;order_id&#39;,&#39;nunique&#39;),
                                                    (&#39;add_to_cart_order&#39;,&#39;mean&#39;)])))
prod_features_df.columns = [&#39;product_id&#39;] + prod_features
print(prod_features_df.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
   product_id  product_total_orders  product_avg_add_to_cart_order
0           1                  1852                       5.801836
1           2                    90                       9.888889
2           3                   277                       6.415162
3           4                   329                       9.507599
4           5                    15                       6.466667

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add thêm đặc trưng sản phẩm vào trong tập huấn luyện&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
df_X = df_X.merge(prod_features_df, on=&#39;product_id&#39;)

#note that dropping rows with NA product_avg_days_since_prior_order is likely a naive choice 
df_X = df_X.dropna()
print(df_X.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   product_id  user_id              ...               product_total_orders product_avg_add_to_cart_order
0           1      138              ...                               1852                      5.801836
1           1      709              ...                               1852                      5.801836
2           1      777              ...                               1852                      5.801836
3           1     1052              ...                               1852                      5.801836
4           1     1494              ...                               1852                      5.801836

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;đặc-trưng-người-dùng&#34;&gt;Đặc trưng người dùng&lt;/h2&gt;

&lt;p&gt;Với người dùng, chúng sa sử dụng các đặc trưng là: Tổng số lượng đơn hàng, trung bình số sản phẩm trong 1 đơn hàng, tổng số lượng sản phẩm người dùng mua, Trung bình số ngày user sẽ mua đơn hàng tiếp theo&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_features = [&#39;user_total_orders&#39;,&#39;user_avg_cartsize&#39;,&#39;user_total_products&#39;,&#39;user_avg_days_since_prior_order&#39;]

user_features_df = (order_products_prior_df.groupby([&#39;user_id&#39;],as_index=False)
                                           .agg(OrderedDict(
                                                   [(&#39;order_id&#39;,[&#39;nunique&#39;, (lambda x: x.shape[0] / x.nunique())]),
                                                    (&#39;product_id&#39;,&#39;nunique&#39;),
                                                    (&#39;days_since_prior_order&#39;,&#39;mean&#39;)])))

user_features_df.columns = [&#39;user_id&#39;] + user_features
print(user_features_df.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và chúng ta merge tiếp đặc trưng user vào trong tập huấn luyện.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
df_X = df_X.merge(user_features_df, on=&#39;product_id&#39;)

#note that dropping rows with NA product_avg_days_since_prior_order is likely a naive choice 
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;đặc-trưng-mối-tương-quan-giữa-người-dùng-và-sản-phẩm&#34;&gt;Đặc trưng mối tương quan giữa người dùng và sản phẩm&lt;/h2&gt;

&lt;p&gt;Ở đây, chúng ta sử dụng đặc trưng trung bình số sản phẩm của 1 người được thêm vào đơn hàng và tần suất 1 sản phẩm 1 user add vào đơn hàng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_prod_features = [&#39;user_product_avg_add_to_cart_order&#39;]

user_prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;,&#39;user_id&#39;],as_index=False) \
                                                .agg(OrderedDict(
                                                     [(&#39;add_to_cart_order&#39;,&#39;mean&#39;)])))

user_prod_features_df.columns = [&#39;product_id&#39;,&#39;user_id&#39;] + user_prod_features
df_X = df_X.merge(user_prod_features_df,on=[&#39;user_id&#39;,&#39;product_id&#39;])
df_X[&#39;user_product_order_freq&#39;] = df_X[&#39;user_product_total_orders&#39;] / df_X[&#39;user_total_orders&#39;] 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;bổ-sung-thêm-đặc-trưng&#34;&gt;Bổ sung thêm đặc trưng&lt;/h1&gt;

&lt;p&gt;Ngoài các đặc trưng cơ bản ở trên, ta sẽ bổ sung thêm một số đặc trưng khác:&lt;/p&gt;

&lt;p&gt;Đặc trưng sản phẩm: bổ sung thêm 3 đặc trưng trung bình ngày trong tuần được đặt hàng  (cột order_down), trung bình giờ đặt hàng (cột order_hour_of_day), trung bình ngày đặt hàng kể từ lần đặt trước đó (cột days_since_prior_order) theo sản phẩm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prod_features = [&#39;product_avg_order_dow&#39;, &#39;product_avg_order_hour_of_day&#39;, &#39;product_avg_days_since_prior_order&#39;]

prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;], as_index=False)
                                     .agg(OrderedDict(
                                     [(&#39;order_dow&#39;,&#39;mean&#39;),
                                      (&#39;order_hour_of_day&#39;, &#39;mean&#39;),
                                      (&#39;days_since_prior_order&#39;, &#39;mean&#39;)])))

prod_features_df.columns = [&#39;product_id&#39;] + prod_features

df_X = df_X.merge(prod_features_df, on=&#39;product_id&#39;)
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trưng người dùng: bổ sung thêm 2 cột đặc trung trung bình ngày trong tuần được đặt hàng  (cột order_down) và  trung bình giờ đặt hàng (cột order_hour_of_day) theo người dùng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_features = [&#39;user_avg_order_dow&#39;,&#39;user_avg_order_hour_of_day&#39;]

user_features_df = (order_products_prior_df.groupby([&#39;user_id&#39;],as_index=False)
                                           .agg(OrderedDict(
                                                   [(&#39;order_dow&#39;,&#39;mean&#39;),
                                                    (&#39;order_hour_of_day&#39;,&#39;mean&#39;)])))

user_features_df.columns = [&#39;user_id&#39;] + user_features
df_X = df_X.merge(user_features_df, on=&#39;user_id&#39;)
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trung người dùng - sản phẩm: Bổ sung thêm đặc trưng tung bình trên cột order_down, order_hour_of_day, days_since_prior_order theo người dùng và sản phẩm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
user_prod_features = [&#39;user_product_avg_days_since_prior_order&#39;,
                      &#39;user_product_avg_order_dow&#39;,
                      &#39;user_product_avg_order_hour_of_day&#39;]

user_prod_features_df = (order_products_prior_df.groupby([&#39;product_id&#39;,&#39;user_id&#39;],as_index=False) \
                                                .agg(OrderedDict(
                                                     [(&#39;days_since_prior_order&#39;,&#39;mean&#39;),
                                                     (&#39;order_dow&#39;,&#39;mean&#39;),
                                                     (&#39;order_hour_of_day&#39;,&#39;mean&#39;)])))

user_prod_features_df.columns = [&#39;product_id&#39;,&#39;user_id&#39;] + user_prod_features 

df_X = df_X.merge(user_prod_features_df, on=[&#39;user_id&#39;, &#39;product_id&#39;])
df_X = df_X.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đặc trưng độ lệch: Tính độ lệch của của một số đặc trưng so với trung bình của chúng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Create delta columns to compare how users perform against averages
df_X[&#39;product_total_orders_delta_per_user&#39;] = df_X[&#39;product_total_orders&#39;] - df_X[&#39;user_product_total_orders&#39;]

df_X[&#39;product_avg_add_to_cart_order_delta_per_user&#39;] = df_X[&#39;product_avg_add_to_cart_order&#39;] - \
                                                            df_X[&#39;user_product_avg_add_to_cart_order&#39;]

df_X[&#39;product_avg_order_dow_per_user&#39;] = df_X[&#39;product_avg_order_dow&#39;] - df_X[&#39;user_product_avg_order_dow&#39;]

df_X[&#39;product_avg_order_hour_of_day_per_user&#39;] = df_X[&#39;product_avg_order_hour_of_day&#39;] - \
                                                            df_X[&#39;user_product_avg_order_hour_of_day&#39;]

df_X[&#39;product_avg_days_since_prior_order_per_user&#39;] = df_X[&#39;product_avg_days_since_prior_order&#39;] - \
                                                            df_X[&#39;user_product_avg_days_since_prior_order&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bổ sung thêm đặc trưng department name&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;f_departments_df = products_df.merge(departments_df, on = &#39;department_id&#39;)
f_departments_df = f_departments_df[[&#39;product_id&#39;, &#39;department&#39;]]

df_X = df_X.merge(f_departments_df, on = &#39;product_id&#39;)
df_X = df_X.dropna()
df_X = pd.concat([df_X, pd.get_dummies(df_X[&#39;department&#39;])], axis=1)
del df_X[&#39;department&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta có tổng cộng 21 department name, vậy chúng ta thêm 21 cột, một cột tương ứng với một department name, ví dụ: alcohol,babies ,bakery, &amp;hellip; Sản phẩm thuộc department name thì sẽ được đánh số 1, không thuộc department name thì đánh số 0.&lt;/p&gt;

&lt;h1 id=&#34;huấn-luyện-mô-hình&#34;&gt;Huấn luyện mô hình&lt;/h1&gt;

&lt;p&gt;Chia tập dữ liệu thành &lt;sup&gt;80&lt;/sup&gt;&amp;frasl;&lt;sub&gt;20&lt;/sub&gt; trong đó 80% là tập train, 20% là tập test. Sử dụng k-fold-cross_validation với k=10&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
np.random.seed(99)
total_users = df_X[&#39;user_id&#39;].unique() 
test_users = np.random.choice(total_users, size=int(total_users.shape[0] * .20), replace=False)



test_user_sets = []
length = len(test_users)
cv = 10


for x in range (0, cv):
    start = int(x/cv*length)
    finish = int((x+1)/cv*length)
    test_user_sets.append(test_users[start:finish])

cv_f1_scores = []
cv_f1_scores_balanced = []
cv_f1_scores_10fit = []

for test_user_set in test_user_sets:
    df_X_tr, df_X_te = df_X[~df_X[&#39;user_id&#39;].isin(test_user_set)], df_X[df_X[&#39;user_id&#39;].isin(test_user_set)] 

    y_tr, y_te = df_X_tr[&#39;in_cart&#39;], df_X_te[&#39;in_cart&#39;]
    X_tr, X_te = df_X_tr.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \
             df_X_te.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \
        
    scaler = MinMaxScaler()
    X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns=X_tr.columns)
    X_te = pd.DataFrame(scaler.fit_transform(X_te), columns=X_te.columns)
    
    lr = LogisticRegression(C=10000000) 
    lr_balanced = LogisticRegression(class_weight=&#39;balanced&#39;, C=10000000)
    lr_10x = LogisticRegression(class_weight={1 : 6, 0 : 1}, C=10000000)
    
    lr.fit(X_tr, y_tr)
    cv_f1_scores.append(f1_score(lr.predict(X_te), y_te))

    lr_balanced.fit(X_tr, y_tr)
    cv_f1_scores_balanced.append(f1_score(lr_balanced.predict(X_te), y_te))

    lr_10x.fit(X_tr, y_tr)
    cv_f1_scores_10fit.append(f1_score(lr_10x.predict(X_te), y_te))   

print(&amp;quot;cv_f1_scores: &amp;quot; +str( np.mean(cv_f1_scores)))
print(&amp;quot;cv_f1_scores_balanced: &amp;quot;+str(np.mean(cv_f1_scores_balanced)))
print(&amp;quot;cv_f1_scores_10fit: &amp;quot;+str(np.mean(cv_f1_scores_10fit)))

df_X_tr, df_X_te = df_X[~df_X[&#39;user_id&#39;].isin(test_users)], df_X[df_X[&#39;user_id&#39;].isin(test_users)]

y_tr, y_te = df_X_tr[&#39;in_cart&#39;], df_X_te[&#39;in_cart&#39;]
X_tr, X_te = df_X_tr.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \
             df_X_te.drop([&#39;product_id&#39;,&#39;user_id&#39;,&#39;latest_cart&#39;,&#39;in_cart&#39;],axis=1), \

lr_10x = LogisticRegression(class_weight={1 : 6, 0 : 1}, C=10000000)
lr_10x.fit(X_tr, y_tr)
print(&amp;quot;F1 store all: &amp;quot;+str(f1_score(lr_10x.predict(X_te), y_te)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_f1_scores: 0.2026889989037295
cv_f1_scores_balanced: 0.3816810646496983
cv_f1_scores_10fit: 0.3899595078917494

F1 store all: 0.3808374055616213
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử in ra hệ số của hàm hồi quy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;coefficients = pd.DataFrame(lr_10x.coef_, columns = X_tr.columns)
coefficients = np.exp(coefficients)
print(coefficients.T)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_product_total_orders                     1.160475
product_total_orders                          1.077254
product_avg_add_to_cart_order                 0.915343
user_total_orders                             0.983272
user_avg_cartsize                             1.059655
user_total_products                           0.993839
user_avg_days_since_prior_order               0.993513
user_product_avg_add_to_cart_order            0.950418
user_product_order_freq                       1.051246
product_avg_order_dow                         0.994744
product_avg_order_hour_of_day                 1.010971
product_avg_days_since_prior_order            0.994498
user_avg_order_dow                            0.997298
user_avg_order_hour_of_day                    1.012958
user_product_avg_days_since_prior_order       1.003382
user_product_avg_order_dow                    0.994477
user_product_avg_order_hour_of_day            1.003457
product_total_orders_delta_per_user           0.928288
product_avg_add_to_cart_order_delta_per_user  0.963095
product_avg_order_dow_per_user                1.000268
product_avg_order_hour_of_day_per_user        1.007489
product_avg_days_since_prior_order_per_user   0.991147
alcohol                                       0.998866
babies                                        1.000313
bakery                                        1.003098
beverages                                     1.007733
breakfast                                     1.000117
bulk                                          0.999980
canned goods                                  0.995017
dairy eggs                                    1.018069
deli                                          1.002720
dry goods pasta                               0.997379
frozen                                        1.000752
household                                     0.992164
international                                 0.996822
meat seafood                                  1.000340
missing                                       1.001953
other                                         0.999607
pantry                                        0.972038
personal care                                 0.992072
pets                                          1.000466
produce                                       1.017809
snacks                                        1.004893
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử show confusion matrix của dữ liệu:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&#39;fivethirtyeight&#39;)

def plot_confusion_matrix(cm,title=&#39;Confusion matrix&#39;, cmap=plt.cm.Reds):
    plt.imshow(cm, interpolation=&#39;nearest&#39;,cmap=cmap)
    plt.title(title)
    plt.colorbar()
    plt.tight_layout()
    plt.ylabel(&#39;True label&#39;)
    plt.xlabel(&#39;Predicted label&#39;)

#y_tr=np.ravel(y_tr)

train_acc=lr_10x.score(X_tr, y_tr)
test_acc=lr_10x.score(X_te, y_te)
print(&amp;quot;Training Data Accuracy: %0.2f&amp;quot; %(train_acc))
print(&amp;quot;Test Data Accuracy:     %0.2f&amp;quot; %(test_acc))
    
y_true = y_te
y_pred = lr_10x.predict(X_te)


conf = confusion_matrix(y_true, y_pred)
print(conf)

print (&#39;\n&#39;)
print (&amp;quot;Precision:              %0.2f&amp;quot; %(conf[1, 1] / (conf[1, 1] + conf[0, 1])))
print (&amp;quot;Recall:                 %0.2f&amp;quot;% (conf[1, 1] / (conf[1, 1] + conf[1, 0])))
    
cm=confusion_matrix(y_true, y_pred, labels=[0, 1])
    
plt.figure()
plot_confusion_matrix(cm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Training Data Accuracy: 0.83
Test Data Accuracy:     0.83
[[1236979  190126]
 [  78107   82493]]


Precision:              0.30
Recall:                 0.51
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/instartcart_plot_confusion_matrix.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Show đường cong ROC của dữ liệu&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from sklearn.metrics import roc_curve, auc

y_score = lr_10x.predict_proba(X_te)[:,1]

fpr, tpr,_ = roc_curve(y_te, y_score)
roc_auc = auc(fpr, tpr)

plt.figure()
# Plotting our Baseline..
plt.plot([0,1],[0,1], linestyle=&#39;--&#39;, color = &#39;black&#39;)
plt.plot(fpr, tpr, color = &#39;green&#39;)
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/instartcart_roc.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dự đoán giá chứng khoán SP500 sử dụng LSTM</title>
      <link>/blog/2018-11-10-stock-prediction_v1/</link>
      <pubDate>Sat, 10 Nov 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-11-10-stock-prediction_v1/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Ở bài viết này, mình sẽ xây dựng mô hình hơn giản để áp dụng vào tập dữ liệu giá chứng khoáng. Mục tiêu của bài này là chúng ta sẽ dự đoán chỉ số S&amp;amp;P 500 sử dụng LSTM. Các bạn có nhu cầu tìm hiểu thêm về chỉ số sp 500 có thể đọc thêm ở &lt;a href=&#34;https://vi.wikipedia.org/wiki/S%26P_500&#34;&gt;https://vi.wikipedia.org/wiki/S%26P_500&lt;/a&gt;. Đây là một ứng dụng nhỏ, không có ý nghĩa nhiều ở thực tế do khi phân tích chứng khoán, ta còn xét thêm rất nhiều yếu tố phụ nữa. Mô hình này thực chất chỉ là một trong những mô hình chơi chơi.&lt;/p&gt;

&lt;h2 id=&#34;dẫn-nhập&#34;&gt;Dẫn nhập&lt;/h2&gt;

&lt;h3 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h3&gt;

&lt;p&gt;Các bạn có thể download dữ liệu ở &lt;a href=&#34;https://github.com/AlexBlack2202/alexmodel/blob/master/GSPC.csv&#34;&gt;https://github.com/AlexBlack2202/alexmodel/blob/master/GSPC.csv&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Đầu tiên, như thường lệ, chúng ta sẽ import các thư viện cần thiết để sử dụng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from subprocess import check_output
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.models import Sequential
from sklearn.cross_validation import  train_test_split
import time #helper libraries
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from numpy import newaxis

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đọc dữ liệu lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
file_name =&#39;GSPC.csv&#39;

prices_dataset =  pd.read_csv(file_name, header=0)

``

Xem kích thước của dữ liệu:

```python
print(prices_dataset.shape)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(17114, 7)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả là ta có  17114 ngàn dòng và 7 cột. Thử show 10 row đầu tiên của dữ liệu lên xem như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(prices_dataset.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;         Date       Open       High        Low      Close  Adj Close   Volume
0  1950-11-09  19.790001  19.790001  19.790001  19.790001  19.790001  1760000
1  1950-11-10  19.940001  19.940001  19.940001  19.940001  19.940001  1640000
2  1950-11-13  20.010000  20.010000  20.010000  20.010000  20.010000  1630000
3  1950-11-14  19.860001  19.860001  19.860001  19.860001  19.860001  1780000
4  1950-11-15  19.820000  19.820000  19.820000  19.820000  19.820000  1620000

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cột đầu tiên là ngày, sau đó là giá mở cửa, giá giao dịch cao nhất, giá giao dịch thấp nhât, giá đóng cử, giá đóng cửa đã điều chỉnh, khối lượng giao dịch.&lt;/p&gt;

&lt;p&gt;Plot đồ thị của mã SP500 lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

plt.plot(prices_dataset.Open.values, color=&#39;red&#39;, label=&#39;open&#39;)
plt.plot(prices_dataset.Close.values, color=&#39;green&#39;, label=&#39;close&#39;)
plt.plot(prices_dataset.Low.values, color=&#39;blue&#39;, label=&#39;low&#39;)
plt.plot(prices_dataset.High.values, color=&#39;black&#39;, label=&#39;high&#39;)
plt.title(&#39;stock price&#39;)
plt.xlabel(&#39;time [days]&#39;)
plt.ylabel(&#39;price&#39;)
plt.legend(loc=&#39;best&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sp500indexv1.png&#34; alt=&#34;Hình ảnh đừng đồ thị của chỉ số sp 500&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hình với số lượng hơi nhiều nên khó phân biệt được giá trị của dữ liệu, chúng ta thử show đồ thị của 50 ngày cuối cùng trong dữ liệu.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prices_dataset_tail_50 = prices_dataset.tail(50)

plt.plot(prices_dataset_tail_50.Open.values, color=&#39;red&#39;, label=&#39;open&#39;)
plt.plot(prices_dataset_tail_50.Close.values, color=&#39;green&#39;, label=&#39;close&#39;)
plt.plot(prices_dataset_tail_50.Low.values, color=&#39;blue&#39;, label=&#39;low&#39;)
plt.plot(prices_dataset_tail_50.High.values, color=&#39;black&#39;, label=&#39;high&#39;)
plt.title(&#39;stock price&#39;)
plt.xlabel(&#39;time [days]&#39;)
plt.ylabel(&#39;price&#39;)
plt.legend(loc=&#39;best&#39;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sp500index_tail_50.png&#34; alt=&#34;Hình ảnh đừng đồ thị của chỉ số sp 500&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hình ảnh trông khá rõ ràng và trực quan hơn rất nhiều.&lt;/p&gt;

&lt;p&gt;Chúng ta sẽ bỏ đi cột DATE,Adj Close,Volume đi. Các cột đó không cần thiết cho quá trình dự đoán.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
prices_dataset_dropout = prices_dataset.drop([&#39;Date&#39;,&#39;Adj Close&#39;,&#39;Volume&#39;], 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scale-dữ-liệu&#34;&gt;Scale dữ liệu&lt;/h3&gt;

&lt;p&gt;Khi sử dụng ANN, chúng ta thông thường sẽ scale dữ liệu input về đoạn [-1,1]. Trong python, thư viện sklearn đã hỗ trợ cho chúng ta sẵn các hàm scale dữ liệu cần thiết.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scale data
def normalize_data(df):
    min_max_scaler = MinMaxScaler()
    df[&#39;Open&#39;] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))
    df[&#39;High&#39;] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))
    df[&#39;Low&#39;] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))
    df[&#39;Close&#39;] = min_max_scaler.fit_transform(df.Close.values.reshape(-1,1))
    return df

prices_dataset_norm = normalize_data(prices_dataset_dropout)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;phân-chia-tập-train-và-test&#34;&gt;Phân chia tập train và test.&lt;/h3&gt;

&lt;p&gt;Chúng ta sẽ chia dữ liệu thành 2 phần với 80% là train và 20% còn lại là test. Chọn seq_len=20, các bạn có thể test với các seq len khác, và sau đó chuyển dữ liệu về dạng numpy array để dễ dàng thực hiện các phép chuyển đổi.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def generate_data(stock_ds, seq_len):
    data_raw = stock_ds.as_matrix()
    data = []
    
    # create all possible sequences of length seq_len
    for index in range(len(data_raw) - seq_len): 
        data.append(data_raw[index: index + seq_len])
    return data

#data as numpy array
def generate_train_test(data_ds,split_percent=0.8):
    print(len(data_ds))
    data = np.asarray(data_ds)
   
    data_size = len(data)
    train_end = int(np.floor(split_percent*data_size))
    
    x_train = data[:train_end,:-1,:]
    y_train = data[:train_end,-1,:]
    
 
    
    x_test = data[train_end:,:-1,:]
    y_test = data[train_end:,-1,:]
    
    return [x_train, y_train, x_test, y_test]



seq_len = 20 # choose sequence length

seq_prices_dataset = generate_data(prices_dataset_norm,seq_len)

x_train, y_train, x_test, y_test = generate_train_test(seq_prices_dataset, 0.8)

print(&#39;x_train.shape = &#39;,x_train.shape)
print(&#39;y_train.shape = &#39;, y_train.shape)
print(&#39;x_test.shape = &#39;, x_test.shape)
print(&#39;y_test.shape = &#39;,y_test.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; x_train.shape =  (13675, 19, 4)
y_train.shape =  (13675, 4)
x_test.shape =  (3419, 19, 4)
y_test.shape =  (3419, 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;xây-dựng-mô-hình-sử-dụng-keras&#34;&gt;Xây dựng mô hình sử dụng keras&lt;/h3&gt;

&lt;p&gt;Ở đây mình sử dụng keras xây dựng mô hình ANN. Mô hình của mình xây dựng gồm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Sequential()

model.add(LSTM(
    input_dim=4,
    output_dim=50,
    return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(
    100,
    return_sequences=False))
model.add(Dropout(0.2))

model.add(Dense(
    output_dim=4))
model.add(Activation(&#39;linear&#39;))



model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])
checkpoint = ModelCheckpoint(filepath=&#39;my_model_stock.h5&#39;, verbose=1, save_best_only=True)
hist = model.fit(x_train, y_train, epochs=300, batch_size=128, verbose=1, callbacks=[checkpoint], validation_split=0.2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau một thời gian chạy, mình cũng thu được model. Các bạn quan tâm có thể download model của mình huấn luyện được tại &lt;a href=&#34;https://drive.google.com/open?id=1ImHQM9yWmOjpF5tjmSI9oqAi5BORa9Rs&#34;&gt;https://drive.google.com/open?id=1ImHQM9yWmOjpF5tjmSI9oqAi5BORa9Rs&lt;/a&gt; . Tiến hành plot dữ liệu tập test lên xem kết quả như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model =load_model(&#39;sp500_stockperdict.h5&#39;)


y_hat = model.predict(x_test)

ft = 3 # 0 = open, 1 = highest, 2 =lowest , 3 = close

plt.plot( y_test[:,ft], color=&#39;blue&#39;, label=&#39;target&#39;)

plt.plot( y_hat[:,ft], color=&#39;red&#39;, label=&#39;prediction&#39;)

plt.title(&#39;future stock prices&#39;)
plt.xlabel(&#39;time [days]&#39;)
plt.ylabel(&#39;normalized price&#39;)
plt.legend(loc=&#39;best&#39;)

plt.show()

from sklearn.metrics import mean_squared_error

# 0 = open, 1 = highest, 2 =lowest , 3 = close
print(&amp;quot;open error: &amp;quot;)
print(mean_squared_error(y_test[:,0], y_hat[ :,0]))

print(&amp;quot;highest error: &amp;quot;)
print(mean_squared_error(y_test[:,1], y_hat[ :,1]))

print(&amp;quot;lowest error: &amp;quot;)
print(mean_squared_error(y_test[:,2], y_hat[ :,2]))

print(&amp;quot;close error: &amp;quot;)
print(mean_squared_error(y_test[:,3], y_hat[ :,3]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sp500index_predict.png&#34; alt=&#34;hình chứng khoán&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;open error:
0.0009739211460315127
highest error:
0.0010539412808401607
lowest error:
0.0010066509540756113
close error:
0.0010840500965408758
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả của mô hình trông khá tốt, về hình dạng thì khá tương đồng với kết quả. Chúng ta có thể cải tiến model bằng cách nâng số lượng layter/ hidden node.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dự đoán chứng khoán sử dụng tensorflow</title>
      <link>/blog/2018-11-03-stock-prediction/</link>
      <pubDate>Sat, 03 Nov 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-11-03-stock-prediction/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Ở bài viết này, mình sẽ xây dựng mô hình hơn giản để áp dụng vào tập dữ liệu giá chứng khoán. Mục tiêu của bài này là chúng ta sẽ dự đoán chỉ số S&amp;amp;P 500 dựa trên chỉ số của 500 mã chứng khoán. Các bạn có nhu cầu tìm hiểu thêm về chỉ số sp 500 có thể đọc thêm ở &lt;a href=&#34;https://vi.wikipedia.org/wiki/S%26P_500&#34;&gt;https://vi.wikipedia.org/wiki/S%26P_500&lt;/a&gt;. Đây là một ứng dụng nhỏ, không có ý nghĩa nhiều ở thực tế do khi phân tích chứng khoán, ta còn xét thêm rất nhiều yếu tố phụ nữa. Mô hình này thực chất chỉ là một trong những mô hình chơi chơi.&lt;/p&gt;

&lt;h2 id=&#34;dẫn-nhập&#34;&gt;Dẫn nhập&lt;/h2&gt;

&lt;h3 id=&#34;phân-tích-dữ-liệu&#34;&gt;Phân tích dữ liệu&lt;/h3&gt;

&lt;p&gt;Các bạn có thể download dữ liệu ở &lt;a href=&#34;https://drive.google.com/open?id=1UTlj5Ced-yj6RBRVc6bBM6IWMjfQR3GR&#34;&gt;https://drive.google.com/open?id=1UTlj5Ced-yj6RBRVc6bBM6IWMjfQR3GR&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Đầu tiên, chúng ta sẽ dùng pandas để load mô hình lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Import data
data = pd.read_csv(&#39;data_stocks.csv&#39;)


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Xem kích thước của dữ liệu:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(data.shape)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(41266, 502)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả là ta có hơn 40 ngàn dòng và 502 cột. Thử show 10 row đầu tiên của dữ liệu lên xem như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(data.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;         DATE      SP500  NASDAQ.AAL  NASDAQ.AAPL  NASDAQ.ADBE  NASDAQ.ADI  \
0  1491226200  2363.6101     42.3300     143.6800     129.6300      82.040   
1  1491226260  2364.1001     42.3600     143.7000     130.3200      82.080   
2  1491226320  2362.6799     42.3100     143.6901     130.2250      82.030   
3  1491226380  2364.3101     42.3700     143.6400     130.0729      82.000   
4  1491226440  2364.8501     42.5378     143.6600     129.8800      82.035   

   NASDAQ.ADP  NASDAQ.ADSK  NASDAQ.AKAM  NASDAQ.ALXN    ...     NYSE.WYN  \
0    102.2300      85.2200       59.760       121.52    ...       84.370   
1    102.1400      85.6500       59.840       121.48    ...       84.370   
2    102.2125      85.5100       59.795       121.93    ...       84.585   
3    102.1400      85.4872       59.620       121.44    ...       84.460   
4    102.0600      85.7001       59.620       121.60    ...       84.470   

   NYSE.XEC  NYSE.XEL  NYSE.XL  NYSE.XOM  NYSE.XRX  NYSE.XYL  NYSE.YUM  \
0   119.035     44.40    39.88     82.03      7.36     50.22     63.86   
1   119.035     44.11    39.88     82.03      7.38     50.22     63.74   
2   119.260     44.09    39.98     82.02      7.36     50.12     63.75   
3   119.260     44.25    39.99     82.02      7.35     50.16     63.88   
4   119.610     44.11    39.96     82.03      7.36     50.20     63.91   

   NYSE.ZBH  NYSE.ZTS  
0   122.000    53.350  
1   121.770    53.350  
2   121.700    53.365  
3   121.700    53.380  
4   121.695    53.240  

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cột đầu tiên là ngày, sau đó là mã chứng khoán. Chúng ta có tổng cộng 500 mã chứng khoán và 1 chỉ số. Để ý cột Date, ta thấy giá trị đầu tiên là 1491226200, giá trị thứ 2 là 1491226260, giá trị thứ 3 là 1491226320, mỗi giá trị cách nhau 60. Chuyển đổi số 1491226200 sang dạng datetime thì ra giá trị  Monday, April 3, 2017 1:30:00 PM giờ GMT, tương tự số 1491226260 ra Monday, April 3, 2017 1:31:00 PM giờ GMT. Ta có thể suy luận ra là giá trị giao dịch lưu theo từng phút một (khoảng interval là 60 giây), và dữ liệu chúng ta có bắt đầu vào 3 tháng 4 năm 2017.&lt;/p&gt;

&lt;p&gt;Plot đồ thị của mã SP500 lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

plt.plot(data[&#39;SP500&#39;])
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/sp500index.png&#34; alt=&#34;Hình ảnh đừng đồ thị của chỉ số sp 500&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; Notes: Ở đây có một lưu ý nhỏ nhưng rất quan trọng. Đó là tại thời điểm phút thứ t lưu trữ giá trị sp500 của thời điểm phút thứ t+1. Ví dụ với chỉ số sp500, dòng đầu tiên ta thấy là 1491226200  2363.6101, nghĩa là giá thực tế của thời điểm 1491226260 là 2363.6101. Do bài toán của chúng ta là dữ đoán giá tương lại, nên tại thời điểm hiện tại ta sẽ dự đoán giá 1 phút sau sẽ bằng bao nhiêu. Và tập dữ liệu đã tự động dịch chuyển giá trị lên 1 phút cho chúng ta đỡ mất công làm. Còn giá của 500 cỗ phiếu còn lại vẫn là giá tại thời điểm t
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;phân-chia-tập-train-và-test&#34;&gt;Phân chia tập train và test.&lt;/h3&gt;

&lt;p&gt;Chúng ta sẽ chia dữ liệu thành 2 phần với 80% là train và 20% còn lại là test. Do tích chất của dữ liệu là time serial nên chúng ta không thể làm thay đổi thứ tự dữ liệu.&lt;/p&gt;

&lt;p&gt;Chúng ta sẽ bỏ đi cột DATE đầu tiên, và sau đó chuyển dữ liệu về dạng numpy array để dễ dàng thực hiện các phép chuyển đổi.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; data_ = data_raw.drop([&#39;DATE&#39;], 1)

data = data_.values
# Training and test data
train_start = 0
train_end = int(np.floor(0.8*n))
test_start = train_end
test_end = n
data_train = data[ :train_end]
data_test = data[train_end:]
 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scale-dữ-liệu&#34;&gt;Scale dữ liệu&lt;/h3&gt;

&lt;p&gt;Khi sử dụng ANN, chúng ta thông thường sẽ scale dữ liệu input về đoạn [-1,1]. Trong python, thư viện sklearn đã hỗ trợ cho chúng ta sẵn các hàm scale dữ liệu cần thiết.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Scale data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data_train = scaler.fit_transform(data_train)
data_test = scaler.transform(data_test)
# Build X and y
X_train = data_train[:, 1:]
y_train = data_train[:, 0]
X_test = data_test[:, 1:]
y_test = data_test[:, 0]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mình cần dự đoán giá trị của chỉ số sp 500, nên giá trị của sp500 sẽ là cái mình cần dự đoán, chính là cột đầu tiên, còn 500 cái còn lại là input của mình.&lt;/p&gt;

&lt;h3 id=&#34;xây-dựng-mô-hình-sử-dụng-keras&#34;&gt;Xây dựng mô hình sử dụng keras&lt;/h3&gt;

&lt;p&gt;Ở đây mình sử dụng keras xây dựng mô hình ANN. Mô hình của mình xây dựng gồm&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.callbacks import ModelCheckpoint
from keras.optimizers import SGD

import os
os.environ[&amp;quot;CUDA_DEVICE_ORDER&amp;quot;]=&amp;quot;PCI_BUS_ID&amp;quot;
# The GPU id to use, usually either &amp;quot;0&amp;quot; or &amp;quot;1&amp;quot;
os.environ[&amp;quot;CUDA_VISIBLE_DEVICES&amp;quot;]=&amp;quot;0&amp;quot; 
# create model
model = Sequential()
model.add(Dense(2048, input_dim=input_dim,kernel_initializer=&#39;normal&#39;, activation=&#39;relu&#39;))
model.add(Dense(1024,kernel_initializer=&#39;normal&#39;, activation=&#39;relu&#39;))
model.add(Dense(512,kernel_initializer=&#39;normal&#39;, activation=&#39;relu&#39;))
model.add(Dense(256,kernel_initializer=&#39;normal&#39;, activation=&#39;relu&#39;))
model.add(Dense(128,kernel_initializer=&#39;normal&#39;, activation=&#39;relu&#39;))
model.add(Dense(1,kernel_initializer=&#39;normal&#39;))



model.compile(loss=&#39;mse&#39;, optimizer=&#39;rmsprop&#39;)
checkpoint = ModelCheckpoint(filepath=&#39;my_model3.h5&#39;, verbose=1, save_best_only=True)
model.fit(X_train, y_train, epochs=100, batch_size=256, verbose=1, callbacks=[checkpoint], validation_split=0.2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sau một thời gian chạy, mình cũng thu được model. Các bạn quan tâm có thể download model của mình huấn luyện được tại &lt;a href=&#34;https://drive.google.com/open?id=1BLQZbcADfnLqzIHlkgpsqZBlhljBp1Eb&#34;&gt;https://drive.google.com/open?id=1BLQZbcADfnLqzIHlkgpsqZBlhljBp1Eb&lt;/a&gt; . Tiến hành plot dữ liệu tập test lên xem kết quả như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
yhat = model.predict(X_test)


x = np.arange(len(yhat))

plt.plot(x, y_test)
plt.plot(x, yhat)
plt.legend([&#39;real&#39;, &#39;test&#39;], loc=&#39;upper right&#39;)
plt.show()


from sklearn.metrics import mean_squared_error

print(&amp;quot;mse: &amp;quot;+ str(mean_squared_error(y_test, yhat)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/chung_khoan_1.png&#34; alt=&#34;hình chứng khoán&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; mse: 0.0014582120695331884
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả của mô hình tạm chấp nhận được, về hình dạng thì khá tương đồng với kết quả. Chúng ta có thể cải tiến model bằng cách nâng số lượng layter/ hidden node, hoặc thêm dropout. Hoặc có thể thay thế mô hình bằng RNN. Chúng ta sẽ đề cập đến mô hình RNN trong bài viết sau.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overview of gradient descent optimization algorithm</title>
      <link>/blog/2018-11-01-overview-of-gradient-descent-optimization-algorithm/</link>
      <pubDate>Mon, 29 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-11-01-overview-of-gradient-descent-optimization-algorithm/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Sau khi thực hiện bài phân loại chó mèo bằng keras, mình phát hiện rằng keras có hỗ trợ rất nhiều thuật toán tối ưu hoá &lt;a href=&#34;https://keras.io/optimizers/&#34;&gt;https://keras.io/optimizers/&lt;/a&gt;. Nhân dịp rãnh rỗi, mình sẽ tổng hợp lại một vài thuật toán mà keras hỗ trợ.&lt;/p&gt;

&lt;h2 id=&#34;dẫn-nhập&#34;&gt;Dẫn nhập&lt;/h2&gt;

&lt;p&gt;Tại thời điểm hiện tại, Gradient descent là một trong những thuật toán phổ biến được sử dụng để tối ưu hoá mạng neural networks. Các thư viện DNN sẽ implement kèm theo một vài biến thể của gradient descent giúp người dùng dễ dàng sử dụng công cụ hơn.&lt;/p&gt;

&lt;p&gt;Bài viết này mình sẽ cập nhật dần đến khi hoàn thiện.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân loại chó mèo sử dụng pretrain model</title>
      <link>/blog/2018-10-29-phan-loai-cho-meo/</link>
      <pubDate>Mon, 29 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-29-phan-loai-cho-meo/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Bài toán phân loại chó mèo là bài toán khá cũ tại thời điểm hiện tại. Tuy nhiên, đối với các bạn mới bước chân vào con đường machine learning thì đây là một trong những bài toán cơ bản để các bạn thực hành sử dụng và tìm hiểu thư viện mà mình đang có. Ở đây, chúng ta sẽ sử dụng pretrain model có sẵn của kares áp dụng trên tập dữ liệu. Các bạn có thể download tập dữ liệu train và test ở địa chỉ &lt;a href=&#34;https://www.kaggle.com/c/dogs-vs-cats/download/train.zip&#34;&gt;https://www.kaggle.com/c/dogs-vs-cats/download/train.zip&lt;/a&gt; và &lt;a href=&#34;https://www.kaggle.com/c/dogs-vs-cats/download/test1.zip&#34;&gt;https://www.kaggle.com/c/dogs-vs-cats/download/test1.zip&lt;/a&gt; để bắt đầu thực hiện.&lt;/p&gt;

&lt;h2 id=&#34;thực-hiện&#34;&gt;Thực hiện&lt;/h2&gt;

&lt;p&gt;Sau khi giải nén dữ liệu, ta thấy rằng thư mục train có cấu trúc đặt trên sẽ là label.số thứ tự.jpg. Trong đó label có thể là dog hoặc cat, số thứ tự tăng dần từ 0 đến &amp;hellip;. 12499. Để đảm bảo đúng với mô hình, ta phải cấu trúc lại dữ liệu thành dạng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir/classname1/*.*
data_dir/classname2/*.*
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vì vậy, ta tạo ra thư mục cat và copy những file bắt đầu bằng cat.* vào thư mục cat. Làm tương tự với thư mục dog.&lt;/p&gt;

&lt;p&gt;Đầu tiên, các bạn download file pretrain model, giải nén ra và để ở đâu đó trong ổ cứng của máy bạn. Đường dẫn file pretrain model các bạn có thể download ở &lt;a href=&#34;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&#34;&gt;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&lt;/a&gt;. Các bạn có thể download các file pretrain khác nếu có hứng thú tìm hiểu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load dataset lên và tranform nó để đưa vào huấn luyện.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys
import os
from collections import defaultdict
import numpy as np
import scipy.misc


def preprocess_input(x0):
    x = x0 / 255.
    x -= 0.5
    x *= 2.
    return x


def reverse_preprocess_input(x0):
    x = x0 / 2.0
    x += 0.5
    x *= 255.
    return x


def dataset(base_dir, n):
    print(&amp;quot;base dir: &amp;quot;+base_dir)
    print(&amp;quot;n: &amp;quot;+str(n))
    n = int(n)
    d = defaultdict(list)
    for root, subdirs, files in os.walk(base_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            assert file_path.startswith(base_dir)
            
            suffix = file_path[len(base_dir):]
            
            suffix = suffix.lstrip(&amp;quot;/&amp;quot;)
            suffix = suffix.lstrip(&amp;quot;\\&amp;quot;)
            if(suffix.find(&#39;/&#39;)&amp;gt;-1): #linux
                label = suffix.split(&amp;quot;/&amp;quot;)[0]
            else: #window
                label = suffix.split(&amp;quot;\\&amp;quot;)[0]
            d[label].append(file_path)
    print(&amp;quot;walk directory complete&amp;quot;)
    tags = sorted(d.keys())

    processed_image_count = 0
    useful_image_count = 0

    X = []
    y = []

    for class_index, class_name in enumerate(tags):
        filenames = d[class_name]
        for filename in filenames:
            processed_image_count += 1
            if processed_image_count%100 ==0:
                print(class_name+&amp;quot;\tprocess: &amp;quot;+str(processed_image_count)+&amp;quot;\t&amp;quot;+str(len(d[class_name])))
            img = scipy.misc.imread(filename)
            height, width, chan = img.shape
            assert chan == 3
            aspect_ratio = float(max((height, width))) / min((height, width))
            if aspect_ratio &amp;gt; 2:
                continue
            # We pick the largest center square.
            centery = height // 2
            centerx = width // 2
            radius = min((centerx, centery))
            img = img[centery-radius:centery+radius, centerx-radius:centerx+radius]
            img = scipy.misc.imresize(img, size=(n, n), interp=&#39;bilinear&#39;)
            X.append(img)
            y.append(class_index)
            useful_image_count += 1
    print(&amp;quot;processed %d, used %d&amp;quot; % (processed_image_count, useful_image_count))

    X = np.array(X).astype(np.float32)
    #X = X.transpose((0, 3, 1, 2))
    X = preprocess_input(X)
    y = np.array(y)

    perm = np.random.permutation(len(y))
    X = X[perm]
    y = y[perm]

    print(&amp;quot;classes:&amp;quot;,end=&amp;quot; &amp;quot;)
    for class_index, class_name in enumerate(tags):
        print(class_name, sum(y==class_index),end=&amp;quot; &amp;quot;)
    print(&amp;quot;X shape: &amp;quot;,X.shape)

    return X, y, tags
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đoạn code trên khá đơn giản và dễ hiểu. Lưu ý ở đây là với những bức ảnh có tỷ lệ width và height &amp;gt; 2 thì mình sẽ loại chúng ra khỏi tập dữ liệu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ xây dựng mô hình dựa trên mô hình InceptionV3 có sẵn, thêm một lớp softmax ở cuối để phân lớp dữ liệu, chúng ta sẽ huấn luyện lớp softmax này. Các lớp trước lớp softmax này sẽ bị đóng băng (không cập nhật trọng số trong quá trình huấn luyện ).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# create the base pre-trained model
def build_model(nb_classes):
    base_model = InceptionV3(weights=&#39;imagenet&#39;, include_top=False)

    # add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    # let&#39;s add a fully-connected layer
    x = Dense(1024, activation=&#39;relu&#39;)(x)
    # and a logistic layer
    predictions = Dense(nb_classes, activation=&#39;softmax&#39;)(x)

    # this is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)

    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional InceptionV3 layers
    for layer in base_model.layers:
        layer.trainable = False

    # compile the model (should be done *after* setting layers to non-trainable)
    print(&amp;quot;starting model compile&amp;quot;)
    compile(model)
    print(&amp;quot;model compile done&amp;quot;)
    return model
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visualize một chút xíu về kiến trúc inceptionV3 mình đang dùng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None, None, 3 0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, None, None, 3 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, None, None, 3 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, None, None, 6 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, None, None, 6 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, None, None, 8 5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, None, None, 8 240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, None, None, 8 0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, None, None, 1 138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, None, None, 1 576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, None, None, 6 192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, None, None, 6 0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, None, None, 9 55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, None, None, 4 144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, None, None, 4 0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, None, None, 1 0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, None, None, 6 76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, None, None, 9 82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, None, None, 3 6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, None, None, 6 192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, None, None, 9 288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, None, None, 3 96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, None, None, 6 0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, None, None, 9 0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, None, None, 3 0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, None, None, 2 0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, None, None, 6 192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, None, None, 6 0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, None, None, 9 55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, None, None, 4 144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, None, None, 4 0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, None, None, 6 76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, None, None, 9 82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, None, None, 6 192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, None, None, 9 288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, None, None, 6 0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, None, None, 9 0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, None, None, 2 0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, None, None, 6 192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, None, None, 6 0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, None, None, 9 55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, None, None, 4 144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, None, None, 4 0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, None, None, 2 0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, None, None, 6 76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, None, None, 9 82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, None, None, 6 192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, None, None, 9 288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, None, None, 6 192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, None, None, 6 0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, None, None, 9 0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, None, None, 6 0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, None, None, 2 0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, None, None, 6 192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, None, None, 6 0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, None, None, 9 55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, None, None, 9 82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, None, None, 3 1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, None, None, 9 288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, None, None, 3 0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, None, None, 9 0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, None, None, 7 0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, None, None, 1 114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, None, None, 1 114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, None, None, 1 384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, None, None, 1 384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, None, None, 1 172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, None, None, 1 172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, None, None, 1 576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, None, None, 1 576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, None, None, 7 0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, None, None, 1 179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, None, None, 1 179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, None, None, 1 480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, None, None, 1 480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, None, None, 1 215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, None, None, 1 215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, None, None, 1 576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, None, None, 1 576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, None, None, 7 0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, None, None, 1 179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, None, None, 1 179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, None, None, 1 480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, None, None, 1 480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, None, None, 1 215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, None, None, 1 215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, None, None, 1 576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, None, None, 1 576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, None, None, 7 0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, None, None, 7 0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, None, None, 1 258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, None, None, 1 258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, None, None, 7 0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, None, None, 1 258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, None, None, 1 576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, None, None, 1 0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, None, None, 3 552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, None, None, 1 331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, None, None, 3 960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, None, None, 1 576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, None, None, 3 0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, None, None, 1 0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, None, None, 1 0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, None, None, 4 1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, None, None, 4 0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, None, None, 3 1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, None, None, 3 442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, None, None, 3 442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, None, None, 1 0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, None, None, 3 1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, None, None, 3 1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, None, None, 3 960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, None, None, 3 0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, None, None, 3 0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, None, None, 1 576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, None, None, 1 0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, None, None, 2 0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, None, None, 4 1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, None, None, 4 0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, None, None, 3 1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, None, None, 3 442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, None, None, 3 442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, None, None, 2 0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, None, None, 3 1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, None, None, 3 1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, None, None, 1 393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, None, None, 3 960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, None, None, 3 0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, None, None, 3 0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, None, None, 1 576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, None, None, 1 0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, None, None, 2 0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 2)            2050        dense_1[0][0]
==================================================================================================
Total params: 23,903,010
Trainable params: 2,100,226
Non-trainable params: 21,802,784
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phần train lại sẽ có khoảng hơn 2 triệu tham số, phần layter ở trước đó không train là khoảng 21 triệu tham số.&lt;/p&gt;

&lt;p&gt;Đồ hình của model (các bạn có thể download về rồi zoom bự lên để xem rõ hơn).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/inception_v2_model_311.png&#34; alt=&#34;Hình ảnh kết quả nhận dạng chó mèo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Chia tập dữ liệu ra thành 5 phần, 4 phần làm tập train, 1 phần làm tập validation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y, tags = dataset.dataset(data_directory, n)
nb_classes = len(tags)


sample_count = len(y)
train_size = sample_count * 4 // 5
X_train = X[:train_size]
y_train = y[:train_size]
Y_train = np_utils.to_categorical(y_train, nb_classes)
X_test  = X[train_size:]
y_test  = y[train_size:]
Y_test = np_utils.to_categorical(y_test, nb_classes)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Để chống overfit, chúng ta sẽ thêm một số yếu tố như thực hiện các phép biến đổi affine trên ảnh gốc.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;datagen = ImageDataGenerator(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,
        zca_whitening=False,
        rotation_range=45,
        width_shift_range=0.25,
        height_shift_range=0.25,
        horizontal_flip=True,
        vertical_flip=False,
        zoom_range=0.5,
        channel_shift_range=0.5,
        fill_mode=&#39;nearest&#39;)
        
datagen.fit(X_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cuối cùng, chúng ta sẽ xây dựng mô hình và tiến hành huấn luyện, lưu mô hình. Quá trình này tốn hơi nhiều thời gian.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
model = net.build_model(nb_classes)
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&amp;quot;accuracy&amp;quot;])

# train the model on the new data for a few epochs

print(&amp;quot;training the newly added dense layers&amp;quot;)

samples_per_epoch = X_train.shape[0]//batch_size*batch_size
steps_per_epoch = samples_per_epoch//batch_size
validation_steps = X_test.shape[0]//batch_size*batch_size

model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),
            samples_per_epoch=samples_per_epoch,
            epochs=nb_epoch,
            steps_per_epoch = steps_per_epoch,
            validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size),
            validation_steps=validation_steps,
            )


net.save(model, tags, model_file_prefix)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Độ chính xác trên tập train.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_pred = model.predict(X_test, batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis=1)

accuracy = float(np.sum(y_test==y_pred)) / len(y_test)
print(&amp;quot;accuracy: &amp;quot;, accuracy)

confusion = np.zeros((nb_classes, nb_classes), dtype=np.int32)
for (predicted_index, actual_index, image) in zip(y_pred, y_test, X_test):
    confusion[predicted_index, actual_index] += 1

print(&amp;quot;rows are predicted classes, columns are actual classes&amp;quot;)
for predicted_index, predicted_tag in enumerate(tags):
    print(predicted_tag[:7], end=&#39;&#39;, flush=True)
    for actual_index, actual_tag in enumerate(tags):
        print(&amp;quot;\t%d&amp;quot; % confusion[predicted_index, actual_index], end=&#39;&#39;)
    print(&amp;quot;&amp;quot;, flush=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy:  0.9907213167661771
rows are predicted classes, columns are actual classes
cat     12238   106
dog     124     12320
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả đạt 0.99 trên tập train, khá tốt phải không các bạn.&lt;/p&gt;

&lt;p&gt;Các bạn có thể download mô hình mình đã huấn luyện ở &lt;a href=&#34;https://drive.google.com/open?id=1qQo8gj3KA6c1rPmJMVS_FZkVDcDmRgSf&#34;&gt;https://drive.google.com/open?id=1qQo8gj3KA6c1rPmJMVS_FZkVDcDmRgSf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thử show ra kết quả trên tập test xem như thế nào.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_pred = model.predict(X_test, batch_size=batch_size)
y_pred = np.argmax(Y_pred, axis=1)

lst_img = []

columns = 5
rows = 5
# fig,= plt.figure(rows)
for idx, val in enumerate(X_test):
    pred =y_pred[idx]
    label = &amp;quot;{}: {:.2f}%&amp;quot;.format(tags[pred], Y_pred[idx][pred] * 100)
    image = dataset.reverse_preprocess_input(val)
    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
    cv2.putText(image,label , (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,0.7, (255, 000, 0), 2)

    plt.subplot(rows,rows,idx+1)
    plt.imshow(image)
    plt.title(label)
    plt.axis(&#39;off&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/nhan_dang_cho_meo1.png&#34; alt=&#34;Hình ảnh kết quả nhận dạng chó mèo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả có một số hình mèo bị nhận nhầm là chó, và một số hình không phải mèo, không phải chó. Nhìn chung kết quả cũng không đến nỗi nào quá tệ.&lt;/p&gt;

&lt;h2 id=&#34;quậy-phá-mô-hình&#34;&gt;Quậy phá mô hình&lt;/h2&gt;

&lt;p&gt;Mô hình InceptionV3 chúng ta đang xài có tổng cộng 311 lớp, chúng ta sẽ tiến hành một số pha quậy phá mô hình xem kết quả như trả ra như thế nào&lt;/p&gt;

&lt;h3 id=&#34;quậy-phá-1-mở-đóng-băng-một-số-lớp-cuối-và-train-trên-chúng&#34;&gt;Quậy phá 1: Mở đóng băng một số lớp cuối và train trên chúng.&lt;/h3&gt;

&lt;p&gt;Nếu các bạn để ý kỹ, trong đoạn mã nguồn của mình có đoạn&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional InceptionV3 layers
    for layer in base_model.layers:
        layer.trainable = False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nghĩa là mình đóng băng toàn bộ 311 lớp, không cho nó train mà chỉ lấy kết quả của nó train lớp softmax cuối cùng. Bây giờ mình sẽ thử nghiệm với việc là để 299 lớp ban đầu vẫn đóng băng, và train lại toàn bộ các lớp còn lại (Các bạn đừng thắc mắc vì sao lại là 299 nha, do mình thích thôi).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for layer in model.layers[:299]:
    layer.trainable = False
for layer in model.layers[299:]:
    layer.trainable = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đồ hình của mô đồ khá giống ở trên, mình chỉ post lại kết quả của số param.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;==================================================================================================
Total params: 23,903,010
Trainable params: 2,493,954
Non-trainable params: 21,409,056
__________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Như vậy là có khoảng 2 triệu 5 tham số được huấn luyện lại&lt;/p&gt;

&lt;p&gt;Model của mình huấn luyện được các bạn có thể download ở &lt;a href=&#34;https://drive.google.com/open?id=1Ts18LICUAh6gcOnXcmuVr7PUG5IxpCdt&#34;&gt;https://drive.google.com/open?id=1Ts18LICUAh6gcOnXcmuVr7PUG5IxpCdt&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Kết quả đạt được:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy:  0.9834610730133119
rows are predicted classes, columns are actual classes
cat     2429    69
dog     13      2447
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/dog_cat_v1.png&#34; alt=&#34;Hình ảnh kết quả nhận dạng chó mèo đóng băng 299 lớp đầu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả 25 hình ngẫu nhiên cũng khá giống kết quả ở trước đó. Một số hình không có con vật bị nhận nhầm như hình còn mèo ở góc phải trên bị nhận nhầm là chó. Tuy nhiên, với chất lượng hình ảnh như thế này thì mình thấy kết quả như vậy là khá tuyệt vời.&lt;/p&gt;

&lt;h3 id=&#34;quậy-phá-2-chỉ-sử-dụng-72-lớp-đầu-tiên-của-inception&#34;&gt;Quậy phá 2: Chỉ sử dụng 72 lớp đầu tiên của inception.&lt;/h3&gt;

&lt;p&gt;Ở lần thí nghiệm này, mình sẽ chỉ sử dụng 72 lớp đầu tiên của inception để huấn luyện. Mình sẽ sửa lại một xíu ở hàm build model như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = base_model.layers[72].output
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Một lưu ý nhỏ là do inception không có tính tuần tự giữa các lớp (các bạn có thể nhìn hình ở trên sẽ thấy rõ), nên index sẽ không phải là 72 như thông thường.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ thực hiện việc huấn luyện lại mô hình và kết quả là:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy:  0.5494150867285196
rows are predicted classes, columns are actual classes
cat     339     131
dog     2103    2385
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả khá tệ, lý do là mô hình các layer không theo sequence, mình lấy ngẫu nhiêu 72 lớp làm thông tin feature của các hình bị mất mát nhiều (ví dụ trường hợp layey 80 là tổng hợp thông tin của layter 79 + layter 4 + layer 48, mà mình chỉ lấy 72 layter đầu, nên sẽ mất đi phần đóng góp cực kỳ quan trọng của layter 4 và 48 ở lớp cao hơn).&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mask R-CNN trong bài toán nhận dạng và phân vùng đối tượng</title>
      <link>/blog/2018-10-08-mask-rnn/</link>
      <pubDate>Mon, 08 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-08-mask-rnn/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Phân vùng đối tượng là một bài toán khá phổ biến trong lĩnh vực computer vision. Trong open cv có hỗ trợ cho chúng ta một số hàm để phân vùng đối tượng rất dễ sử dụng. Đặc điểm chung của các hàm này là độ chính xác không được cao cho lắm. Ở bài viết này, chúng ta sẽ tìm hiểu cách sử dụng mô hình pretrain của DNN để phân vùng các đối tượng trong ảnh.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-pretrain-model&#34;&gt;Sử dụng pretrain model&lt;/h2&gt;

&lt;p&gt;Đầu tiên, các bạn download file pretrain model, giải nén ra và để ở đâu đó trong ổ cứng của máy bạn. Đường dẫn file pretrain model các bạn có thể download ở &lt;a href=&#34;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&#34;&gt;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&lt;/a&gt;. Các bạn có thể download các file pretrain khác nếu có hứng thú tìm hiểu.&lt;/p&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load mô hình lên:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import os
import sys
import tarfile
import tensorflow as tf

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image
import PIL.ImageDraw as ImageDraw
import PIL.ImageFont as ImageFont
import cv2

import pprint

import PIL.Image as Image
import PIL.ImageColor as ImageColor

# Model preparation


# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_CKPT = &#39;mask_rcnn_inception_v2_coco_2018_01_28&#39; + &#39;/frozen_inference_graph.pb&#39;

# List of the strings that is used to add correct label for each box.
#PATH_TO_LABELS = &#39;mscoco_label_map.pbtxt&#39;

NUM_CLASSES = 1


# categories

category_index = {1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;person&#39;},
# 3: {&#39;id&#39;: 3, &#39;name&#39;: &#39;car&#39;},
 }

detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, &#39;rb&#39;) as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ở đây, mình chỉ demo detect người trong hình, nên mình chỉ để category_index chỉ là &amp;ldquo;person&amp;rdquo;. Thực tế, mô hình COCO hỗ trợ cho chúng ta nhận dạng 90 loại đối tượng khác nhau, các bạn có nhu cầu tìm hiểu thì thay bằng đoạn mã sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;category_index = {1: {&#39;id&#39;: 1, &#39;name&#39;: &#39;person&#39;},
 2: {&#39;id&#39;: 2, &#39;name&#39;: &#39;bicycle&#39;},
 3: {&#39;id&#39;: 3, &#39;name&#39;: &#39;car&#39;},
 4: {&#39;id&#39;: 4, &#39;name&#39;: &#39;motorcycle&#39;},
 5: {&#39;id&#39;: 5, &#39;name&#39;: &#39;airplane&#39;},
 6: {&#39;id&#39;: 6, &#39;name&#39;: &#39;bus&#39;},
 7: {&#39;id&#39;: 7, &#39;name&#39;: &#39;train&#39;},
 8: {&#39;id&#39;: 8, &#39;name&#39;: &#39;truck&#39;},
 9: {&#39;id&#39;: 9, &#39;name&#39;: &#39;boat&#39;},
 10: {&#39;id&#39;: 10, &#39;name&#39;: &#39;traffic light&#39;},
 11: {&#39;id&#39;: 11, &#39;name&#39;: &#39;fire hydrant&#39;},
 13: {&#39;id&#39;: 13, &#39;name&#39;: &#39;stop sign&#39;},
 14: {&#39;id&#39;: 14, &#39;name&#39;: &#39;parking meter&#39;},
 15: {&#39;id&#39;: 15, &#39;name&#39;: &#39;bench&#39;},
 16: {&#39;id&#39;: 16, &#39;name&#39;: &#39;bird&#39;},
 17: {&#39;id&#39;: 17, &#39;name&#39;: &#39;cat&#39;},
 18: {&#39;id&#39;: 18, &#39;name&#39;: &#39;dog&#39;},
 19: {&#39;id&#39;: 19, &#39;name&#39;: &#39;horse&#39;},
 20: {&#39;id&#39;: 20, &#39;name&#39;: &#39;sheep&#39;},
 21: {&#39;id&#39;: 21, &#39;name&#39;: &#39;cow&#39;},
 22: {&#39;id&#39;: 22, &#39;name&#39;: &#39;elephant&#39;},
 23: {&#39;id&#39;: 23, &#39;name&#39;: &#39;bear&#39;},
 24: {&#39;id&#39;: 24, &#39;name&#39;: &#39;zebra&#39;},
 25: {&#39;id&#39;: 25, &#39;name&#39;: &#39;giraffe&#39;},
 27: {&#39;id&#39;: 27, &#39;name&#39;: &#39;backpack&#39;},
 28: {&#39;id&#39;: 28, &#39;name&#39;: &#39;umbrella&#39;},
 31: {&#39;id&#39;: 31, &#39;name&#39;: &#39;handbag&#39;},
 32: {&#39;id&#39;: 32, &#39;name&#39;: &#39;tie&#39;},
 33: {&#39;id&#39;: 33, &#39;name&#39;: &#39;suitcase&#39;},
 34: {&#39;id&#39;: 34, &#39;name&#39;: &#39;frisbee&#39;},
 35: {&#39;id&#39;: 35, &#39;name&#39;: &#39;skis&#39;},
 36: {&#39;id&#39;: 36, &#39;name&#39;: &#39;snowboard&#39;},
 37: {&#39;id&#39;: 37, &#39;name&#39;: &#39;sports ball&#39;},
 38: {&#39;id&#39;: 38, &#39;name&#39;: &#39;kite&#39;},
 39: {&#39;id&#39;: 39, &#39;name&#39;: &#39;baseball bat&#39;},
 40: {&#39;id&#39;: 40, &#39;name&#39;: &#39;baseball glove&#39;},
 41: {&#39;id&#39;: 41, &#39;name&#39;: &#39;skateboard&#39;},
 42: {&#39;id&#39;: 42, &#39;name&#39;: &#39;surfboard&#39;},
 43: {&#39;id&#39;: 43, &#39;name&#39;: &#39;tennis racket&#39;},
 44: {&#39;id&#39;: 44, &#39;name&#39;: &#39;bottle&#39;},
 46: {&#39;id&#39;: 46, &#39;name&#39;: &#39;wine glass&#39;},
 47: {&#39;id&#39;: 47, &#39;name&#39;: &#39;cup&#39;},
 48: {&#39;id&#39;: 48, &#39;name&#39;: &#39;fork&#39;},
 49: {&#39;id&#39;: 49, &#39;name&#39;: &#39;knife&#39;},
 50: {&#39;id&#39;: 50, &#39;name&#39;: &#39;spoon&#39;},
 51: {&#39;id&#39;: 51, &#39;name&#39;: &#39;bowl&#39;},
 52: {&#39;id&#39;: 52, &#39;name&#39;: &#39;banana&#39;},
 53: {&#39;id&#39;: 53, &#39;name&#39;: &#39;apple&#39;},
 54: {&#39;id&#39;: 54, &#39;name&#39;: &#39;sandwich&#39;},
 55: {&#39;id&#39;: 55, &#39;name&#39;: &#39;orange&#39;},
 56: {&#39;id&#39;: 56, &#39;name&#39;: &#39;broccoli&#39;},
 57: {&#39;id&#39;: 57, &#39;name&#39;: &#39;carrot&#39;},
 58: {&#39;id&#39;: 58, &#39;name&#39;: &#39;hot dog&#39;},
 59: {&#39;id&#39;: 59, &#39;name&#39;: &#39;pizza&#39;},
 60: {&#39;id&#39;: 60, &#39;name&#39;: &#39;donut&#39;},
 61: {&#39;id&#39;: 61, &#39;name&#39;: &#39;cake&#39;},
 62: {&#39;id&#39;: 62, &#39;name&#39;: &#39;chair&#39;},
 63: {&#39;id&#39;: 63, &#39;name&#39;: &#39;couch&#39;},
 64: {&#39;id&#39;: 64, &#39;name&#39;: &#39;potted plant&#39;},
 65: {&#39;id&#39;: 65, &#39;name&#39;: &#39;bed&#39;},
 67: {&#39;id&#39;: 67, &#39;name&#39;: &#39;dining table&#39;},
 70: {&#39;id&#39;: 70, &#39;name&#39;: &#39;toilet&#39;},
 72: {&#39;id&#39;: 72, &#39;name&#39;: &#39;tv&#39;},
 73: {&#39;id&#39;: 73, &#39;name&#39;: &#39;laptop&#39;},
 74: {&#39;id&#39;: 74, &#39;name&#39;: &#39;mouse&#39;},
 75: {&#39;id&#39;: 75, &#39;name&#39;: &#39;remote&#39;},
 76: {&#39;id&#39;: 76, &#39;name&#39;: &#39;keyboard&#39;},
 77: {&#39;id&#39;: 77, &#39;name&#39;: &#39;cell phone&#39;},
 78: {&#39;id&#39;: 78, &#39;name&#39;: &#39;microwave&#39;},
 79: {&#39;id&#39;: 79, &#39;name&#39;: &#39;oven&#39;},
 80: {&#39;id&#39;: 80, &#39;name&#39;: &#39;toaster&#39;},
 81: {&#39;id&#39;: 81, &#39;name&#39;: &#39;sink&#39;},
 82: {&#39;id&#39;: 82, &#39;name&#39;: &#39;refrigerator&#39;},
 84: {&#39;id&#39;: 84, &#39;name&#39;: &#39;book&#39;},
 85: {&#39;id&#39;: 85, &#39;name&#39;: &#39;clock&#39;},
 86: {&#39;id&#39;: 86, &#39;name&#39;: &#39;vase&#39;},
 87: {&#39;id&#39;: 87, &#39;name&#39;: &#39;scissors&#39;},
 88: {&#39;id&#39;: 88, &#39;name&#39;: &#39;teddy bear&#39;},
 89: {&#39;id&#39;: 89, &#39;name&#39;: &#39;hair drier&#39;},
 90: {&#39;id&#39;: 90, &#39;name&#39;: &#39;toothbrush&#39;}} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ load một số hàm giúp hỗ trợ việc hậu xử lý ảnh để vẽ các mask cho chúng ta xem trực quan hơn.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
    draw  =  ImageDraw.Draw(image)
    im_width,  im_height  =  image.size
    if  use_normalized_coordinates:
        (left,  right,  top,  bottom)  =  (xmin  *  im_width,  xmax  *  im_width,
                                                                    ymin  *  im_height,  ymax  *  im_height)
    else:
        (left,  right,  top,  bottom)  =  (xmin,  xmax,  ymin,  ymax)
    draw.line([(left,  top),  (left,  bottom),  (right,  bottom),
                          (right,  top),  (left,  top)],  width=thickness,  fill=color)
    try:
        font  =  ImageFont.truetype(&#39;arial.ttf&#39;,  24)
    except  IOError:
        font  =  ImageFont.load_default()

    #  If  the  total  height  of  the  display  strings  added  to  the  top  of  the  bounding
    #  box  exceeds  the  top  of  the  image,  stack  the  strings  below  the  bounding  box
    #  instead  of  above.
    display_str_heights  =  [font.getsize(ds)[1]  for  ds  in  display_str_list]
    #  Each  display_str  has  a  top  and  bottom  margin  of  0.05x.
    total_display_str_height  =  (1  +  2  *  0.05)  *  sum(display_str_heights)

    if  top  &amp;gt;  total_display_str_height:
        text_bottom  =  top
    else:
        text_bottom  =  bottom  +  total_display_str_height
    #  Reverse  list  and  print  from  bottom  to  top.
    for  display_str  in  display_str_list[::-1]:
        text_width,  text_height  =  font.getsize(display_str)
        margin  =  np.ceil(0.05  *  text_height)
        draw.rectangle(
                [(left,  text_bottom  -  text_height  -  2  *  margin),  (left  +  text_width,
                                                                                                                    text_bottom)],
                fill=color)
        draw.text(
                (left  +  margin,  text_bottom  -  text_height  -  margin),
                display_str,
                fill=&#39;black&#39;,
                font=font)
        text_bottom  -=  text_height  -  2  *  margin



def visualize_boxes_and_labels_on_image_array(
        image,
        boxes,
        classes,
        scores,
        category_index,
        instance_masks=None,
        instance_boundaries=None,
        keypoints=None,
        use_normalized_coordinates=False,
        max_boxes_to_draw=20,
        min_score_thresh=.5,
        agnostic_mode=False,
        line_thickness=4,
        groundtruth_box_visualization_color=&#39;black&#39;,
        skip_scores=False,
        skip_labels=False):

    box_to_display_str_map = collections.defaultdict(list)
    box_to_color_map = collections.defaultdict(str)
    box_to_instance_masks_map = {}
    box_to_instance_boundaries_map = {}
    box_to_keypoints_map = collections.defaultdict(list)
    if not max_boxes_to_draw:
        max_boxes_to_draw = boxes.shape[0]
    #print(boxes)
    for i in range(min(max_boxes_to_draw, boxes.shape[0])):
        if scores is None or scores[i] &amp;gt; min_score_thresh:
            box = tuple(boxes[i].tolist())
        if instance_masks is not None:
            box_to_instance_masks_map[box] = instance_masks[i]
        if instance_boundaries is not None:
            box_to_instance_boundaries_map[box] = instance_boundaries[i]
        if keypoints is not None:
            box_to_keypoints_map[box].extend(keypoints[i])
        if scores is None:
            box_to_color_map[box] = groundtruth_box_visualization_color
        else:
            display_str = &#39;&#39;
            if not skip_labels:
                if not agnostic_mode:
                    if classes[i] in category_index.keys():
                        class_name = category_index[classes[i]][&#39;name&#39;]
                    else:
                        class_name = &#39;N/A&#39;
                    display_str = str(class_name)
            if not skip_scores:
                if not display_str:
                    display_str = &#39;{}%&#39;.format(int(100 * scores[i]))
                else:
                    display_str = &#39;{}: {}%&#39;.format(
                        display_str, int(100 * scores[i]))
            box_to_display_str_map[box].append(display_str)
            if agnostic_mode:
                box_to_color_map[box] = &#39;DarkOrange&#39;
            else:
                box_to_color_map[box] = STANDARD_COLORS[classes[i] %
                                                        len(STANDARD_COLORS)]

    # Draw all boxes onto image.
    for box, color in box_to_color_map.items():
        ymin, xmin, ymax, xmax = box
        if instance_masks is not None:
            draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color)

        draw_bounding_box_on_image_array(
        image,
        ymin,
        xmin,
        ymax,
        xmax,
        color=color,
        thickness=line_thickness,
        display_str_list=box_to_display_str_map[box],
        use_normalized_coordinates=use_normalized_coordinates)

    return image


def reframe_box_masks_to_image_masks(box_masks,  boxes,  image_height,
                                     image_width):
    &amp;quot;&amp;quot;&amp;quot;Transforms  the  box  masks  back  to  full  image  masks.

    Embeds  masks  in  bounding  boxes  of  larger  masks  whose  shapes  correspond  to
    image  shape.

    Args:
        box_masks:  A  tf.float32  tensor  of  size  [num_masks,  mask_height,  mask_width].
        boxes:  A  tf.float32  tensor  of  size  [num_masks,  4]  containing  the  box
                      corners.  Row  i  contains  [ymin,  xmin,  ymax,  xmax]  of  the  box
                      corresponding  to  mask  i.  Note  that  the  box  corners  are  in
                      normalized  coordinates.
        image_height:  Image  height.  The  output  mask  will  have  the  same  height  as
                                    the  image  height.
        image_width:  Image  width.  The  output  mask  will  have  the  same  width  as  the
                                  image  width.

    Returns:
        A  tf.float32  tensor  of  size  [num_masks,  image_height,  image_width].
    &amp;quot;&amp;quot;&amp;quot;
    #  TODO(rathodv):  Make  this  a  public  function.
    def reframe_box_masks_to_image_masks_default():
        &amp;quot;&amp;quot;&amp;quot;The  default  function  when  there  are  more  than  0  box  masks.&amp;quot;&amp;quot;&amp;quot;
        def transform_boxes_relative_to_boxes(boxes,  reference_boxes):
            boxes = tf.reshape(boxes,  [-1,  2,  2])
            min_corner = tf.expand_dims(reference_boxes[:,  0:2],  1)
            max_corner = tf.expand_dims(reference_boxes[:,  2:4],  1)
            transformed_boxes = (boxes - min_corner) / \
                (max_corner - min_corner)
            return tf.reshape(transformed_boxes,  [-1,  4])

        box_masks_expanded = tf.expand_dims(box_masks,  axis=3)
        num_boxes = tf.shape(box_masks_expanded)[0]
        unit_boxes = tf.concat(
            [tf.zeros([num_boxes,  2]),  tf.ones([num_boxes,  2])],  axis=1)
        reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes,  boxes)
        return tf.image.crop_and_resize(
            image=box_masks_expanded,
            boxes=reverse_boxes,
            box_ind=tf.range(num_boxes),
            crop_size=[image_height,  image_width],
            extrapolation_value=0.0)
    image_masks = tf.cond(
        tf.shape(box_masks)[0] &amp;gt; 0,
        reframe_box_masks_to_image_masks_default,
        lambda:  tf.zeros([0,  image_height,  image_width,  1],  dtype=tf.float32))
    return tf.squeeze(image_masks,  axis=3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cho hình ảnh vào và rút ra kết quả.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def detect_frame(image_np, sess, detection_graph):

    with detection_graph.as_default():

        ops = tf.get_default_graph().get_operations()
        all_tensor_names = {output.name for op in ops for output in op.outputs}
        tensor_dict = {}
        for key in [
            &#39;num_detections&#39;, &#39;detection_boxes&#39;, &#39;detection_scores&#39;,
            &#39;detection_classes&#39;, &#39;detection_masks&#39;
        ]:
            tensor_name = key + &#39;:0&#39;
            if tensor_name in all_tensor_names:
                tensor_dict[key] = tf.get_default_graph(
                ).get_tensor_by_name(tensor_name)
        if &#39;detection_masks&#39; in tensor_dict:
            # The following processing is only for single image
            detection_boxes = tf.squeeze(tensor_dict[&#39;detection_boxes&#39;], [0])
            detection_masks = tf.squeeze(tensor_dict[&#39;detection_masks&#39;], [0])
            # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
            real_num_detection = tf.cast(
                tensor_dict[&#39;num_detections&#39;][0], tf.int32)
           
            detection_boxes = tf.slice(detection_boxes, [0, 0], [
                                       real_num_detection, -1])
            detection_masks = tf.slice(detection_masks, [0, 0, 0], [
                                       real_num_detection, -1, -1])
            detection_masks_reframed = reframe_box_masks_to_image_masks(
                detection_masks, detection_boxes, image_np.shape[0], image_np.shape[1])
            detection_masks_reframed = tf.cast(
                tf.greater(detection_masks_reframed, 0.5), tf.uint8)
            # Follow the convention by adding back the batch dimension
            tensor_dict[&#39;detection_masks&#39;] = tf.expand_dims(
                detection_masks_reframed, 0)
        image_tensor = tf.get_default_graph().get_tensor_by_name(&#39;image_tensor:0&#39;)

      # Run inference
        output_dict = sess.run(tensor_dict,
                               feed_dict={image_tensor: np.expand_dims(image_np, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
        output_dict[&#39;num_detections&#39;] = int(output_dict[&#39;num_detections&#39;][0])
        #print(&amp;quot;num detect &amp;quot;+str(output_dict[&#39;num_detections&#39;]))
        output_dict[&#39;detection_classes&#39;] = output_dict[&#39;detection_classes&#39;][0].astype(
            np.uint8)
        output_dict[&#39;detection_boxes&#39;] = output_dict[&#39;detection_boxes&#39;][0]
        output_dict[&#39;detection_scores&#39;] = output_dict[&#39;detection_scores&#39;][0]
        if &#39;detection_masks&#39; in output_dict:
            output_dict[&#39;detection_masks&#39;] = output_dict[&#39;detection_masks&#39;][0]

        visualize_boxes_and_labels_on_image_array(
            image_np,
            output_dict[&#39;detection_boxes&#39;],
            output_dict[&#39;detection_classes&#39;],
            output_dict[&#39;detection_scores&#39;],
            category_index,
            instance_masks=output_dict.get(&#39;detection_masks&#39;),
            use_normalized_coordinates=True,
            line_thickness=1,
            max_boxes_to_draw=min(output_dict[&#39;num_detections&#39;],20)
            )

    return image_np
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image = cv2.imread(&#39;img2.jpg&#39;)
with detection_graph.as_default():
    with tf.Session(graph=detection_graph) as sess:
        image_np = detect_frame(image, sess, detection_graph)

cv2.imwrite(&#39;output.jpg&#39;, image)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả file output.jpg của chúng ta là:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mask_rnn_output_mieule.jpg&#34; alt=&#34;Phân vùng của mark ca sĩ midu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thử với bức ảnh người và xe hơi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/mask_rnn_output_nguoidep_xehoi.jpg&#34; alt=&#34;Phân vùng của người và xe hơi&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi. Hẹn gặp bạn ở các bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning based Multiple Human Pose Estimation using OpenCV</title>
      <link>/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/</link>
      <pubDate>Fri, 05 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Lưu ý: Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv &amp;gt; 3.4.1.&lt;/p&gt;

&lt;p&gt;Ở bài viết trước, chúng ta đã tìm hiểu cách thức rút trích khung xương sử dụng DNN và đã áp dụng thành công trên ảnh có chứa 1 đối tượng người. Trong bài viết này, chúng ta sẽ thực hiện áp dụng mô hình cho bài toán có nhiều người trong cùng 1 bức ảnh.&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-pretrain-model-trong-bài-toán-multiple-pose-estimation&#34;&gt;Sử dụng pretrain model trong bài toán multiple Pose Estimation&lt;/h2&gt;

&lt;p&gt;Trong bài viết này, chúng ta tiếp tục sử dụng mô hình MPI để dò tìm các điểm đặc trưng của con người và rút ra mô hình khung xương. Kết quả trả về của thuật toán gồm 15 đặc trưng như bên dưới.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
Left Ankle – 13, Chest – 14, Background – 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Áp dụng mô hình với ảnh của nhóm T-ARA.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2

nPoints = 15
POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]

protoFile = &amp;quot;pose/mpi/pose_deploy_linevec.prototxt&amp;quot;
weightsFile = &amp;quot;pose/mpi/pose_iter_160000.caffemodel&amp;quot;

net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

frame = cv2.imread(&amp;quot;tara1.jpg&amp;quot;)

inWidth = 368
inHeight = 368
 
# Prepare the frame to be fed to the network
inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
 
# Set the prepared object as the input blob of the network
net.setInput(inpBlob)

output = net.forward()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thử show lên vị trí vùng cổ trong hình.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
i = 0
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_head_heatmap.png&#34; alt=&#34;Hình với điểm đặc trưng vùng đầu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Thử show lên hình điểm đặc trưng vùng cổ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 1
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_neck_heatmap.png&#34; alt=&#34;Hình với điểm đặc trưng vùng cổ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bằng một số phép biến đổi quen thuộc có sẵn trong opencv, chúng ta hoàn toàn có thể lấy được toạ độ của các điểm keypoint một cách dễ dàng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Find the Keypoints using Non Maximum Suppression on the Confidence Map
def getKeypoints(probMap, threshold=0.1):
    
    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)

    mapMask = np.uint8(mapSmooth&amp;gt;threshold)
    keypoints = []
    
    #find the blobs
    _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    #for each blob find the maxima
    for cnt in contours:
        blobMask = np.zeros(mapMask.shape)
        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)
        maskedProbMap = mapSmooth * blobMask
        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)
        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))

    return keypoints


detected_keypoints = []
keypoints_list = np.zeros((0,3))
keypoint_id = 0
threshold = 0.1
for i in range(nPoints):
    probMap = output[0, i, :, :]
    probMap = cv2.resize(probMap, (frameWidth, frameHeight))

    keypoints = getKeypoints(probMap, threshold)
    keypoints_with_id = []
    for j in range(len(keypoints)):
        keypoints_with_id.append(keypoints[j] + (keypoint_id,))
        keypoints_list = np.vstack([keypoints_list, keypoints[j]])
        keypoint_id += 1

    detected_keypoints.append(keypoints_with_id)



frameClone = cv2.cvtColor(frameCopy,cv2.COLOR_BGR2RGB)
for i in range(nPoints):
    for j in range(len(detected_keypoints[i])):
        cv2.circle(frameClone, detected_keypoints[i][j][0:2], 3, [0,0,255], -1, cv2.LINE_AA)

plt.imshow(frameClone) 
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_keypoint.png&#34; alt=&#34;Hình toàn bộ điểm đặc trưng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cuối cùng, chúng ta sẽ nối các điểm đặc trưng của các nhân vật thông qua thuật toán Part Affinity Heatmaps. Thuật toán này được đề xuất bởi nhóm tác giả Zhe Cao, Tomas Simon,Shih-En Wei, Yaser Sheikh thuộc phòng thí nghiệm The Robotics Institute trường đại học Carnegie Mellon. Các bạn có nhu cầu có thể tìm hiểu ở &lt;a href=&#34;https://arxiv.org/pdf/1611.08050.pdf&#34;&gt;https://arxiv.org/pdf/1611.08050.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
mapIdx = [[16,17], [18,19], [20,21], [22,23], [24,25], [26,27], [28,29], [30,31], [32,33], [34,35], [36,37], [38,39], [40,41], [42,43]]



colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],
         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],
         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]
# Find valid connections between the different joints of a all persons present
def getValidPairs(output):
    valid_pairs = []
    invalid_pairs = []
    n_interp_samples = 10
    paf_score_th = 0.1
    conf_th = 0.5
    # loop for every POSE_PAIR
    for k in range(len(mapIdx)):
        # A-&amp;gt;B constitute a limb
        pafA = output[0, mapIdx[k][0], :, :]
        pafB = output[0, mapIdx[k][1], :, :]
        pafA = cv2.resize(pafA, (frameWidth, frameHeight))
        pafB = cv2.resize(pafB, (frameWidth, frameHeight))


        # Find the keypoints for the first and second limb
        candA = detected_keypoints[POSE_PAIRS[k][0]]
        candB = detected_keypoints[POSE_PAIRS[k][1]]
        nA = len(candA)
        nB = len(candB)

        # fig=plt.figure(figsize=(8, 8))

        # interp_coord = list(zip(np.linspace(candA[0][0], candB[0][0], num=n_interp_samples),
        #                                     np.linspace(candA[0][1], candB[0][1], num=n_interp_samples)))

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 1)
        
        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)


        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafA, alpha=0.5)

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 2)
        

        

        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)
        
        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafB, alpha=0.5)
        # plt.show()



        

        # If keypoints for the joint-pair is detected
        # check every joint in candA with every joint in candB 
        # Calculate the distance vector between the two joints
        # Find the PAF values at a set of interpolated points between the joints
        # Use the above formula to compute a score to mark the connection valid
        
        if( nA != 0 and nB != 0):
            valid_pair = np.zeros((0,3))
            for i in range(nA):
                max_j=-1
                maxScore = -1
                found = 0
                for j in range(nB):
                    # Find d_ij
                    d_ij = np.subtract(candB[j][:2], candA[i][:2])
                    norm = np.linalg.norm(d_ij)
                    if norm:
                        d_ij = d_ij / norm
                    else:
                        continue
                    # Find p(u)
                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),
                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))
                    # Find L(p(u))
                    paf_interp = []
                    for k in range(len(interp_coord)):
                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],
                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) 
                    # Find E
                    paf_scores = np.dot(paf_interp, d_ij)
                    avg_paf_score = sum(paf_scores)/len(paf_scores)
                    
                    # Check if the connection is valid
                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -&amp;gt; Valid Pair  
                    if ( len(np.where(paf_scores &amp;gt; paf_score_th)[0]) / n_interp_samples ) &amp;gt; conf_th :
                        if avg_paf_score &amp;gt; maxScore:
                            max_j = j
                            maxScore = avg_paf_score
                            found = 1
                # Append the connection to the list
                if found:            
                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)

            # Append the detected connections to the global list
            valid_pairs.append(valid_pair)

            pprint(valid_pair)
        else: # If no keypoints are detected
            print(&amp;quot;No Connection : k = {}&amp;quot;.format(k))
            invalid_pairs.append(k)
            valid_pairs.append([])
    pprint(valid_pairs)
    return valid_pairs, invalid_pairs

# This function creates a list of keypoints belonging to each person
# For each detected valid pair, it assigns the joint(s) to a person
# It finds the person and index at which the joint should be added. This can be done since we have an id for each joint
def getPersonwiseKeypoints(valid_pairs, invalid_pairs):
    # the last number in each row is the overall score 
    personwiseKeypoints = -1 * np.ones((0, 19))

    for k in range(len(mapIdx)):
        if k not in invalid_pairs:
            partAs = valid_pairs[k][:,0]
            partBs = valid_pairs[k][:,1]
            indexA, indexB = np.array(POSE_PAIRS[k])

            for i in range(len(valid_pairs[k])): 
                found = 0
                person_idx = -1
                for j in range(len(personwiseKeypoints)):
                    if personwiseKeypoints[j][indexA] == partAs[i]:
                        person_idx = j
                        found = 1
                        break

                if found:
                    personwiseKeypoints[person_idx][indexB] = partBs[i]
                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]

                # if find no partA in the subset, create a new subset
                elif not found and k &amp;lt; 17:
                    row = -1 * np.ones(19)
                    row[indexA] = partAs[i]
                    row[indexB] = partBs[i]
                    # add the keypoint_scores for the two keypoints and the paf_score 
                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]
                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])
    return personwiseKeypoints

valid_pairs, invalid_pairs = getValidPairs(output)

personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)


for i in range(nPoints-1):
    for n in range(len(personwiseKeypoints)):
       
        index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]
        if -1 in index:
            continue
        B = np.int32(keypoints_list[index.astype(int), 0])
        A = np.int32(keypoints_list[index.astype(int), 1])
        cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)
       
        
        
plt.imshow(frameClone)
    # plt.imshow(mapMask, alpha=0.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/multiple_pose_estimation_t-ara_finalresult.png&#34; alt=&#34;Hình kết quả cuối cùng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hẹn gặp lại các bạn ở những bài viết tiếp theo.&lt;/p&gt;

&lt;p&gt;Bài viết này được viết dựa vào nguồn &lt;a href=&#34;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&#34;&gt;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&lt;/a&gt; của tác giả VIKAS GUPTA. Tôi sử dụng tập model và hình ảnh khác với bài viết nguyên gốc của tác giả.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning based Human Pose Estimation using OpenCV</title>
      <link>/blog/2018-10-04-deep-learning-base-human-pose-estimation/</link>
      <pubDate>Thu, 04 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-04-deep-learning-base-human-pose-estimation/</guid>
      <description>

&lt;h3 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h3&gt;

&lt;p&gt;Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv &amp;gt; 3.4.1.&lt;/p&gt;

&lt;h2 id=&#34;pose-estimation-là-gì&#34;&gt;Pose Estimation là gì?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=ohX-wkLYhdM&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/ohX-wkLYhdM/0.jpg&#34; alt=&#34;POST ESTIMATION EXAMPLE - Make by Phạm Duy Tùng&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Post Estimation ( đôi khi được dùng với thuật ngữ Keypoint Detection) là một vấn đề khá phổ biến trong lĩnh vực xử lý ảnh khi chúng ta cần xác định vị trí và hướng của một đối tượng. Mức ý nghĩa ở đây là chúng ta phải rút ra được những đặc điểm chính, những đặc điểm đó là những đặc trưng của đối tượng ( có thể mô tả được đối tượng).&lt;/p&gt;

&lt;p&gt;Ví dụ, trong bài toán face pose estimation ( có tên khác là facial landmark detection), chúng ta cần xác định được đâu là vị trí của những điểm landmark trên khuôn mặt người.&lt;/p&gt;

&lt;p&gt;Một bài toán có liên quan đến bài toán trên là head pose estimation. Chúng ta cần xác định những điểm landmark để mô hình hoá lại được mô hình 3D của đầu người.&lt;/p&gt;

&lt;p&gt;Ở trong bài viết này, chúng ta đề cập đến bài toán human pose estimation, công việc chính là xác định và chỉ ra được một phần/ toàn bộ các phần chính của cơ thể con người (vd vai, khuỷu tay, cổ tay, đầu gối v.v).&lt;/p&gt;

&lt;p&gt;Trong bài viết này, chúng ta sẽ sử dụng mô hình được huấn luyện sẵn để chỉ ra các phần chính của cơ thể con người. Kết quả cơ bản của phần nhận diện này sẽ gần giống như hình bên dưới.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/midu_pose_estimation.png&#34; alt=&#34;Hình ảnh rút trích những thành phần quan trọng trên cơ thể con người&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sử-dụng-pretrain-model-trong-bài-toán-pose-estimation&#34;&gt;Sử dụng pretrain model trong bài toán Pose Estimation&lt;/h2&gt;

&lt;p&gt;Vào nằm 2016, 2017, Phòng thí nghiệm Perceptual Computing của trường đại học Carnegie Mellon University đã công bố một bài báo có liên quan đến chủ đề Multi-Person Pose Estimation. Và đến nay, họ đã công bố mô hình huấn luyện cho chúng ta sử dụng. Các bạn có nhu cầu tìm hiểu sâu hơn có thể đọc kỹ nguồn dữ liệu của họ công bố ở link &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34;&gt;https://github.com/CMU-Perceptual-Computing-Lab/openpose&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trong bài post này, mình sẽ không đề cập kỹ đến phần kiến trúc mạng neural net họ sử dụng bên dưới, thay vào đó, mình sẽ tập trung hơn vào cách thức sử dụng mô hình để thu được kết quả cần thiết.&lt;/p&gt;

&lt;p&gt;Trước khi bắt đầu vào thực hành, mình sẽ mô tả một chút về mô hình pretrain có sẵn. Ở đây, họ cung cấp cho chúng ta 2 mô hình là MPII model và COCO  model. Đó chính là tên của hai bộ database mà họ sử dụng để đào tạo mô hình. Kết quả trả về của mỗ bộ database là khác nhau hoàn toàn.&lt;/p&gt;

&lt;p&gt;Với bộ COCO dataset, kết quả trả về là 18 đặc trưng gồm các thông tin:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Nose – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
LAnkle – 13, Right Eye – 14, Left Eye – 15, Right Ear – 16,
Left Ear – 17, Background – 18
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Với bộ MPII, kết quả trả về là 15 đặc trưng gồm các thông tin:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
Left Ankle – 13, Chest – 14, Background – 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong phần này, chúng ta sẽ tập trung vào mô hình MPII, mô hình COCO sử dụng tương tự, chỉ việc thay lại đường dẫn file mô hình là được.&lt;/p&gt;

&lt;h2 id=&#34;bắt-đầu-code&#34;&gt;Bắt đầu code.&lt;/h2&gt;

&lt;p&gt;Bước 1: Download mô hình.&lt;/p&gt;

&lt;p&gt;Nhóm tác giả sử dụng caffe để huấn luyện mô hình, do đó, để sử dụng được, chúng ta cần download file mô hình ở đường dẫn &lt;a href=&#34;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel&#34;&gt;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel&lt;/a&gt; và file cấu hình ở đường dẫn &lt;a href=&#34;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_deploy_linevec.prototxt&#34;&gt;http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_deploy_linevec.prototxt&lt;/a&gt;. Các bạn có thể để đâu đó tuỳ thích, ở đây tôi để trong thư mục pose/mpi để dễ dàng nhận biết với các mô hình khác.&lt;/p&gt;

&lt;p&gt;Bước 2: Load mô hình.&lt;/p&gt;

&lt;p&gt;Để load mô hình lên bộ nhớ chính, đơn giản là chúng ta thực hiện câu lệnh sau trong python&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
# Specify the paths for the 2 files
protoFile = &amp;quot;pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt&amp;quot;
weightsFile = &amp;quot;pose/mpi/pose_iter_160000.caffemodel&amp;quot;
 
# Read the network into Memory
net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Đơn giản quá phải không các bạn :).&lt;/p&gt;

&lt;p&gt;Bước 3: Đọc ảnh và đưa ảnh vào trong mô hình.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Read image
frame = cv2.imread(&amp;quot;img2.jpg&amp;quot;)

frameCopy = np.copy(frame)
frameWidth = frame.shape[1]
frameHeight = frame.shape[0]
t = time.time()
# Specify the input image dimensions
inWidth = 368
inHeight = 368
 
# Prepare the frame to be fed to the network
inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
 
# Set the prepared object as the input blob of the network
net.setInput(inpBlob)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chắc không cần phải nói gì thêm, phần comment chú thích đã mô tả khá đầy đủ chức năng của từng phần trong này rồi.&lt;/p&gt;

&lt;p&gt;Bước 4: Thu thập kết quả và trích xuất điểm đặc trưng&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
frameCopy = frame.copy()

output = net.forward()
print(&amp;quot;time taken by network : {:.3f}&amp;quot;.format(time.time() - t))
H = output.shape[2]
W = output.shape[3]

nPoints = 15
POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]


threshold = 0.01
# Empty list to store the detected keypoints
points = []
for i in range(nPoints):
    # confidence map of corresponding body&#39;s part.
    probMap = output[0, i, :, :]
 
    # Find global maxima of the probMap.
    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)
     
    # Scale the point to fit on the original image
    x = (frameWidth * point[0]) / W
    y = (frameHeight * point[1]) / H

    print(prob)
 
    if prob &amp;gt; threshold : 
        cv2.circle(frame, (int(x), int(y)), 15, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)
        cv2.putText(frame, &amp;quot;{}&amp;quot;.format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 2, lineType=cv2.LINE_AA)
 
        # Add the point to the list if the probability is greater than the threshold
        points.append((int(x), int(y)))
    else :
        points.append(None)
 
# cv2.imshow(&amp;quot;Output-Keypoints&amp;quot;,frame)
# cv2.waitKey(0)
# cv2.destroyAllWindows()

cv2.imwrite(&amp;quot;dot_keypoint.png&amp;quot;,frame)

# Draw Skeleton
for pair in POSE_PAIRS:
    partA = pair[0]
    partB = pair[1]

    if points[partA] and points[partB]:
        cv2.line(frameCopy, points[partA], points[partB], (0, 255, 255), 2)
        cv2.circle(frameCopy, points[partA], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)
        cv2.circle(frameCopy, points[partB], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)


cv2.imwrite(&amp;quot;line_keypoint.png&amp;quot;,frameCopy)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả của giá trị output là một ma trận 4D, với ý nghĩa của mỗi chiều như sau:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Chiều đầu tiên là image ID (định danh ảnh trong trường hợp bạn truyền nhiều ảnh vào mạng)&lt;/li&gt;
&lt;li&gt;Chiều thứ 2 là chỉ số của các điểm đặc trưng. Tập MPI trả về tập gồm 44 điểm dữ liệu, ta chỉ sử dụng một vài điểm dữ liệu tương ứng với vị trí các điểm đặc trưng mà chúng ta quan tâm.&lt;/li&gt;
&lt;li&gt;Chiều thứ 3 là height của output map.&lt;/li&gt;
&lt;li&gt;Chiều thứ 4 là width của output map.
Một lưu ý ở đây là tôi có sử dụng đặt giá trị chặn dưới threshold để giảm thiểu sự sai sót do nhận diện sai. Và kết quả đạt được là hai hình bên dưới:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/midu_pose_estimation_keypoint.png&#34; alt=&#34;Hình nhữn điểm đặc trưng&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/midu_pose_estimation.png&#34; alt=&#34;Hình khung xương&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hẹn gặp lại các bạn ở những bài viết tiếp theo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Phân biệt Epoch - Batch size và Iterations</title>
      <link>/blog/2018-10-02-understanding-epoch-batchsize-iterations/</link>
      <pubDate>Tue, 02 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-02-understanding-epoch-batchsize-iterations/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Khi mới bắt đầu bước vào thế giới của ML/DL chúng ta sẽ bắt gặp các thuật ngữ Epoch - Batch size và Iterations. Và sẽ cảm thấy bối rối vì chúng khá giống nhau, nhưng thực tế là chúng khác xa nhau.&lt;/p&gt;

&lt;p&gt;Để hiểu rõ sự khác biệt giữa chúng, các bạn cần tìm hiểu một số khái niệm trong machine learning như Gradient Descent.&lt;/p&gt;

&lt;p&gt;Định nghĩa ngắn gọn của Gradient Descent:&lt;/p&gt;

&lt;p&gt;Gradient Descent là thuật toán lặp tối ưu (iteractive optimization algorithm) được sử dụng trong machine learning để tìm kết quả tốt nhất (minima of a curve).&lt;/p&gt;

&lt;p&gt;Trong đó:
..* Gradient có nghĩa là tỷ lệ của độ nghiêm của đường dốc.&lt;/p&gt;

&lt;p&gt;..* Descent là từ viết tắt của decending - nghĩa là giảm.&lt;/p&gt;

&lt;p&gt;Thuật toán sẽ lặp đi lặp lại nhiều lần để tìm ra được kết quả tối ưu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/gradient.gif&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a&#34;&gt;https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a&lt;/a&gt; Nguồn ảnh&lt;/p&gt;

&lt;p&gt;Thuật toán gradient Descent có một tham số là learning rate. Như hình phía trên bên trái, ban đầu bước nhảy khá lớn, nghĩa là giá trị learning rate lớn, và sau một vài lần lặp, điểm chấm đen đi xuống dần, và giá trị learning rate nhỏ dần theo.&lt;/p&gt;

&lt;p&gt;Chúng ta sử dụng thuật ngữ epochs, batch size, iterations khi dữ liệu của chúng ta quá (rất) lớn (vd 10 triệu mẫu). Lúc này các khái niệm trên mới trở nên rõ ràng, còn với trường hợp dữ liệu nhỏ thì chúng khá tương tự nhau.&lt;/p&gt;

&lt;h2 id=&#34;khái-niện-epoch&#34;&gt;Khái niện Epoch&lt;/h2&gt;

&lt;p&gt;Một Epoch được tính là khi chúng ta đưa tất cả dữ liệu vào mạng neural network 1 lần.&lt;/p&gt;

&lt;p&gt;Khi dữ liệu quá lớn, chúng ta không thể đưa hết mỗi lần tất cả tập dữ liệu vào để huấn luyện được. Buộc lòng chúng ta phải chia nhỏ  tập dữ liệu ra thành các batch (size nhỏ hơn).&lt;/p&gt;

&lt;h3 id=&#34;tại-sao-phải-dùng-hơn-1-epoch&#34;&gt;Tại sao phải dùng hơn 1 Epoch.&lt;/h3&gt;

&lt;p&gt;Câu trả lời ở đây là tại vì chúng ta đang dùng thuật toán tối ưu là Gradient Descent. Thuật toán này đòi hỏi chúng ta phải đem toàn bộ dữ liệu qua mạng một vài lần để tìm được kết quả tối ưu. Vì vậy, dùng 1 epoch thật sự không đủ để tìm được kết quả tốt nhất.&lt;/p&gt;

&lt;p&gt;Với việc chỉ sử dụng 1 lần lặp, xác suất rất cao là dữ liệu sẽ bị underfitting(như hình mô tả bên dưới).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/overfit_underfit.png&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Khi số lần lặp tăng dần, trạng thái của mô hình sẽ chuyển dần từ underfitting sang optimal và sau đó là overfitting (thông thường là vậy, trừ khi mô hình huấn luyện của bạn đang sử dụng quá đơn giản, quá ít trọng số thì chúng không thể nào overfitting nổi).&lt;/p&gt;

&lt;p&gt;Chúng ta có thể dùng 1 epoch để huấn luyện mô hình, với điều kiện là ta sử dụng thuật toán tối ưu không phải là gradient descent.&lt;/p&gt;

&lt;h3 id=&#34;số-lần-lặp-tối-ưu-là-bao-nhiêu&#34;&gt;Số lần lặp tối ưu là bao nhiêu?&lt;/h3&gt;

&lt;p&gt;Tiếc rằng không có câu trả lời cho câu hỏi này. Phụ thuộc hoàn toàn vào tập dữ liệu của bạn đang có.&lt;/p&gt;

&lt;h2 id=&#34;batch-size&#34;&gt;Batch Size&lt;/h2&gt;

&lt;p&gt;Batch size là số lượng mẫu dữ liệu trong một batch.&lt;/p&gt;

&lt;p&gt;Ở đây, khái niệm batch size và số lượng batch(number of batch) là hoàn toàn khác nhau.&lt;/p&gt;

&lt;p&gt;Như đã nói ở trên, chúng ta không thể đưa hết toàn bộ dữ liệu vào huấn luyện trong 1 epoch, vì vậy chúng ta cần phải chia tập dữ liệu thành các phần (number of batch), mỗi phần có kích thước là batch size.&lt;/p&gt;

&lt;h2 id=&#34;iterations&#34;&gt;Iterations&lt;/h2&gt;

&lt;p&gt;Iterations là số lượng batchs cần để hoàn thành 1 epoch.&lt;/p&gt;

&lt;p&gt;Ví dụ chúng ta có tập dữ liệu có 20,000 mẫu, batch size là 500, vậy chúng ta cần 40 lần lặp (iteration) để hoàn thành 1 epoch.&lt;/p&gt;

&lt;p&gt;Cảm ơn các bạn đã theo dõi bài viết.&lt;/p&gt;

&lt;p&gt;Nguồn: &lt;a href=&#34;https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9&#34;&gt;https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Xây dựng chương trình gợi ý phim dựa vào tập dữ liệu movie len</title>
      <link>/blog/2018-10-01-buiding-a-movie-model/</link>
      <pubDate>Mon, 01 Oct 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-10-01-buiding-a-movie-model/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;MovieLens là một tập dữ liệu được sử dụng rộng rãi cách đây nhiều năm. Hôm nay, mình sẽ sử dụng tập dữ liệu này và mô hình ALS của spark để xây dựng chương trình dự đoán phim cho người dùng.&lt;/p&gt;

&lt;h2 id=&#34;chuẩn-bị-dữ-liệu&#34;&gt;Chuẩn bị dữ liệu&lt;/h2&gt;

&lt;p&gt;Các bạn có thể download tập dữ liệu MovieLens ở link &lt;a href=&#34;https://grouplens.org/datasets/movielens/&#34;&gt;https://grouplens.org/datasets/movielens/&lt;/a&gt;. Các bạn có thể download trực tiếp 2 file nén ở link &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#34;&gt;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&lt;/a&gt; và link  &lt;a href=&#34;http://files.grouplens.org/datasets/movielens/ml-latest.zip&#34;&gt;http://files.grouplens.org/datasets/movielens/ml-latest.zip&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Ở trên bao gồm 2 tập dữ liệu. chúng ta tạo thư mục datasets và download rồi bỏ chúng vào trong thư mục đấy.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;complete_dataset_url = &#39;http://files.grouplens.org/datasets/movielens/ml-latest.zip&#39;
small_dataset_url = &#39;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#39;

import os

datasets_path = &#39;datasets&#39;
if not os.path.exists(datasets_path):
    os.makedirs(datasets_path))

complete_dataset_path = os.path.join(datasets_path, &#39;ml-latest.zip&#39;)
small_dataset_path = os.path.join(datasets_path, &#39;ml-latest-small.zip&#39;)

import urllib
import zipfile

if not os.path.exists(small_dataset_url):
    small_f = urllib.urlretrieve (small_dataset_url, small_dataset_path)#Download
    with zipfile.ZipFile(small_dataset_path, &amp;quot;r&amp;quot;) as z:#Giải nén
        z.extractall(datasets_path)
if not os.path.exists(small_dataset_url):
    complete_f = urllib.urlretrieve (complete_dataset_url, complete_dataset_path)#Download
    with zipfile.ZipFile(complete_dataset_path, &amp;quot;r&amp;quot;) as z:#Giải nén
        z.extractall(datasets_path)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trong thư mục giải nén, chúng ta sẽ có các file ratings.csv, movies.csv, tags.csv, links.csv, README.txt.&lt;/p&gt;

&lt;h2 id=&#34;loading-và-parsing-dữ-liệu&#34;&gt;Loading và parsing dữ liệu.&lt;/h2&gt;

&lt;p&gt;Mỗi dòng trong tập ratings.csv có định dạng &lt;code&gt;&amp;quot;userId,movieId,rating,timestamp&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mỗi dòng trong tập movies.csv có định dạng &lt;code&gt;&amp;quot;movieId,title,genres&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mỗi dòng trong tập tags.csv có định dạng &lt;code&gt;&amp;quot;userId,movieId,tag,timestamp&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mỗi dòng trong tập links.csv có định dạng &lt;code&gt;&amp;quot;movieId,imdbId,tmdbId&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Tóm lại, các trường dữ liệu trong các file csv đều ngăn cách nhau bởi dấu phẩy (,). Trong python, ta có thể dùng hàm split để cắt chúng ra. Sau đó sẽ load toàn bộ dữ liệu lên RDDs.&lt;/p&gt;

&lt;p&gt;Lưu ý nhỏ:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ở tập dữ liệu ratings, chúng ta chỉ giữ lại các trường &lt;code&gt;(UserID, MovieID, Rating)&lt;/code&gt; bỏ đi trường timestamp vì không cần thiết.&lt;/li&gt;
&lt;li&gt;Ở tập dữ liệu movies  chúng ta giữ lại trường &lt;code&gt;(MovieID, Title)&lt;/code&gt; và bỏ đi trường genres vì lý do tương tự.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;small_ratings_file = os.path.join(datasets_path, &#39;ml-latest-small&#39;, &#39;ratings.csv&#39;)
small_ratings_raw_data = sc.textFile(small_ratings_file)
small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]
small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header).map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()
print(small_ratings_data.take(3)) #Hiện thị top 3 ratting đầu tiên

small_movies_file = os.path.join(datasets_path, &#39;ml-latest-small&#39;, &#39;movies.csv&#39;)

small_movies_raw_data = sc.textFile(small_movies_file)
small_movies_raw_data_header = small_movies_raw_data.take(1)[0]

small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\
    .map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (tokens[0],tokens[1])).cache()
    
small_movies_data.take(3) #Hiện thị top 3 movie đầu tiên
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phần tiếp theo, chúng ta sẽ tìm hiểu lọc cộng tác (Collaborative Filtering) và cách sử dụng Spark MLlib để xây dựng mô hình dự báo.&lt;/p&gt;

&lt;h2 id=&#34;collaborative-filtering&#34;&gt;Collaborative Filtering&lt;/h2&gt;

&lt;p&gt;Ở đây, tôi sẽ không đề cập đến lọc cộng tác là gì, các bạn có nhu cầu tìm hiểu có thể xem ở bài post khác hoặc tham khảo trên wiki. Chúng ta sẽ tập trung vào tìm hiểu cách sử dụng ALS trong thư viện MLlib của Spark. Các tham số của thuật toán này bao gồm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;numBlocks: số lượng block được sử dụng trong tính toán song song (-1 với ý nghĩa là auto configure).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rank: số lượng nhân tố ẩn (latent factor) trong mô hình.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;iterations: số lần lặp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lambda: tham số của chuẩn hoá(regularization ) trong ALS.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;chọn-các-tham-số-cho-als&#34;&gt;Chọn các tham số cho ALS&lt;/h2&gt;

&lt;p&gt;Để chọn được các tham số tốt nhất cho mô hình ALS, chúng ta sẽ sử dụng tập small để grid search. Đầu tiên, chúng ta chia tập dữ liệu thành 3 phần là tập train, tập vali và  tập test. Sau đó tiến hành huấn luyện trên tập train và predict trên tập valid để tìm được tham số tốt nhất. Cuối cùng đánh giá kết quả đạt được trên tập test.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)
validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))
test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))

from pyspark.mllib.recommendation import ALS
import math

seed = 5L
iterations = 10
regularization_parameter = 0.1
ranks = [4, 8, 12]
errors = [0, 0, 0]
err = 0
tolerance = 0.02

min_error = float(&#39;inf&#39;)
best_rank = -1
best_iteration = -1
for rank in ranks:
    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,
                      lambda_=regularization_parameter)
    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))
    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)
    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())
    errors[err] = error
    err += 1
    print(&#39;For rank %s the RMSE is %s&#39; % (rank, error))
    if error &amp;lt; min_error:
        min_error = error
        best_rank = rank

print(&#39;The best model was trained with rank %s&#39; % best_rank)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kết quả sau khi thực hiện đoạn code trên là:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;For rank 4 the RMSE is 0.963681878574
For rank 8 the RMSE is 0.96250475933
For rank 12 the RMSE is 0.971647563632
The best model was trained with rank 8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiến hành thực hiện test.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_test = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,
                      lambda_=regularization_parameter)
predictions = model_test.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))
rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)
error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())
    
print(&#39;For testing data the RMSE is %s&#39; % (error))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;For testing data the RMSE is 0.972342381898
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Xem kỹ hơn một chút về dữ liệu mà spark trả về cho chúng ta. Với predictions và rates_and_preds, ta có:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(predictions.take(3))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[((32, 4018), 3.280114696166238),
 ((375, 4018), 2.7365714977314086),
 ((674, 4018), 2.510684514310653)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tập dữ liệu trả về bao gồm cặp &lt;code&gt;(UserID, MovieID)&lt;/code&gt; và &lt;code&gt;Rating&lt;/code&gt; (tương ứng với colum 0, column 1 và column 2 ở trên),được hiểu ở đây là với người dùng UserID và phim MovieID thì mô hình sẽ dự đoán người dùng sẽ rating kết quả Rating.&lt;/p&gt;

&lt;p&gt;Sau đó chúng ta sẽ nối(join) chúng với tập valid tương ứng theo cặp &lt;code&gt;(UserID, MovieID)&lt;/code&gt;, kết quả đạt được là:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rates_and_preds.take(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[((558, 788), (3.0, 3.0419325487471403)),
 ((176, 3550), (4.5, 3.3214065001580986)),
 ((302, 3908), (1.0, 2.4728711204440765))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Việc còn lại là chúng ta sẽ tính trung bình độ lỗi bằng hàm &lt;code&gt;mean()&lt;/code&gt; và &lt;code&gt;sqlt()&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;xây-dựng-mô-hình-với-tập-dữ-liệu-large&#34;&gt;Xây dựng mô hình với tập dữ liệu large&lt;/h2&gt;

&lt;p&gt;Tiếp theo, chúng ta sẽ sử dụng tập dự liệu bự hơn để xây dựng mô hình. Cách thực hiện y chang như tập dữ liệu nhỏ đã được trình bày ở trên, nên tôi sẽ bỏ qua một số giải thích không cần thiết để tránh lặp lại.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the complete dataset file
complete_ratings_file = os.path.join(datasets_path, &#39;ml-latest&#39;, &#39;ratings.csv&#39;)
complete_ratings_raw_data = sc.textFile(complete_ratings_file)
complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]

# Parse
complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\
    .map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()
    
print(&amp;quot;There are %s recommendations in the complete dataset&amp;quot; % (complete_ratings_data.count()))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;There are 21063128 recommendations in the complete dataset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiến hành train và test.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)

complete_model = ALS.train(training_RDD, best_rank, seed=seed,iterations=iterations, lambda_=regularization_parameter)

test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))

predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))
rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)
error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())
    
print(&#39;For testing data the RMSE is %s&#39; % (error))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;For testing data the RMSE is 0.82183583368
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;xây-dựng-mô-hình-dự-đoán-phim&#34;&gt;Xây dựng mô hình dự đoán phim&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;complete_movies_file = os.path.join(datasets_path, &#39;ml-latest&#39;, &#39;movies.csv&#39;)
complete_movies_raw_data = sc.textFile(complete_movies_file)
complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]

# Parse
complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\
    .map(lambda line: line.split(&amp;quot;,&amp;quot;)).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()

complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))
    
print(&amp;quot;There are %s movies in the complete dataset&amp;quot; % (complete_movies_titles.count()))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;There are 27303 movies in the complete dataset
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_counts_and_averages(ID_and_ratings_tuple):
    nratings = len(ID_and_ratings_tuple[1])
    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)

movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())
movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)
movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Giả sử chúng ta có 1 người dùng mới, với các ratting như sau:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_user_ID = 0

# The format of each line is (userID, movieID, rating)
new_user_ratings = [
     (0,260,4), # Star Wars (1977)
     (0,1,3), # Toy Story (1995)
     (0,16,3), # Casino (1995)
     (0,25,4), # Leaving Las Vegas (1995)
     (0,32,4), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)
     (0,335,1), # Flintstones, The (1994)
     (0,379,1), # Timecop (1994)
     (0,296,3), # Pulp Fiction (1994)
     (0,858,5) , # Godfather, The (1972)
     (0,50,4) # Usual Suspects, The (1995)
    ]
new_user_ratings_RDD = sc.parallelize(new_user_ratings)
print(&#39;New user ratings: %s&#39; % new_user_ratings_RDD.take(10))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Chúng ta tiến hành huấn luyện lại mô hình khi có thêm người mới:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)

from time import time

t0 = time()
new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, 
                              iterations=iterations, lambda_=regularization_parameter)
tt = time() - t0

print(&amp;quot;New model trained in %s seconds&amp;quot; % round(tt,3))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;New model trained in 56.61 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiến hành dự đoán ratting của người dùng mới cho toàn bộ các phim người dùng đó chưa xem.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs
# keep just those not on the ID list (thanks Lei Li for spotting the error!)
new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))

# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies
new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Và show ra top 3 kết quả :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)
new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))
new_user_recommendations_rating_title_and_count_RDD = \
    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)
new_user_recommendations_rating_title_and_count_RDD.take(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hiển thị top recommend (Ở đây sẽ flat dữ liệu hiển thị thành dàng &lt;code&gt;((Title, Rating, Ratings Count))&lt;/code&gt; ra cho dễ nhìn).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))

top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]&amp;gt;=25).takeOrdered(25, key=lambda x: -x[1])

print (&#39;TOP recommended movies (with more than 25 reviews):\n%s&#39; %
        &#39;\n&#39;.join(map(str, top_movies)))

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TOP recommended movies (with more than 25 reviews):
    (u&#39;&amp;quot;Godfather: Part II&#39;, 8.503749129186701, 29198)
    (u&#39;&amp;quot;Civil War&#39;, 8.386497469089297, 257)
    (u&#39;Frozen Planet (2011)&#39;, 8.372705479107108, 31)
    (u&#39;&amp;quot;Shawshank Redemption&#39;, 8.258510064442426, 67741)
    (u&#39;Cosmos (1980)&#39;, 8.252254825768972, 948)
    (u&#39;Band of Brothers (2001)&#39;, 8.225114960311624, 4450)
    (u&#39;Generation Kill (2008)&#39;, 8.206487040524653, 52)
    (u&amp;quot;Schindler&#39;s List (1993)&amp;quot;, 8.172761674773625, 53609)
    (u&#39;Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)&#39;, 8.166229786764168, 23915)
    (u&amp;quot;One Flew Over the Cuckoo&#39;s Nest (1975)&amp;quot;, 8.15617022970577, 32948)
    (u&#39;Casablanca (1942)&#39;, 8.141303207981174, 26114)
    (u&#39;Seven Samurai (Shichinin no samurai) (1954)&#39;, 8.139633165142612, 11796)
    (u&#39;Goodfellas (1990)&#39;, 8.12931139039048, 27123)
    (u&#39;Star Wars: Episode V - The Empire Strikes Back (1980)&#39;, 8.124225700242096, 47710)
    (u&#39;Jazz (2001)&#39;, 8.078538221315313, 25)
    (u&amp;quot;Long Night&#39;s Journey Into Day (2000)&amp;quot;, 8.050176820606127, 34)
    (u&#39;Lawrence of Arabia (1962)&#39;, 8.041331489948814, 13452)
    (u&#39;Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)&#39;, 8.0399424815528, 45908)
    (u&#39;12 Angry Men (1957)&#39;, 8.011389274280754, 13235)
    (u&amp;quot;It&#39;s Such a Beautiful Day (2012)&amp;quot;, 8.007734839026181, 35)
    (u&#39;Apocalypse Now (1979)&#39;, 8.005094327199552, 23905)
    (u&#39;Paths of Glory (1957)&#39;, 7.999379786394267, 3598)
    (u&#39;Rear Window (1954)&#39;, 7.9860865203540214, 17996)
    (u&#39;State of Play (2003)&#39;, 7.981582126801772, 27)
    (u&#39;Chinatown (1974)&#39;, 7.978673289692703, 16195)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dự-đoán-rating-của-1-cá-nhân&#34;&gt;Dự đoán rating của 1 cá nhân&lt;/h2&gt;

&lt;p&gt;Một trường hợp khác là chúng ta cần dự đoán giá trị ratting của 1 người dùng với 1 bộ phim cụ thể nào đó.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)
individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)
individual_movie_rating_RDD.take(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[Rating(user=0, product=122880, rating=4.955831875971526)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;lưu-trữ-mô-hình&#34;&gt;Lưu trữ mô hình&lt;/h2&gt;

&lt;p&gt;Sau khi có được mô hình. Chúng ta cần phải lưu trữ chúng lại để sau này dùng.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.mllib.recommendation import MatrixFactorizationModel

model_path = os.path.join(&#39;models&#39;, &#39;movie_lens_als&#39;)

# Save and load model
model.save(sc, model_path)
same_model = MatrixFactorizationModel.load(sc, model_path)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tìm hiểu về mạng neural network AlexNet</title>
      <link>/blog/2018-06-15-understanding-alexnet/</link>
      <pubDate>Fri, 15 Jun 2018 00:19:00 +0300</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/blog/2018-06-15-understanding-alexnet/</guid>
      <description>

&lt;h2 id=&#34;lời-mở-đầu&#34;&gt;Lời mở đầu&lt;/h2&gt;

&lt;p&gt;Tỷ phú Peter Thiel đã từng đưa ra câu hỏi tréo ngoe như thế này: &amp;ldquo;What important truth do very few people agree with you on?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Nếu bạn đem câu này hỏi giáo sư Geoffrey Hinton vào năm 2010, ông ấy sẽ trả lời rằng mạng Convolutional Neural Networks (CNN) sẽ có bước đột phá lớn và giúp chúng ta giải quyết hoàn toàn bài toán phân loại ảnh. Tại thời điểm năm 2010, các nhà nghiên cứu trong lĩnh vực phân loại ảnh đều không nghĩ như giáo sư Geoffrey Hinton. Và Deep Learning tại thời điểm đó chưa thật sự giải quyết được bài toán này.&lt;/p&gt;

&lt;p&gt;Năm 2010 cũng là năm ra đời của cuộc thi ImageNet Large Scale Visual Recognition Challenge. Tập dữ liệu ảnh trong cuộc thi bao gồm khoảng 1.2 triệu ảnh thuộc 1000 lớp khác nhau, người thắng cuộc là người tạo ra mô hình làm cho độ lỗi trên tập dữ liệu trên là nhỏ nhất.&lt;/p&gt;

&lt;p&gt;Hai năm sau, trong bài báo &amp;ldquo;ImageNet Classification with Deep Convolutional Neural Networks&amp;rdquo; của nhóm tác giả Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, Geoffrey và các cộng sự của mình đã chứng minh điều ông ấy nói hai năm trước là hoàn toàn chính xác.
Ở bài báo này, nhóm tác giả đã huấn luyện mạng CNN và và đạt độ lỗi top-5 error rate là 15.3% (nhóm tác giả đã giành hạng nhất), cách biệt khá xa so với kết quả của nhóm đứng thứ hai(độ lỗi 26.2%). Trong các năm tiếp theo, rất nhiều nhóm đã nghiên cứu, cải tiến kiến trúc của mô hình CNN để đạt được kết quả tốt hơn, thậm chí hơn luôn khả năng nhận biết của con người.&lt;/p&gt;

&lt;p&gt;Kiến trúc mạng CNN được sử dụng vào năm 2012 được cộng đồng nghiên cứu gọi với tên gọi thân thương là AlexNet do tác giả chính của nhóm nghiên cứu là Alex Krizhevsky. Ở trong bài viết này, chúng ta sẽ đi sâu vào tìm hiểu kiến trúc AlexNet và đóng góp chính của nó trong CNN.&lt;/p&gt;

&lt;h2 id=&#34;đầu-vào&#34;&gt;Đầu vào&lt;/h2&gt;

&lt;p&gt;Như đã đề cập ở phần trên, mạng AlexNet đã thắng hạng nhất trong cuộc thi ILSVRC năm 2012. Mô hình giải quyết bài toán phân lớp một bức ảnh vào 1 lớp trong 1000 lớp khác nhau (vd gà, chó, mèo &amp;hellip; ). Đầu ra của mô hình là một vector có 1000 phần tử. Phần tử thứ i của vector đại diện cho xác suất bức ảnh thuộc về lớp thứ i. Do đó, tổng của các phần tử trong vector là 1.&lt;/p&gt;

&lt;p&gt;Đầu vào của mạng AlexNet là một bức ảnh RGB có kích thước 256x256 pixel. Toàn bộ các bức ảnh của tập train và tập test đều có cùng kích thước là 256x256. Nếu một bức ảnh nào đó không có kích thước 256x256, bức ảnh đó sẽ được chuyển về kích thước đúng 256x256. Những bức hình có kích thước nhỏ hơn 256 thì sẽ được phóng bự lên đến kích thước 256, những bức hình nào có kích thước lớn hơn 256 thì sẽ được cắt loại phần thừa để nhận được bức hình có kích thước 256x256. Hình ảnh ở dưới là một ví dụ về việc điều chỉnh bức ảnh về kích thước 256x256.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/AlexNet-Resize-Crop-Input.jpg&#34; alt=&#34;Hình ảnh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nếu ảnh đầu vào là ảnh xám (grayscale), bức ảnh trên sẽ được chuyển đổi thành định dạng RGB bằng cách tạo ra 3 layer kênh màu giống nhau từ ảnh xám.&lt;/p&gt;

&lt;p&gt;Sau khi chuẩn hoá hết tất cả các ảnh về dạng 256x256x3, nhóm tác giả chỉ sử dụng một phần của bức ảnh có kích thước 227x227x3 của một bức ảnh làm đầu vào cho mạng neural network. Trong bài báo nhóm tác giả ghi là 224x224, nhưng đây là một lỗi nhỏ của nhóm tác giả, và kích thước thực tế đầu vào của bức ảnh là 227x227.&lt;/p&gt;

&lt;h2 id=&#34;kiến-trúc-alexnet&#34;&gt;Kiến trúc AlexNet&lt;/h2&gt;

&lt;p&gt;Kiến trúc AlexNet lớn hơn nhiều so với các kiến trúc CNNs được sử dụng trong thị giác máy tính trước kia (trước năm 2010), vd kiến trúc LeNet của Yann LeCun năm 1998. Nó có 60 triệu tham số và 650000 neural và tốn khoảng từ năm đến sáu ngày huấn luyện trên hai GPU GTX 580 3GB. Ngày nay, với sự tiến bộ vượt bật của GPU, chúng ta có nhiều kiến trúc CNN có cấu trúc phức tạp hơn, và hoạt động rất hiệu quả trên những tập dữ liệu phức tạp. Nhưng tại thời điểm năm 2012 thì việc huấn luyện mô hình với lượng tham số và neural lớn như vậy là một vấn đề cực kỳ khó khăn. Nhìn kỹ vào hình bên dưới để hiểu rõ hơn về kiến trúc AlexNet.
&lt;img src=&#34;/post_image/AlexNet-1.png&#34; alt=&#34;Kiến trúc AlexNet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AlexNet bao gồm 5 convolution Layer và 3 Fully connected Layers.&lt;/p&gt;

&lt;p&gt;Những convolution layer ( hay còn gọi với tên khác là các filter) rút trích các thông tin hữu ích trong các bức ảnh. Trong một convolution layer bất kỳ thường bao gồm nhiều kernel có cùng kích thước. Ví dụ như convolution layer đầu tiên của AlexNet chứa 96 kernel có kích thước 11x11x3. Thông thường thì width và height của một kernel bằng nhau, và độ sâu (depth) thường bằng số lượng kênh màu.&lt;/p&gt;

&lt;p&gt;Convolutional 1 và convolution 2 kết nối với nhau qua một Overlapping Max Pooling ở giữa. Tương tự như vậy giữa convolution 2 và convolution 3. Convolutional 3, convolution 4, convolution 5 kết nối trực tiếp với nhau, không thông qua trung gian. Convolutional 5 kết nối fully connected layter 1 thông qua một Overlapping Max pooling, tiếp theo mà một fully connected layter nữa. Và cuối cùng là một bộ phân lớp softmax với 1000 lớp nhãn (các bạn có thể xem hình kiến trúc mạng AlexNet ở trên để có cái nhìn tổng quát hơn).&lt;/p&gt;

&lt;p&gt;ReLU nonlinerity được sử dụng sau tất các các convolution và fully connected layer. Trước đây, ReLU nonlinerity của lớp convolution 1 và 2 thường theo sau bởi một bước chuẩn hoá cục bộ (local normalization) rồi mới thực hiện pooling. Tuy nhiên, các nghiên cứu sau đó nhận thấy rằng việc sử dụng normalization không thật sự hữu ích. Do vậy chúng ta sẽ không đi chi tiết về vấn đề đó.&lt;/p&gt;

&lt;h2 id=&#34;overlapping-max-pooling&#34;&gt;Overlapping Max Pooling&lt;/h2&gt;

&lt;p&gt;Max Pooling layer thường được sử dụng để giảm chiều rộng và chiều dài của một tensor nhưng vẫn giữ nguyên chiều sâu. Overlapping Max Pool layter cũng tương tự như Max Pool layter, ngoại trừ việc là một window của bước này sẽ có một phần chồng lên window của bước tiếp theo. Tác giả sử dụng pooling có kích thước 3x3 và bước nhảy là 2 giữa các pooling. Nghĩa là giữa pooling này và pooling khác sẽ overlapping với nhau 1 pixel. Các thí nghiệm thực tế đã chứng minh rằng việc sử dụng overlapping giữa các pooling giúp giảm độ lỗi top-1 error 0.4% và top-5 error là 0.3% khi so với việc sử dụng pooling có kích thước 2x2 và bước nhảy 2 (vector output của cả hai đều có số chiều bằng nhau).&lt;/p&gt;

&lt;h2 id=&#34;relu-nonlinearity&#34;&gt;ReLu Nonlinearity&lt;/h2&gt;

&lt;p&gt;Một cải tiến quan trọng khác của AlexNet là việc sử dụng hàm phi tuyến ReLU. Trước đây, các nhóm nghiên cứu khác thường sử dụng hàm kích hoạt là hàm Tanh hoặc hàm Sigmoid để huấn luyên mô hình neural network. AlexNet chỉ ra rằng, khi sử dụng ReLU, mô hình deep CNN sẽ huấn luyện nhanh hơn so với viêc sử dụng tanh hoặc sigmoid. Hình bên dưới được rút ra từ bài báo chỉ ra rằng với việc sử dụng ReLU (đường nét liền trong hình), AlexNet đạt độ lỗi 25% trên tập huấn luyện và nhanh hơn gấp 6 lần so với mô hình tương tự nhưng sử dụng Tanh (đường nét đứt trong hình). Thí nghiệm trên sử dụng tập dữ liệu CIFAR-10 để huấn luyện.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/ReluNonlinearity-768x635.png&#34; alt=&#34;Tốc độ hội tụ của mạng AlexNet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Để hiểu rõ hơn lý do vì sao ReLU lại nhanh hơn so với các hàm khác, chúng ta hãy đối sánh hình dạng giá trị output của các hàm trên.&lt;/p&gt;

&lt;p&gt;Công thức của ReLU là: f(X) = max(0,x)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/Tanh-300x238.png&#34; alt=&#34;Hàm kích hoạt của ReLU và tanh&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn kỹ vào hình trên, ta có nhận xét rằng: hàm tanh đạt giá trị bão hoà khi giá trị z &amp;gt;2.5 và z &amp;lt; -2.5 (số 2.5 là số cảm tính của mình). Và tại vùng |z|&amp;gt;2.5, thì độ dốc của hàm hầu như gần như bằng 0, |z| càng lớn thì độ dốc càng gần 0 hơn. Vì lý do này nên gradient descent sẽ hội tụ chậm. Còn đối với hàm ReLU, với giá trị z dương thì độ dốc của hàm không gần bằng 0 như hàm tanh. Điều này giúp cho việc hội tụ xảy ra nhanh hơn. Với giá trị z âm, độ dốc bằng 0, tuy nhiên, hầu hết các giá trị của các neural trong mạng thường có giá trị dương, nên trường hợp âm ít (hiếm) khi xảy ra. ReLU huấn luyện nhanh hơn so với sigmoid cũng bởi lý do tương tự.&lt;/p&gt;

&lt;h2 id=&#34;reducing-overfitting&#34;&gt;Reducing overfitting&lt;/h2&gt;

&lt;h3 id=&#34;overfitting-là-gì&#34;&gt;Overfitting là gì?&lt;/h3&gt;

&lt;p&gt;Khi bạn dạy một đứa trẻ từ 2-5 tuổi về việc cộng hai số, chúng sẽ học rất nhanh và trả lời đúng hầu hết các câu hỏi mà chúng ta đã dạy chúng. Tuy nhiên, chúng sẽ trả lời sai đối với những câu hỏi hơi lắc léo một chút (câu hỏi tương tự câu chúng ta đã dạy, nhưng thêm một xíu thông tin đòi hỏi trẻ phải suy nghĩ), hoặc các câu hỏi chưa được dạy. Lý do chúng trả lời sai những câu hỏi đó là khi trả lời những câu hỏi được dạy, chúng thường nhớ lại câu trả lời, chứ không thực sự hiểu câu hỏi. Cái này ở Việt Nam ta gọi là học vẹt.&lt;/p&gt;

&lt;p&gt;Tương tự vậy, Neural network chính bản thân nó có khả năng học được những gì được dạy, tuy nhiên, nếu quá trình huấn luyện của bạn không tốt, mô hình có khả năng sẽ giống như những đứa trẻ trên kia, hồi tưởng lại những gì đã dạy cho chúng mà không hiểu bản chất. Và kết quả Neural Network sẽ hoạt động tốt trên tập huấn luyện ( nhưng chúng không rút ra được bản chất chính của vấn đề), và kết quả trên tập test tệ. Người ta gọi trường hợp trên là &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Nhóm nghiên cứu AlexNet sử dụng nhiều phương pháp khác nhau để giảm overfitting.&lt;/p&gt;

&lt;h3 id=&#34;data-augmentation&#34;&gt;Data Augmentation&lt;/h3&gt;

&lt;p&gt;Việc sử dụng nhiều biến thể khác nhau của một bức hình có thể giúp ngăn mô hình không bị overfitting. Với việc sử dụng nhiều biến thể của 1 bức hình, bạn bắt ép mô hình không học vẹt dữ liệu. Có nhiều cách khác nhau để sinh ra dữ liệu mới dựa vào dữ liệu có sẵn. Một vài các mà nhóm AlexNet đã sử dụng là.&lt;/p&gt;

&lt;h4 id=&#34;data-augmentation-by-mirroring&#34;&gt;Data Augmentation by Mirroring&lt;/h4&gt;

&lt;p&gt;Ý tưởng của việc này là lấy ảnh trong gương của một bức hình (ảnh ảo). Nhìn vào ảnh bên dưới, bên trái là hình gốc của con mèo trong tập huấn luyện, bên phải là ảnh của con mèo khi thêm hiệu ứng hình qua gương (đơn giản là xoay qua trục y là được )
&lt;img src=&#34;/post_image/AlexNet-Data-Augmentation-Mirror-Image.jpg&#34; alt=&#34;Tái tạo ảnh sử dụng phản ảnh&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;data-augmentation-by-random-crops&#34;&gt;Data Augmentation by Random Crops&lt;/h4&gt;

&lt;p&gt;Việc lựa chọn vị trí ảnh gốc một cách ngẫu nhiên cũng giúp chúng ta có thêm một ảnh khác so với ảnh gốc ban đầu.&lt;/p&gt;

&lt;p&gt;Nhóm tác giả của AlexNet rút trích ngẫu nhiên bức ảnh có kích thước 227x227 từ bức ảnh 256x256 ban đầu làm input dầu vào cho mô hình. Bằng cách này, chúng ta có thể tăng số lượng dữ liệu lên gấp 2048 lần bằng việc sử dụng cách này.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/AlexNet-Data-Augmentation-Random-Crops.jpg&#34; alt=&#34;radom select&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bốn bức ảnh được crop ngẫu nhiên ở trên thoạt nhìn có vẻ giống nhau, nhưng thực chất không phải như vậy.&lt;/p&gt;

&lt;p&gt;Với việc sử dụng Data Augmentation, chúng ta đang bố gắng dạy cho mô hình rằng với việc nhìn hình con mèo qua gương, nó vẫn là con mèo, hoặc hình hình con mèo ở bất kỳ góc độ nào thì nó vẫn là nó.&lt;/p&gt;

&lt;h3 id=&#34;dropout&#34;&gt;Dropout&lt;/h3&gt;

&lt;p&gt;Với gần 60 triệu tham số trong tập huấn luyện, việc overfitting xảy ra là điều dễ hiểu. Các tác giả của AlexNet đã thực nghiệm nhiều cách nữa để giảm overfitting. Họ sử dụng một kỹ thuật gọi là dropout - kỹ thuật này được giới thiệu ở bài báo khác của G.E. Hintol vào năm 2012. Kỹ thuật này khá đơn giản, một neural sẽ có xác suất bị loại khỏi mô hình là 0.5. Khi một neural bị loại khỏi mô hình, nó sẽ không được tham qia vào quá trình lan truyền tiến hoặc lan truyền ngược. Cho nên, mỗi giá trị input sẽ đi qua một kiến trúc mạng khác nhau. Như mô tả ở hình động ở dưới, kết quả là giá trị của tham số trọng số sẽ tốt hơn và khó bị overfitting hơn. Trong quá trình test, toàn bộ network được sử dụng, không có dropout, tuy nhiên, giá trị output sẽ scaled bởi tham số 0.5 tương ứng với những neural không sử dụng trong quá trình trainning. Với việc sử dụng dropout, chúng ta sẽ tăng gấp đôi lần lặp cần thiết để đạt được độ hội tụ, nhưng khi không sử dụng dropout, mạng AlexNet rất dễ bị overfitting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post_image/dropoutAnimation.gif&#34; alt=&#34;drop out&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ngày nay, chuẩn hoá dropout là một yếu tố không thể thiếu và các mô hình sử dụng nó thường có kết quả tốt hơn so với mô hình tương tự không sử dụng dropout. Chúng ta sẽ bàn sâu hơn về dropout ở một bài khác trong tương lai.&lt;/p&gt;

&lt;p&gt;Tham khảo&lt;/p&gt;

&lt;p&gt;ImageNet Classification with Deep Convolutional Neural Networks  by Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, 2012&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.learnopencv.com/understanding-alexnet/&#34;&gt;https://www.learnopencv.com/understanding-alexnet/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>alexblack2202@gmail.com (Phạm Duy Tùng)</author>
      <guid>/about/</guid>
      <description>&lt;p&gt;Phạm Duy Tùng.&lt;/p&gt;

&lt;p&gt;R&amp;amp;D at MWG.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>