<!DOCTYPE HTML>

<html>

    <head>

	<script defer="defer" async="async" src="https://www.googleoptimize.com/optimize.js?id=OPT-52RT2BV"></script>
        <script type="application/ld+json">
    {
        "@context" : "http://schema.org",
        "@type" : "BlogPosting",
        "mainEntityOfPage": {
             "@type": "WebPage",
             "@id": "/"
        },
        "articleSection" : "blog",
        "name" : "Deep Learning based Multiple Human Pose Estimation using OpenCV",
        "headline" : "Deep Learning based Multiple Human Pose Estimation using OpenCV",
        "description" : "Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation.",
        "inLanguage" : "en",
        "author" : "alexblack2202@gmail.comPhạm Duy Tùng",
        "creator" : "alexblack2202@gmail.comPhạm Duy Tùng",
        "publisher": "alexblack2202@gmail.comPhạm Duy Tùng",
        "accountablePerson" : "alexblack2202@gmail.comPhạm Duy Tùng",
        "copyrightHolder" : "alexblack2202@gmail.comPhạm Duy Tùng",
        "copyrightYear" : "2018",
        "datePublished": "2018-10-05 00:19:00 &#43;0300 &#43;0300",
        "dateModified" : "2018-10-05 00:19:00 &#43;0300 &#43;0300",
        "url" : "/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/",
        "wordCount" : "1263",
        "keywords" : [ "Machine learning","Deeplearning","multiple pose estimation","Blog" ]
    }
    </script>
        
            
                <title>Deep Learning based Multiple Human Pose Estimation using OpenCV</title>
            
        

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        
		<meta name="generator" content="Phạm Duy Tùng" />
        <meta property="fb:pages" content="1244186728962161" />
<meta name='dmca-site-verification' content='Uk5YSGpDdzE4c0xnV0FaM3lEQXpLd0Exa29OQXFIT2d3Y05OM0daMmFlVT01' />
        
  
    
  

  

  
  
  
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content='/favicon/mstile.png'>
  <meta name="application-name" content="Phạm Duy Tùng Machine Learning Blog">
  <meta name="msapplication-tooltip" content="Blog ML của Phạm Duy Tùng và Đặng Thị Hằng">
   
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">



        
            <meta name="author" content="Phạm Duy Tùng">
        
        
            <meta name="description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation.">
        

        <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning based Multiple Human Pose Estimation using OpenCV"/>
<meta name="twitter:description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation."/>
<meta name="twitter:site" content="@example"/>

        <meta property="og:title" content="Deep Learning based Multiple Human Pose Estimation using OpenCV" />
<meta property="og:description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation/" /><meta property="article:published_time" content="2018-10-05T00:19:00&#43;03:00"/>
<meta property="article:modified_time" content="2018-10-05T00:19:00&#43;03:00"/>

        <meta property="og:image" content="//images/logo.png">
        <meta property="og:image:type" content="image/png">
        <meta property="og:image:width" content="512">
        <meta property="og:image:height" content="512">
        
<meta itemprop="name" content="Deep Learning based Multiple Human Pose Estimation using OpenCV">
<meta itemprop="description" content="Ở bài viết này, chúng ta đề cập đến vấn đề sử dụng Deep Neural Net trong việc thực hiện Human Pose Estimation.">


<meta itemprop="datePublished" content="2018-10-05T00:19:00&#43;03:00" />
<meta itemprop="dateModified" content="2018-10-05T00:19:00&#43;03:00" />
<meta itemprop="wordCount" content="1263">



<meta itemprop="keywords" content="Machine learning,Deeplearning,multiple pose estimation," />

        

        
            
        

        
        
          
			 <link rel="stylesheet" type="text/css" href="/css/bootstrap.min.css">


          
            
        

        
            
                
            
        


  
    
    <link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/styles/xcode.min.css' rel='stylesheet' type='text/css' />
  


      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-114911596-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

	  <style>
	  
	  body{
font-family: Helvetica,Arial,sans-serif;
}

.card{
	margin-bottom: 10px;
}

#disqus_thread{
padding: 0 5px;
}

.item-header{
padding: 0;
}

.single-content-img{
width: 100%;
    max-height: 450px !important;
    background-size: cover;
    display: block;
    background-position: center;
}

.thumbnail {
    position: relative;
}

.caption {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
	background: rgba(0, 0, 0, 0.25);
	text-align:left;
}

.caption .title{
	font-size: 1.6em;
    line-height: 1.4em;
    top: 0;
	margin-left:20px;
	margin-top:20px;
	
}

.caption .title-caption{
margin-left:10px;
}

#content p{
text-align: justify;
}

#content img{
	display: block;
    margin-left: auto;
margin-right: auto;
max-width:98%;
}

img + strong {
    font-style: normal;
    display: inherit;
    text-align: center;
}
.img-news{
max-height:150px;
width:100%;
}

.news-tittle{
	padding-top:15px;
	text-align:justify;
}

.author{
	color: orange;
}
.author-inline{
	color: orange;
}

.adv{
height:18px;
}


.hljs{
    white-space: pre-wrap;
    white-space: -moz-pre-wrap;
    white-space: -pre-wrap;
    white-space: -o-pre-wrap;
    word-wrap: break-word;}
.titledetail {
    display: block;
    overflow: hidden;
    line-height: 53px;
    font-size: 45px;
    font-family: 'Roboto Condensed',sans-serif;
    font-weight: 600;

    margin: auto;
	padding: 0 ;
}

.newsrelate {
    display: block;
    overflow: hidden;
	 list-style:none;
}
a ,a:hover{
    text-decoration: none;
}
.newsrelate li {
    float: left;
    overflow: hidden;
    width: 30%;
    margin-left: 2.5%;
    margin-bottom: 15px;
}

.newsrelate li a {
    display: block;
    overflow: hidden;
}

.userdetail {
    display: block;
    overflow: hidden;
    margin: 0 10px 0 0;
    padding: 15px 0;
}
.newsrelate li h3 {
    display: block;
    overflow: hidden;
    line-height: 1.3em;
    font-size: 16pt;
    line-height: 22px;
    font-weight: 300;
    font-family: Arial,Helvetica,sans-serif;
    width: auto;
    margin: 5px auto;
}

.titlerelate {
    overflow: hidden;
    font-size: 18px;
    font-weight: 600;
    font-family: 'Roboto Condensed',sans-serif;
    line-height: 32px;
    text-transform: uppercase;
}
article .captionnews {
    color: #999;
    font-size: 14px;
    font-style: italic;
    padding: 10px;
    text-align: center;
    margin-bottom: 15px;
}

.bgtrans h1 {
    display: block;
    overflow: hidden;
    font-size: 45px;
    line-height: 55px;
    margin: auto;
    left: 0;
    right: 0;
    bottom: 50px;
    font-family: 'Roboto Condensed',sans-serif;
    font-weight: 600;
}

.bgcover {
    display: block;
    overflow: hidden;
    height: 450px;
    background: no-repeat center center;
    -webkit-background-size: cover;
    -moz-background-size: cover;
    -o-background-size: cover;
    background-size: cover;
    position: relative;
    margin-bottom: -65px;
}
.bgcover .bgtrans .userdetail {
    width: 800px;
    margin: auto;
    position: absolute;
    left: 0;
    right: 0;
    bottom: 10px;
}
.userdetail {
    display: block;
    overflow: hidden;
    margin: 0 10px 0 0;
    padding: 15px 0;
}

.amlich{border-collapse:collapse;font-size:14px;font-family:Roboto,sans-serif}.calendar{font-size:12px}.calendar td{background-color:#e9eff3}.calendar-month{background-color:#1e8cbe!important;color:#fff;text-shadow:0 0 3px #000;padding:6px;font-weight:700;text-transform:uppercase;font-size:14px!important}.amlich-tennam{text-align:center;font-weight:700;color:#000;background-color:#ccc;font-size:14px;font-family:Roboto,sans-serif}.amlich .calendar-month,.amlich .calendar-b-left,.amlich .calendar-b-right{text-align:center;padding:4px 0;font-size:11px}.amlich .calendar-day{text-align:center;font-weight:700}.amlich .calendar-day .day-num{font-size:80px;font-family:Roboto,sans-serif;line-height:100%;color:#31708f}.amlich .lunar-day-num{font-size:44px;line-height:100%;font-weight:700;color:#3c763d}.amlich .calendar-holiday,.amlich .calendar-hoangdao{padding:0 4px 4px;font-size:11px;text-align:center}.amlich .calendar-holiday{color:#a94442;font-weight:700}.amlich a{text-decoration:none;color:#fff}.amlich a:hover{color:red}.amlich .tenthang,.amlich .navi-l,.amlich .navi-r{text-align:center;padding:6px;background-color:#1e8cbe;color:#fff;font-weight:700}.amlich .tenthang{text-shadow:0 0 3px #000}.amlich .navi-l{font-size:12px}.amlich .navi-r{font-size:12px}.amlich .ngaytuan{text-align:center;color:#303;background-color:#ddd;padding:3px;width:14.286%;font-size:10px;font-weight:700}.amlich .ngaythang,.amlich .homnay,.amlich .tet,.amlich .leam,.amlich .leduong{cursor:pointer;border-bottom:solid 1px #eee;padding:3px;width:14.286%}.amlich .ngaythang div,.amlich .homnay div,.amlich .tet div,.amlich .leam div,.amlich .leduong div{line-height:110%}.amlich .ngaythang{color:#5a5c5b}.amlich tr:nth-child(odd) td.ngaythang:nth-child(odd){background-color:#f9f9f9}.amlich tr:nth-child(odd) td.ngaythang:nth-child(even){background-color:#fff}.amlich tr:nth-child(even) td.ngaythang:nth-child(odd){background-color:#fff}.amlich tr:nth-child(even) td.ngaythang:nth-child(even){background-color:#f9f9f9}.amlich tr td.ngaythang:hover{background-color:#f5f5f5!important}.amlich .homnay{background-color:#fcf8e3;color:#fff}.amlich .homnay:hover{background-color:#faf2cc}.amlich .tet{background-color:#f2dede}.amlich .tet:hover{background-color:#ebcccc}.amlich .leam{background-color:#d9edf7}.amlich .leam:hover{background-color:#c4e3f3}.amlich .leduong{background-color:#dff0d8}.amlich .leduong:hover{background-color:#d0e9c6}.amlich .am{text-align:right;font-size:75%;color:#554c00}.amlich .am2{text-align:right;font-size:75%;color:#337ab7;font-weight:700}.amlich .t2t6{text-align:left;color:#5a5c5b;font-weight:700}.amlich .t7{font-weight:700;text-align:left;color:blue}.amlich .cn{font-weight:700;text-align:left;color:red}
	  </style>
	  
<script>    (function(c,l,a,r,i,t,y){        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i+"?ref=bwt";        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);    })(window, document, "clarity", "script", "882qzk4o0n");</script>
    </head>
    <body class="text-dark bg-light">
<script  async defer>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '1546237302193677',
      xfbml      : true,
      version    : 'v5.0'
    });
    FB.AppEvents.logPageView();
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "https://connect.facebook.net/en_US/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v5.0&appId=1853483258232756&autoLogAppEvents=1"></script>
      
      

    
    
<header id="header"  style="background: #790014; color: hsla(0,0%,100%,1);">
<div class="container">
    <nav class="navbar navbar-expand-md navbar-dark">
	
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav mr-auto">
            
                <li class="nav-item">
                    <a class='nav-link' href="/blog">
                            <i class="fa fa-home active">&nbsp;</i>Home
                    </a>
                </li>
            
        </ul>
    </nav>
    </div></div>
</header>


   
    
	<div class="container">
	<div class="adv"></div>
	<div class="row">
    <main role="main"  class="col-md-9 border-left border-right" >
	
        
        
          


        
		 <div class="header">
           
		<div class="bgtrans">
		 <h1 class="titledetail">Deep Learning based Multiple Human Pose Estimation using OpenCV</h1>
		<div class="userdetail">
			 
			  <time class="published" 
            datetime='2018-10-05'>
            05/10/2018</time>
		 - 
			   <span class="author">Phạm Duy Tùng</span>
			   
			</div>	
		
			</div>
			</div>
  <div class="fb-like" data-share="true"  data-width="800"  data-show-faces="true">
</div>
			
		
		
			 
            
        
       



  

  
 
  <div style="    margin: 0;
    border-bottom: 1px dotted #d9d9d9;
    padding-bottom: 20px;"></div>
  <div id="content">
    

<h2 id="lời-mở-đầu">Lời mở đầu</h2>

<p>Lưu ý: Để sử dụng được các mô hình trong bài viết này, bạn phải sử dụng phiên bản opencv &gt; 3.4.1.</p>

<p>Ở bài viết trước, chúng ta đã tìm hiểu cách thức rút trích khung xương sử dụng DNN và đã áp dụng thành công trên ảnh có chứa 1 đối tượng người. Trong bài viết này, chúng ta sẽ thực hiện áp dụng mô hình cho bài toán có nhiều người trong cùng 1 bức ảnh.</p>

<h2 id="sử-dụng-pretrain-model-trong-bài-toán-multiple-pose-estimation">Sử dụng pretrain model trong bài toán multiple Pose Estimation</h2>

<p>Trong bài viết này, chúng ta tiếp tục sử dụng mô hình MPI để dò tìm các điểm đặc trưng của con người và rút ra mô hình khung xương. Kết quả trả về của thuật toán gồm 15 đặc trưng như bên dưới.</p>

<pre><code class="language-python">Head – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,
Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,
Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,
Left Ankle – 13, Chest – 14, Background – 15
</code></pre>

<p>Áp dụng mô hình với ảnh của nhóm T-ARA.</p>

<pre><code class="language-python">import cv2

nPoints = 15
POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]

protoFile = &quot;pose/mpi/pose_deploy_linevec.prototxt&quot;
weightsFile = &quot;pose/mpi/pose_iter_160000.caffemodel&quot;

net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)

frame = cv2.imread(&quot;tara1.jpg&quot;)

inWidth = 368
inHeight = 368
 
# Prepare the frame to be fed to the network
inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)
 
# Set the prepared object as the input blob of the network
net.setInput(inpBlob)

output = net.forward()
</code></pre>

<p>Thử show lên vị trí vùng cổ trong hình.</p>

<pre><code class="language-python">
i = 0
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_head_heatmap.png" alt="Hình với điểm đặc trưng vùng đầu" /></p>

<p>Thử show lên hình điểm đặc trưng vùng cổ</p>

<pre><code class="language-python">i = 1
probMap = output[0, i, :, :]
probMap = cv2.resize(probMap, (frameWidth, frameHeight))

import matplotlib.pyplot as plt 

plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))
plt.imshow(probMap, alpha=0.5)
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_neck_heatmap.png" alt="Hình với điểm đặc trưng vùng cổ" /></p>

<p>Bằng một số phép biến đổi quen thuộc có sẵn trong opencv, chúng ta hoàn toàn có thể lấy được toạ độ của các điểm keypoint một cách dễ dàng.</p>

<pre><code class="language-python">
# Find the Keypoints using Non Maximum Suppression on the Confidence Map
def getKeypoints(probMap, threshold=0.1):
    
    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)

    mapMask = np.uint8(mapSmooth&gt;threshold)
    keypoints = []
    
    #find the blobs
    _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    #for each blob find the maxima
    for cnt in contours:
        blobMask = np.zeros(mapMask.shape)
        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)
        maskedProbMap = mapSmooth * blobMask
        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)
        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))

    return keypoints


detected_keypoints = []
keypoints_list = np.zeros((0,3))
keypoint_id = 0
threshold = 0.1
for i in range(nPoints):
    probMap = output[0, i, :, :]
    probMap = cv2.resize(probMap, (frameWidth, frameHeight))

    keypoints = getKeypoints(probMap, threshold)
    keypoints_with_id = []
    for j in range(len(keypoints)):
        keypoints_with_id.append(keypoints[j] + (keypoint_id,))
        keypoints_list = np.vstack([keypoints_list, keypoints[j]])
        keypoint_id += 1

    detected_keypoints.append(keypoints_with_id)



frameClone = cv2.cvtColor(frameCopy,cv2.COLOR_BGR2RGB)
for i in range(nPoints):
    for j in range(len(detected_keypoints[i])):
        cv2.circle(frameClone, detected_keypoints[i][j][0:2], 3, [0,0,255], -1, cv2.LINE_AA)

plt.imshow(frameClone) 
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_keypoint.png" alt="Hình toàn bộ điểm đặc trưng" /></p>

<p>Cuối cùng, chúng ta sẽ nối các điểm đặc trưng của các nhân vật thông qua thuật toán Part Affinity Heatmaps. Thuật toán này được đề xuất bởi nhóm tác giả Zhe Cao, Tomas Simon,Shih-En Wei, Yaser Sheikh thuộc phòng thí nghiệm The Robotics Institute trường đại học Carnegie Mellon. Các bạn có nhu cầu có thể tìm hiểu ở <a href="https://arxiv.org/pdf/1611.08050.pdf">https://arxiv.org/pdf/1611.08050.pdf</a>.</p>

<pre><code class="language-python">
mapIdx = [[16,17], [18,19], [20,21], [22,23], [24,25], [26,27], [28,29], [30,31], [32,33], [34,35], [36,37], [38,39], [40,41], [42,43]]



colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],
         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],
         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]
# Find valid connections between the different joints of a all persons present
def getValidPairs(output):
    valid_pairs = []
    invalid_pairs = []
    n_interp_samples = 10
    paf_score_th = 0.1
    conf_th = 0.5
    # loop for every POSE_PAIR
    for k in range(len(mapIdx)):
        # A-&gt;B constitute a limb
        pafA = output[0, mapIdx[k][0], :, :]
        pafB = output[0, mapIdx[k][1], :, :]
        pafA = cv2.resize(pafA, (frameWidth, frameHeight))
        pafB = cv2.resize(pafB, (frameWidth, frameHeight))


        # Find the keypoints for the first and second limb
        candA = detected_keypoints[POSE_PAIRS[k][0]]
        candB = detected_keypoints[POSE_PAIRS[k][1]]
        nA = len(candA)
        nB = len(candB)

        # fig=plt.figure(figsize=(8, 8))

        # interp_coord = list(zip(np.linspace(candA[0][0], candB[0][0], num=n_interp_samples),
        #                                     np.linspace(candA[0][1], candB[0][1], num=n_interp_samples)))

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 1)
        
        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)


        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafA, alpha=0.5)

        # frameClone1 = frameClone.copy() 
        # fig.add_subplot(1, 2, 2)
        

        

        # for xx in interp_coord:
        #     cv2.circle(frameClone1,(int(xx[0]),int(xx[1])), 3, [0,0,255], -1, cv2.LINE_AA)
        
        # plt.imshow(cv2.cvtColor(frameClone1, cv2.COLOR_BGR2RGB))
        # plt.imshow(pafB, alpha=0.5)
        # plt.show()



        

        # If keypoints for the joint-pair is detected
        # check every joint in candA with every joint in candB 
        # Calculate the distance vector between the two joints
        # Find the PAF values at a set of interpolated points between the joints
        # Use the above formula to compute a score to mark the connection valid
        
        if( nA != 0 and nB != 0):
            valid_pair = np.zeros((0,3))
            for i in range(nA):
                max_j=-1
                maxScore = -1
                found = 0
                for j in range(nB):
                    # Find d_ij
                    d_ij = np.subtract(candB[j][:2], candA[i][:2])
                    norm = np.linalg.norm(d_ij)
                    if norm:
                        d_ij = d_ij / norm
                    else:
                        continue
                    # Find p(u)
                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),
                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))
                    # Find L(p(u))
                    paf_interp = []
                    for k in range(len(interp_coord)):
                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],
                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) 
                    # Find E
                    paf_scores = np.dot(paf_interp, d_ij)
                    avg_paf_score = sum(paf_scores)/len(paf_scores)
                    
                    # Check if the connection is valid
                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -&gt; Valid Pair  
                    if ( len(np.where(paf_scores &gt; paf_score_th)[0]) / n_interp_samples ) &gt; conf_th :
                        if avg_paf_score &gt; maxScore:
                            max_j = j
                            maxScore = avg_paf_score
                            found = 1
                # Append the connection to the list
                if found:            
                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)

            # Append the detected connections to the global list
            valid_pairs.append(valid_pair)

            pprint(valid_pair)
        else: # If no keypoints are detected
            print(&quot;No Connection : k = {}&quot;.format(k))
            invalid_pairs.append(k)
            valid_pairs.append([])
    pprint(valid_pairs)
    return valid_pairs, invalid_pairs

# This function creates a list of keypoints belonging to each person
# For each detected valid pair, it assigns the joint(s) to a person
# It finds the person and index at which the joint should be added. This can be done since we have an id for each joint
def getPersonwiseKeypoints(valid_pairs, invalid_pairs):
    # the last number in each row is the overall score 
    personwiseKeypoints = -1 * np.ones((0, 19))

    for k in range(len(mapIdx)):
        if k not in invalid_pairs:
            partAs = valid_pairs[k][:,0]
            partBs = valid_pairs[k][:,1]
            indexA, indexB = np.array(POSE_PAIRS[k])

            for i in range(len(valid_pairs[k])): 
                found = 0
                person_idx = -1
                for j in range(len(personwiseKeypoints)):
                    if personwiseKeypoints[j][indexA] == partAs[i]:
                        person_idx = j
                        found = 1
                        break

                if found:
                    personwiseKeypoints[person_idx][indexB] = partBs[i]
                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]

                # if find no partA in the subset, create a new subset
                elif not found and k &lt; 17:
                    row = -1 * np.ones(19)
                    row[indexA] = partAs[i]
                    row[indexB] = partBs[i]
                    # add the keypoint_scores for the two keypoints and the paf_score 
                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]
                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])
    return personwiseKeypoints

valid_pairs, invalid_pairs = getValidPairs(output)

personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)


for i in range(nPoints-1):
    for n in range(len(personwiseKeypoints)):
       
        index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]
        if -1 in index:
            continue
        B = np.int32(keypoints_list[index.astype(int), 0])
        A = np.int32(keypoints_list[index.astype(int), 1])
        cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)
       
        
        
plt.imshow(frameClone)
    # plt.imshow(mapMask, alpha=0.5)
plt.show()
</code></pre>

<p><img src="/post_image/multiple_pose_estimation_t-ara_finalresult.png" alt="Hình kết quả cuối cùng" /></p>

<p>Hẹn gặp lại các bạn ở những bài viết tiếp theo.</p>

<p>Bài viết này được viết dựa vào nguồn <a href="https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/">https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/</a> của tác giả VIKAS GUPTA. Tôi sử dụng tập model và hình ảnh khác với bài viết nguyên gốc của tác giả.</p>

	<div style="    margin: 0;
    border-bottom: 1px dotted #d9d9d9;
    padding-bottom: 20px;"></div>
	<div><i class='fas fa-donate'>Nếu bạn thấy nội dung của bài viết thật sự hữu ích và bạn muốn đóng góp cho blog để có thêm nhiều bài viết chất lượng hơn nữa, các bạn có thể ủng hộ blog bằng một cốc trà hoặc một cốc cà phê nhỏ qua <a target="_blank" href="https://nhantien.momo.vn/7abl2tSivGa"> https://nhantien.momo.vn/7abl2tSivGa</a> hoặc <a  target="_blank" href="paypal.me/tungduypham">paypal.me/tungduypham</a>. Sự ủng hộ của các bạn là nguồn động viên quý giá để chúng tôi có thêm động lực và chia sẻ nhiều điều mà chúng tôi tìm hiểu được đến với cộng đồng. Trân trọng cảm ơn.</i></div>
  </div>
  
		
  <footer class="col-md-10  mx-auto">
  <ul class="stats list-unstyled">
 
    
  <li class="tags">
    <ul class="list-inline">
       
            
            
                <i class="fa fa-tags"></i>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/machine-learning">Machine learning</a></li>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/deeplearning">Deeplearning</a></li>
                
                
                <li class="list-inline-item"><a class="article-category-link" href="/tags/multiple-pose-estimation">multiple pose estimation</a></li>
                
            
        
    </ul>
  </li>
  
</ul>

  <div class="fb-like" data-share="true"  data-width="800"  data-show-faces="true">
</div>
    
  </footer>
  <hr/>
<div class="infinite-container featured-task col-md-8 mx-auto">
<div class="titlerelate">Bài viết khác</div>

<div class="card-deck card-break infinite-item">



    
        <div class="card  col-md-6" style="padding-top:15px;">
		<a href="/blog/2018-10-04-deep-learning-base-human-pose-estimation/"
                class="button big previous">
		
		<img class="card-img-top lazy" src="/thumbnails/post_estimator_shark.gif" width="100" />
		<div class="card-body">
		<h5 class="card-title">
		Deep Learning based Human Pose Estimation using OpenCV
				</h5>
				</div>
				</a>
				</div>
    

    
        <div class="card  col-md-6"  style="padding-top:15px;">
		<a href="/blog/2018-10-08-mask-rnn/"
                class="button big previous">
		
		<img class="card-img-top lazy" src="/thumbnails/maxresdefault.jpg" width="100" />
		
		<div class="card-body">
		<h5 class="card-title">
		Mask R-CNN trong bài toán nhận dạng và phân vùng đối tượng
				</h5>
				</div>
				</a>
				</div>
    

</div>
</div>



<div class="fb-comments" data-href="" data-width="" data-numposts="5"></div>

    <article class="post">
        
        <div class="disqus-comments">                  
            <button id="show-comments" class="btn btn-warning" type="button">Show <span class="disqus-comment-count" data-disqus-url="blog/2018-10-05-deep-learning-base-multiple-human-pose-estimation">comments</span></button>
            <div id="disqus_thread"></div>
          </div>
    </article>





		
    </main>
    
<section id="sidebar" class="col-md-3">
<br/>
<div>
		<ul class="list-group" id="news-contents"></ul>
    <div id="calander">
	</div>
    </div>

  
   
  
  
  
  
  
  
  
  
 

</section>

</div>
	</div>
    
	<hr>
  <footer class="footer">
  <div class="container text-center">
    
    <p class="copyright">
      
        &copy; 2018 Phạm Duy Tùng. Website chia sẻ kiến thức của Phạm Duy Tùng và Đặng Thị Hằng. Vui lòng liên hệ email alexblack2202@gmail.com nếu bạn có thông tin cần trao đổi.
      
     
    </p>
	
	<a href="//www.dmca.com/Protection/Status.aspx?ID=5cadb1e5-83a1-4033-96e9-b0f21edfaca7" title="DMCA.com Protection Status" class="dmca-badge"> <img src ="https://images.dmca.com/Badges/dmca_protected_sml_120m.png?ID=5cadb1e5-83a1-4033-96e9-b0f21edfaca7"  alt="DMCA.com Protection Status" /></a>  <script src="https://images.dmca.com/Badges/DMCABadgeHelper.min.js"> </script>
	</div>
	

      <div id="fb-root"></div>
      <script>
        window.fbAsyncInit = function() {
          FB.init({
            xfbml            : true,
            version          : 'v10.0'
          });
        };

        (function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = 'https://connect.facebook.net/vi_VN/sdk/xfbml.customerchat.js';
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));</script>

      
      <div class="fb-customerchat"
        attribution="setup_tool"
        page_id="1244186728962161">
      </div>
  </footer>
    
    

    
      
    

    
      
      
      
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
        
        
        
        <script  src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/languages/python.min.js"></script>
        <script defer="defer" async="async">hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>
      
    
    
    
    <script defer="defer" src="https://cdnjs.cloudflare.com/ajax/libs/skel/3.0.1/skel.min.js"></script>
     

   <script  src="/js/jquery-2.2.4.min.js" data-cfasync="false"></script>
   
    <script defer="defer" async="async" src="/js/bootstrap.min.js"></script>
      <script defer="defer" async="async" src="/js/util.js"></script>
	  
      <script defer="defer"  src="/js/main.js"></script>
     
    

    
      
        
      
    
	
    
    
      

<script defer="defer" async="async" src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script  defer="defer" async="async" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


	  
	  
	   

<script id="dsq-count-scr" src="//phamduytung.disqus.com/count.js" async></script>

 <script defer="defer"   src="/js/jquery.amlich.js" data-cfasync="false" ></script>
	   <script defer="defer"   type="text/javascript"  data-cfasync="false">

  function getcontent(){
 

   

           
                }
            
			
			$(document).ready(function(){
      getcontent();
      $('#calander').amLich({
  type: 'calendar', 
  tableWidth: '100%' 
});
  

			}); 
			
			
			$(function(){
  $('#show-comments').on('click', function(){
    var disqus_shortname = 'phamduytung';

    (function() {
      var disqus = document.createElement('script'); 
      disqus.type = 'text/javascript'; 
      disqus.async = true;
      disqus.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(disqus);
    })();

    $(this).hide(); 
  });
});

var disqus_config = function () {
  this.page.url = 'blog\/2018-10-05-deep-learning-base-multiple-human-pose-estimation';
};
</script>

<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script> (adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "ca-pub-4644989745435991",
enable_page_level_ads: true
});
</script>
 


  </body>
</html>

